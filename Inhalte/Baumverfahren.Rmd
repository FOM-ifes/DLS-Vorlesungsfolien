```{r setup-Baum-basierte-Verfahren, include=FALSE}
# ---------------------------------------------------------------------------
#% maintainer:
#%   - Karsten Luebke
#%
# ---------------------------------------------------------------------------
source("../prelude.R")
initPart(
    "Baumverfahren",          # Dateiname ohne Suffix
    "Baum-basierte-Verfahren"   # Verzeichnisname im Bilderverzeichnis 
    )
pathToImages <- getPathToImages()
# ---------------------------------------------------------------------------

library(mosaic)
library(rpart)
library(rpart.plot)
library(viridis)

tips <- assertData("tips.csv", "https://goo.gl/whKjnl")
```
# Baum-basierte Verfahren



### Lernziele {exclude-only=NOlernziele}

Die Studierenden ...

- können die Begriffe *Klassifikationsbaum* und *Regressionsbaum* erläutern.
- wissen, wie man einen Klassifikations- oder Regressionsbaum in R berechnet.
- wissen, wie man die Modellgüte eines Baummodells quantifizieren kann.
- können Vor- und Nachteile von Baummodellen anführen.
- wissen, was man unter einem *Random Forest*-Modell versteht.
- kennen Verfahren, um die Variablenwichtigkeit zu quantifizeren.




### Modellierung

Modelliere Zusammenhang zwischen einer abhängigen Variable $y$ und einer oder mehrerer unabhängiger $x_i$:^[Leo Breiman (2001): *Statistical modeling: The two cultures.* Statistical Science, 16(3), 199--231. [https://projecteuclid.org/euclid.ss/1009213726](https://projecteuclid.org/euclid.ss/1009213726)]

$$y=f(x)+\epsilon$$

- **Data Modeling**: Annahme über die Funktion $f$, Schätzung von unbekannten Modellparametern mit Hilfe einer Stichprobe: z. B. lineare oder logistische Regression. 
- **Algorithmic Modeling**: $f$ unbekannt, algorithmische Bestimmung des Zusammenhangs von $x$ und $y$ anhand der vorliegenden Daten: z. B. Baum-basierte Verfahren.

### Baumverfahren

**Regressions- und Klassifikationsbäume** (CART, Classification and Regression Trees) ähneln Entscheidungsbäumen (`rpart()`): 

- Von der *Wurzel* (root) über Äste (branches) und Aufteilungen (internal nodes) hin zu *Blättern* (terminal node; leaf).
- Aufteilung anhand eines **Kriteriums** (splitting criterion):  Werte der abhängigen Variable in den Ästen bzw. Blättern immer einheitlicher (impurity $\rightarrow$ purity).
- Um Überanpassung zu vermeiden, wird der Baum beschnitten (pruning).

### Beispiel Klassifikationsbaum Raucher (I/II)

```{r Baumverfahren_Beispiel_1, echo=FALSE, fig.align="center", out.width="70%"}
b1 <- rpart(smoker ~ sex+day+time+size, data = tips)
rpart.plot(b1, box.palette = viridis(2, alpha = 0.6))
```

### Beispiel Klassifikationsbaum Raucher (II/II)

```{r Baumverfahren_Beispiel_2, echo=FALSE, fig.align="center", out.width="70%"}
b1b <- rpart(smoker~total_bill+tip, minsplit = 30, data = tips)
rpart.plot(b1b, box.palette = viridis(2, alpha = 0.6))
```

### Beispiel Partition Klassifikationsbaum Raucher

```{r Baumverfahren_Beispiel_3, echo=FALSE, fig.align="center", out.width="80%"}
tips_fac <- tips
tips_fac$smoker <- as.factor(tips$smoker)
klaR::partimat(smoker ~ tip + total_bill, 
                method = "rpart", 
                minsplit = 30, 
                imageplot = TRUE, 
                image.colors = viridis(2, alpha = 0.6), 
                data = tips_fac)
rm(tips_fac)
```


## Regressionsbaum

### Regressionsbaum

- **Numerische** Zielvariable
- Splitting à la ANOVA: Minimiere Fehlerquadratsumme.
- Prognose $\hat{y}$: arithmetischer Mittelwert von $y$ im Blatt.

### Vorbereitung: Trinkgelddaten

Einlesen der "Tipping"^[Bryant, P. G. and Smith, M (1995) Practical Data Analysis: Case Studies in Business Statistics. Homewood, IL: Richard D. Irwin Publishing] Daten sowie laden des Pakets `rpart`.

```{r Baumverfahren_Vorbereitung_Trinkgelddaten, eval= FALSE, message=FALSE}
download.file("https://goo.gl/whKjnl", destfile = "tips.csv")
tips <- read.csv2("tips.csv")
# Alternativ - heruntergeladene Datei einlesen:
# tips <- read.csv2(file.choose()) 

library(rpart) # Paket für Baumverfahren laden
```

### Vorbereitung Installation Zusatzpakete

Einmalige Installation von Zusatzpaketen:

```{r, eval=FALSE}
install.packages("rpart.plot")
```

Paket laden:
```{r}
library(rpart.plot)
```

### Modellierung relative Trinkgeldhöhe

```{r, eval=FALSE}
ergregbaum <- rpart( (tip/total_bill) ~ size + 
                       day + time + smoker + sex, 
                     data=tips)

rpart.plot(ergregbaum, box.palette = viridis(2, alpha = 0.6))
```

*Hinweis:* Die obere Zahl in den Wurzeln, Aufteilungen und Blättern gibt $\hat{y}$ an, die Prozentzahl darunter ist der jeweilige Anteil der Beobachtungen (bezogen auf $n$) in diesem Zweig.

### Ergebnis Modellierung relative Trinkgeldhöhe
```{r, echo=FALSE, fig.align="center", out.width="85%"}
ergregbaum <- rpart( (tip/total_bill) ~ size + 
                       day + time + smoker + sex, 
                     data = tips)

rpart.plot(ergregbaum, box.palette = viridis(2, alpha = 0.6))
```

### Übung `r nextExercise()`: Ergebnis Aufteilung relative Trinkgeldhöhe {.exercise type=A-B-C-D answer=C}

Welche der folgenden Aussagen stimmt?

A.  Bei mehr als $50\,\%$ der Beobachtungen waren mehr als $2$ Personen am Tisch.
B.  Die meisten Beobachtungen fallen ins Blatt "Nicht $\geq 2.5$ Personen am Tisch" und "Nicht Samstag oder Donnerstag" und "Nicht Mann".
C.  Die mittlere relative Trinkgeldhöhe liegt bei $2$ oder weniger Personen bei $17\,\%$.
D.  Die Variable `smoker` ist ein wichtiger Indikator für die relative Trinkgeldhöhe.

<div class="notes">
Zu *A*: nein, da nur $34\,\%$ die Bedingung $size\geq2.5$ erfüllen (Blatt unten links). *B* ist falsch, da "Nicht $\geq 2.5$ Personen am Tisch" und "Nicht Samstag oder Donnerstag" und "Nicht Mann" nur von $6\,\%$ erfüllt wird (Blatt unten rechts). *D* ist falsch, da die Variable `smoker` nicht Bestandteil des Baumes ist. ***C*** stimmt, da, wenn die Bedingung $size\geq2.5$ nicht erfüllt wird, $\hat{y}=0.17$ ist (Aufteilung rechts in der zweiten Zeile).
</div>

### Übung `r nextExercise()`: Ergebnis relative Trinkgeldhöhe {.exercise type=A-B-C answer=B}

Stimmt die folgende Aussage: An einem Freitag zahlen Männer bei maximal $2$ Personen am Tisch im Mittel ein höheres relatives Trinkgeld als Frauen?

A.  Ja.
B.  Nein.
C.  Weiß nicht.

<div class="notes">
Nein (***B***), da die mittlere relative Trinkgeldhöhe bei maximal $2$ Personen am Freitag bei den Frauen bei $0.21$ liegt (siehe unten 2. Blatt von rechts), bei den Männer hingegen bei $0.17$ (siehe Blatt unten rechts).
</div>

### Übung `r nextExercise()`: Ergebnis relative Trinkgeldhöhe Tageszeit {.exercise type=A-B-C answer=C}

Stimmt die folgende Aussage: Die relative Trinkgeldhöhe ist zum Dinner höher als beim Lunch?

A.  Ja.
B.  Nein.
C.  Weiß nicht.

<div class="notes">
Weiß nicht (***C***), da die Variable `time` nicht im Baum angezeigt wird.
</div>

## Klassifikationsbaum

### Klassifikationsbaum

- **Kategoriale** Zielvariable
- Splitting à la Gini (Konzentration): Maximiere Einheitlichkeit. 
- Prognose: $\hat{y}$: Modalwert, d. h. häufigste Merkmalsausprägung des Blattes $k$ .

### Modellierung Geschlecht

```{r, eval=FALSE}
ergklassbaum <- rpart(sex ~ size + day + time + smoker, 
                     data=tips)

rpart.plot(ergklassbaum, box.palette = viridis(2, alpha = 0.6))
```

*Hinweis:* Die obere Zahl in den Wurzeln, Aufteilungen und Blättern gibt $\hat{y}$ an, bei binären Merkmalen also $P(Y=1)$. Somit bezieht sich die angegebene Wahrscheinlichkeit auf die zweite Merkmalsausprägung -- im Vergleich zur ersten, der Referenz. Hier wird also $P(\text{sex=='Male'})$ ausgegeben. Die Prozentzahl darunter ist wie bei metrischen Zielvariablen der jeweilige Anteil der Beobachtungen in diesem Zweig. Der Name der Wurzel, der Aufteilung und des Blattes bezieht sich immer darauf, welche Ausprägung der Zielvariable in diesem Zweig den höheren Anteil hat.


### Ergebnis Modellierung Geschlecht 
```{r, echo=FALSE, fig.align="center", out.width="85%"}
ergklassbaum <- rpart(sex ~ size + day + time + smoker, 
                     data = tips)

rpart.plot(ergklassbaum, box.palette = viridis(2, alpha = 0.6))
```

### Übung `r nextExercise()`: Ergebnis Aufteilung Modellierung Geschlecht  {.exercise type=A-B-C answer=A}

Welche der folgenden Aussagen stimmt?

A.  Ca. $67\,\%$ der Rechnungen fallen *nicht* am Donnerstag oder Freitag an.
B.  Die meisten Rechnungen werden von Frauen bezahlt.
C.  $20\,\%$ der Beobachtungen sind Tischgesellschaften am Donnerstag oder Freitag, an denen geraucht wird.

<div class="notes">
***A*** stimmt -- siehe Blatt rechts unten. *B* stimmt nicht, da die meisten ($64\,\%$) Rechnungen von Männern bezahlt werden (siehe Wurzel). *C* stimmt nicht, da Rauchen Nicht-Nichtrauchen bedeutet und das gilt für $13\,\%$ der Beobachtungen am Donnerstag oder Freitag (siehe mittleres Blatt unten).
</div>

### Übung `r nextExercise()`: Ergebnis Aufteilung Modellierung Geschlecht  {.exercise type=A-B-C-D answer=B}

Welche der folgenden Aussagen stimmt?

A.  Am Donnerstag oder Freitag, wenn nicht geraucht wird, beträgt der Anteil der Frauen $0.45$.
B.  Am Donnerstag oder Freitag, wenn nicht geraucht wird, beträgt der Anteil der Männer $0.45$.
C.  Am Donnerstag oder Freitag, wenn nicht geraucht wird, beträgt der Anteil der Frauen $0.20$.
D.  Am Donnerstag oder Freitag, wenn nicht geraucht wird, beträgt der Anteil der Männer $0.20$.

<div class="notes">
***B*** ist korrekt (siehe Blatt links unten). *Hinweis:* Da der Anteil der Frauen in diesem Zweig höher ist, wird das Blatt mit "Female" bezeichnet, die angegebene Ausprägung bezieht sich aber auf $P(Y=1)$, also $P(\text{sex=='Male'}).$
</div>

### Übung `r nextExercise()`: Ergebnis Aufteilung Modellierung Geschlecht durch Wochentag {.exercise type=A-B-C answer=C}

Stimmt die Aussage: Die beste Prognose im Modell an einem Donnerstag oder Freitag lautet: Eine Frau zahlt die Rechnung?

A.  Ja.
B.  Nein.
C.  Hängt davon ab, ob geraucht wird.

<div class="notes">
***C***, da, wenn Donnerstag oder Freitag ist, zusätzlich noch nach Rauchen getrennt wird. Bei den Nichtrauchern liegt der Anteil Frauen über $50\,\%$ (siehe Bezeichnung des Blatts), bei den Rauchern hingegen der Anteil der Männer.
</div>

### Optionen `rpart()`

Es gibt viele Optionen, die die Modellierung und das Ergebnis eines Baumes beeinflussen:
```{r, eval=FALSE}
?rpart.control
```

Optimierung z. B. über `train()` aus dem Paket `caret`.^[[https://topepo.github.io/caret/index.html](https://topepo.github.io/caret/index.html)]


## Modellgüte

### Modellgüte (Root) Mean Squared Error

(Test) **Mean Squared Error** (MSE):
$$MSE=E(y_0-\hat{y}_0)^2=Var(\hat{y}_0)+Bias^2(\hat{y}_0)+Var(\epsilon)$$

- **Varianz**: Streuung in der Prognose bedingt durch Varianz der Trainingsdaten -- je flexibler/komplexer ein Modell desto größer
- **Bias**$^2$: Abweichung$^2$ zwischen wahren Wert und Schätzwert, z. B.  bedingt durch Modellfehler -- je flexibler/komplexer ein Modell desto kleiner

Der **Root Mean Squared Error** $RMSE=\sqrt{MSE}$ hat die gleiche Einheit wie $y$.

### Vorbereitung: Schätzen RMSE

Zwei Drittel der Beobachtungen als Lernstichprobe, ein Drittel als Teststichprobe:
```{r}
set.seed(1896)
n <- nrow(tips)
lern <- sample(1:n, size = round(n*(2/3)))

train <- tips[lern,]
test <- tips[-lern,]
```

### Modelle lernen

```{r}
ergbase <- lm(total_bill ~ 1, data = train)
erglm <- lm(total_bill ~ size + day + time + 
              smoker + sex, data = train)
ergbaum <- rpart(total_bill ~ size + day + time + 
                   smoker + sex, data = train)
```

### Modelle anwenden, Vorhersagen

```{r}
# y_0
ytest <- test$total_bill

# Vorhersagen
ydachbase <- predict(ergbase, newdata = test)
ydachlm <- predict(erglm, newdata = test)
ydachbaum <- predict(ergbaum, newdata = test)
```

### RMSE schätzen

```{r}
# Nullmodell
sqrt(mean((ytest - ydachbase)^2))
# lm
sqrt(mean((ytest - ydachlm)^2))
# rpart
sqrt(mean((ytest - ydachbaum)^2))
```

### Übung `r nextExercise()`: RMSE {.exercise type=A-B-C answer=B}

Welches ist hier das im Bezug auf den RMSE beste Verfahren?

A.  Basismodell: Vorhersage durch Mittelwert
B.  Lineare Regression
C.  Regressionsbaum

<div class="notes">
Die Lineare Regression hat den kleinsten $RMSE$ mit $RMSE=`r round(sqrt(mean((ytest-ydachlm)^2)),2)`$, es ist damit also das beste Verfahren, also stimmt ***B***.
</div>


### Vorteile Baumverfahren

- leicht zu erklären und zu interpretieren
- Evtl. spiegeln sie menschliches Verhalten wider
- Robust gegen Ausreißer
- Bimodale/nicht-monotone Zusammenhänge können dargestellt werden
- Qualitative und quantitative Variablen können direkt verwendet werden, fehlende Werte ggf. als eigene Merkmalsausprägung
- Integrierte Variablenselektion

### Nachteile Baumverfahren 

- Nicht optimal im Sinne eines Zielkriteriums (i.d.R. existieren "bessere" Verfahren)
- Hängen von *Parametern* ab (`rpart.control`)
- Evtl. instabil (kleine Änderungen der Daten können zu völlig anderen Bäumen führen)
- Nur eindimensionale hierarchische Partitionen

## Random Forests

### Random Forests

- Hintergrund: Aggregieren (z. B. Mittelwert bilden) kann die Streuung reduzieren: 
$$Var(X_i)=\sigma^2 \Rightarrow Var(\bar{X})=\frac{\sigma_2}{n} \quad (\text{wenn }X_i, \ldots,X_n \text{ }i.i.d.)$$
- Bagging (Bootstrap aggregation): Ziehe $B$ Bootstrapstichproben, schätze darauf $f$ und aggregiere: 
$$\hat{f}_{avg}(x)=\frac{1}{B}\sum_{i=1}^B\hat{f}^b(x)$$
- **Random Forrest**: Bilde Bäume auf Bootstrapstichproben, wobei jeweils nur eine zufällige Auswahl der unabhängigen Variablen $x_p$ zum Aufteilen herangezogen werden darf -- so wird die Korrelation der Variablen und damit die Korrelation und Varianz der Bäume reduziert.

### Random Forests mit R

```{r, eval=FALSE}
# Einmalig installieren
install.packages("randomForest")
```

```{r, message=FALSE, warning=FALSE}
# Paket laden
library(randomForest)
# Zufallszahlengenerator setzen
set.seed(1896)
```

### Random Forest zur Trinkgeldprognose

```{r}
ergrf <- randomForest(tip ~ ., data = tips)
ergrf
```

### Variablenwichtigkeit mit Random Forests

Für alle Bäume kann für jede Aufteilung die Verbesserung (Reduktion) der Uneinheitlichkeit (impurity), d. h. RSS (Regression) bzw. Gini (Klassifikation) der unabhängigen Variablen berechnet werden. Variablen die zu einer deutlichen Verbesserung führen sind wichtiger.

```{r, fig.align="center", out.width="50%"}
varImpPlot(ergrf)
```


### Übung `r nextExercise()`: Variablenwichtigkeit {.exercise type=A-B answer=A}

Welche Variable scheint hier die wichtigste zu sein?

A.  Rechnungshöhe
B.  Tageszeit

<div class="notes">
Die Einheitlichkeit steigt am stärksten mit der Rechnungshöhe `total_bill`, daher ***A***.
</div>

### Prognose durch Random Forests

```{r}
ergwald <- randomForest(total_bill ~ size + day + time + 
                   smoker + sex, data = train)

ydachwald <- predict(ergwald, newdata = test)

sqrt(mean((ytest - ydachwald)^2))
```

### Eigenschaften Random Forests

- Vorteile:
    - In der Regel sehr gute Prognoseeigenschaften
    - Bestimmung der Variablenwichtigkeit
- Nachteile:    
    - Keine einfache Interpretierbarkeit mehr
    - Hängt von Parametern ab (siehe Hilfeseite zur Funktion)
    - Rechenaufwendig
    

```{r finish-Baum-basierte-Verfahren, include=FALSE}
rm(pathToImages)

detach("package:rpart.plot", unload = TRUE)
detach("package:rpart", unload = TRUE)
finalizePart(partname)
```