```{r setup-Oeko-LogistischeRegression, include=FALSE}
# ---------------------------------------------------------------------------
#% maintainer:
#%   - Karsten Luebke, Bianca Krol
#%
# ---------------------------------------------------------------------------
source("../prelude.R")
initPart(
    "Oeko-LogistischeRegression",  # Dateiname ohne Suffix
    "Regression"     # Verzeichnisname im Bilderverzeichnis 
    )
pathToImages <- getPathToImages()
# ---------------------------------------------------------------------------

library(mosaic)
library(gridExtra)

```

# `r nextChapter()` Logistische Regression

### Übersicht 

\large

-  Grundlagen Logistische Regression
-  Interpretation der Modellparameter
-  Globale Modellgüte
-  Signifikanz der Regressionskoeffizienten
-  Regressionsdiagnostik
-  Klassifikationseigenschaften

## Grundlagen Logistische Regression

### Warum brauchen wir die logistische Regression?

\footnotesize

Beispiel: Modellierung der monatlichen Geldausgangs auf dem Girokonto mittels linearer Regression

Kundennummer|Anzahl Konten|Durchschnittlicher monatlicher Geldeingang|Durchschnittlicher monatlicher Geldausgang
:------------:|:------------:|:------------:|:------------:
1|4|2500|2300
2|2|1800|1000
3|1|2000|1800
4|1|1500|1200
5|2|2000|?
6|1|2300|?
7|2|1800|?

Vorgehensweise

1. Modellierung der Regressionsgleichung mit den Daten, bei denen die Zielgröße eine Ausprägung hat.
2. Anwendung der Regressionsgleichung auf die Daten, für die die Zielgröße zu prognostizieren ist.

### Warum brauchen wir die logistische Regression?

\footnotesize

Beispiel: Modellierung Kreditausfall ja/nein mittels linearer Regression

Kundennummer|Kreditlaufzeit in Jahren|Monatliche Rate|Kreditausfall (1=ja, 0=nein)
:------------:|:------------:|:------------:|:------------:
1|2|500|1
2|5|200|1
3|10|1000|0
4|5|800|0
5|5|400|?
6|10|500|?
7|1|100|?

Vorgehensweise

1. Modellierung der Regressionsgleichung mit den Daten, bei denen die Zielgröße eine Ausprägung hat.
2. Anwendung der Regressionsgleichung auf die Daten, für die die Zielgröße zu prognostizieren ist.

\large\center
\textcolor{blue}{\textbf{Funktioniert das?}}

### Warum brauchen wir die logistische Regression?

\footnotesize

Angenommen, es seien folgende Parameter mit der linearen Regression geschätzt worden:
$$\hat\beta_0=0.01,\ \hat\beta_1=-0.008,\ \hat\beta_2=0.0025 $$

Damit lautet die lineare Regressionsgleichung:

$$\widehat{\textsf{Kreditausfall}}=0.01-0.008\cdot\textsf{Kreditlaufzeit}+0.0025\cdot\textsf{Rate}$$

Wird diese Regressionsgleichung auf die Daten der Tabelle angewendet, so ergibt sich:

Kundennummer|Kreditlaufzeit in Jahren|Monatliche Rate|Kreditausfall (1=ja, 0=nein)
:------------:|:------------:|:------------:|:------------:
1|2|500|1
2|5|200|1
3|10|1000|0
4|5|800|0
5|5|400|**0.97**
6|10|500|**1.18**
7|1|100|**0.252**

\large\center
\textcolor{blue}{\textbf{Ergebnis nicht sinnvoll interpretierbar!}}

### Transformation

\footnotesize

Da die lineare Regression bei einer binären abhängigen Variable nicht sinnvolle Ergebnisse liefert, ist sie nicht direkt geeignet.

Wenn wir ein Ereigniss betrachten, dass eintreten kann (z. B. Kreditausfall = ja) oder nicht und dies mit $1$ und $0$ codieren, können wir die relative Häufigkeit vieler solcher Ereignisse als Näherung für die Wahrscheinlichkeit $p$ interpretieren, dass das Ereignis eintritt.

Die Wahrscheinlichkeit $p$ liegt im Intervall $[0,1]$ und ist somit nicht direkt für die Regression geeignet. Sie wird in 2 Schritten transformiert:

1. Transformation in die Chance (*Odd*), die im Wertebereich $[0,+\infty]$ liegt:

$$O=\frac{p}{1-p}$$

2. Logarithmierung des Odds ergibt einen Wertebereich $[-\infty,+\infty]$:

$$\textrm{Logit }L=\ln(O)=\ln(p)-\ln(1-p)$$

Dies führt zu den sogenannten *Logit*-Modellen.

Statt der Chancenbildung und Logarithmierung kann auch die Normalverteilungsfunktion zur Transformation genutzt werden: *Probit*-Modelle.

### Transformation

\footnotesize

```{r echo=FALSE, out.width = "60%", fig.align="center"}
p <- seq(0, 1, length = 100)
p <- p[-c(1, 100)]
o <- p/(1-p)
# Graphik Odd
p1 <- gf_line(p ~ o, color = "blue", title = "Odd", xlab = "")
# Graphik ln(Odd)
p2 <- gf_line(p ~ log(o), color = "blue", title = "Logit", xlab = "")
grid.arrange(p1, p2, nrow = 1)
```


### Anwendungen der logistischen Regression

Lineare Regression: abhängige Variable ist metrisch.

Logistische Regression: Schätzung der Wahrscheinlichkeit für eine Gruppenzugehörigkeit; abhängige Variable ist kategorial.

Beispiele:

- Kreditrückzahlung: Kunde kündigt Kredit vorzeitig oder Kunde zahlt Kredit wie vorgesehen ab oder Kunde kann Kredit nicht abbezahlen
- Spezialfall Kreditausfall: ja oder nein?
- Börsengang: ja oder nein?
- Feindliche Übernahme: ja oder nein?
- Dividendenausschüttung: ja oder nein?
- \dots

Hier: Beschränkung auf binäre (dichotome) logistische Regression, d. h., die abhängige Variable hat nur zwei Ausprägungen.

### Modellgleichung

$Y$ ist die abhängige binäre Variable mit den Ausprägungen $0,1$, $p_1=P(Y=1)$ ist die Wahrscheinlichkeit, dass $Y$ die Ausprägung $1$ annimmt. $\mathcal{L}$ ist die Linkfunktion $\mathcal{L}:\mathbb{R}\to [0,1]$.

Die Modellgleichung der logistischen Regression lautet:^[Siehe z.B. Krafft (1997), S. 627]

$$p_{i1}=\mathcal{L}\left (\beta_0+\sum_{k=1}^K \beta_kx_{ik}+u_i \right ) \qquad\textsf{für}\quad i=1,\dots,n $$
mit

\begin{tabular}{llll}
& $p_{i1}$ & & Wahrscheinlichkeit, dass die abhängige Variable $y_i$ den Wert 1 annimmt  \\
& $x_{ki}$ & & Messwerte für die unabhängigen Variablen $X_k$  \\
& $\beta_0 $ & & Konstante  \\
& $\beta_k $ & & Regressionskoeffizient für die unabhängige Variable $k$  \\
& $i$ & & Index der Beobachtungen ($i=1,\dots,n$)  \\
& $k$ & & Index der unabhängigen Variablen $X_k$ ($k=1,\dots,K$)  \\
& $u_i$ & & Werte der Residualgröße bzw. des Residuums  \\
& $n $ & & Stichprobenumfang, Gesamtzahl der Beobachtungen \\ 
\end{tabular}

### Linkfunktion

Die Linkfunktion $\mathcal{L}$ transformiert den linearen Prädiktor $\eta = \beta_0+\sum_{k=1}^K \beta_kx_{ik}$ auf das Intervall $[0,1]$:

```{r echo = FALSE, fig.width=10, fig.asp=0.7, out.width="10cm"}
x <- seq(-4, 4, length = 200)
logit = plogis(x, location = 0, scale = 1)

data <- data.frame(x = x, y = logit)

ggplot(data, aes(x = x, y = y)) +
  theme(plot.margin = unit(c(0, 0, 0, 0), "cm")) +
  geom_segment(aes(x = -4, xend = +4, y = -0.05, yend = 1.05), size = 1, color = "darkgrey") +
  geom_line(color = "blue") + 
  xlab(expression(eta)) + ylab("Wahrscheinlichkeit") + 
  theme(axis.title = element_text(size = 20), axis.text = element_text(size = 20)) +
  scale_y_continuous(breaks = c(0, 0.25, 0.5, 0.75, 1), limits = c(-0.05, 1.05)) + 
  scale_x_continuous(breaks = c(-4, -2, -0, 2, 4)) +
  annotate("text", x = 0.5, y = 0.75, parse = TRUE, 
           label = "'Linearer Prädiktor ' * eta" ,
           hjust = 1, vjust = 0, size = 7, color = "blue") +
  annotate("text", x = 0.5, y = 0.75, parse = FALSE, 
           label = "mit Linkfunktion" ,
           hjust = 1, vjust = 1, size = 7, color = "blue") +
  annotate("text", x = 0.8, y = 0.5, parse = TRUE, 
           label = "'Linearer Prädiktor ' * eta" ,
           hjust = 0, vjust = 0, size = 7, color = "darkgrey") +
  annotate("text", x = 0.8, y = 0.5, parse = FALSE, 
           label = "ohne Linkfunktion" ,
           hjust = 0, vjust = 1, size = 7, color = "darkgrey")
```

### Linkfunktion

Die logistische Regression nutzt die Logit-Funktion:

\[{\rm{logit}}\left( \eta  \right) = \frac{{{e^\eta }}}{{1 + {e^\eta }}} = \frac{{{e^{{\beta _0} + \sum\limits_{k = 1}^K {{\beta _k}{x_k}} }}}}{{1 + {e^{{\beta _0} + \sum\limits_{k = 1}^K {{\beta _k}{x_k}} }}}}\,\,\,\,\, = \,\,\,\,\,\frac{1}{{1 + {e^{ - \eta }}}} = \frac{1}{{1 + {e^{ - \left( {{\beta _0} + \sum\limits_{k = 1}^K {{\beta _k}{x_k}} } \right)}}}}\]

Die Probit-Regression nutzt die Verteilungsfunktion $\Phi$ der Normalverteilung:

\[{\rm{probit}}\left( \eta  \right) = \Phi \left( \eta  \right) = \int\limits_{ -\infty }^\eta  {\frac{1}{{\sqrt {2\pi } }}} e^{-\frac{1}{2}t^2}dt\]

### Linkfunktion

\footnotesize

Beide unterscheiden sich nur geringfügig im Verlauf und liefern vergleichbare Ergebnisse.^[Siehe z.B. Gujarati/Porter (2009), S. 571]

```{r echo = FALSE, fig.asp=0.6, out.width = "60%", fig.align="center"}
x <- seq(-4, 4, length = 200)
logit = plogis(x, location = 0, scale = 1)
probit = pnorm(x, mean=0, sd=1)

# data <- data.frame(x = o[2:99], y = p[2:99])
d1 <- data.frame(x = x, y = logit, Funktion = "Logit")
d2 <- data.frame(x = x, y = probit, Funktion = "Probit")
data <- rbind(d1, d2)

gf_line(y ~ x, group = ~ Funktion, data = data, color = ~ Funktion, ylab = "P(y=1)", xlab = expression(eta)) + scale_color_viridis_d()
```

**Hinweis:** Die Ausdifferenzierung der Wahrscheinlichkeiten $p_1$ erfolgt nur in einem kleinen Wertebereich von $\eta$.



## Interpretation der Modellparameter

### Direkte Interpretation

\footnotesize

Es gilt wieder $\eta = \beta_0+\sum_{k=1}^K \beta_kx_{ik}$ und $p_1=P(y=1)$.

```{r echo = FALSE, fig.asp = 1, out.width = "40%", fig.align="center"}
x=seq(-4,4,length=200)
logit=plogis(x,location=0,scale=1)
gf_line(logit ~ x, ylab = "p1", xlab = expression(eta)) %>% 
gf_hline(yintercept = 0.5, col = "darkgrey") %>% 
gf_vline(xintercept = 0, col = "darkgrey")
```
\vspace{-0.5cm}


- Ist $\eta=0$ (und damit auch $\beta_0=0$), dann ist $p_1=0.5$.
- Sei $x_1=0, \dots, x_K=0$. Dann gilt für $\beta_0>0$ $p_1>0.5$ und für $\beta_0<0$ $p_1<0.5$.
- Ist $\eta>0\quad(<0)$, dann ist $p_1>0.5\quad(<0.5)$.
- Positive Werte für $\beta_k$ (für $k=1, \dots, K$) erhöhen erhöhen bei steigendem $x$ $p_1$, negative verringern $p_1$.

### Direkte Interpretation

\footnotesize

Beispiel Kreditausfallprognose

Es sei folgender linearer Prädiktor gegeben:

$$\eta=-0.2-0.008 \cdot \textsf{Bonität}+0.0025 \cdot \textsf{Monatliche Rate}$$

Weiter sei $p_1=P(\textrm{Kreditausfall} = ja)$.

Dann lassen sich die Modellparameter wie folgt interpretieren:

- Die Konstante $\beta_0$ ist kleiner $0$, also ist $p_1$ für eine Bonität von 0 und einer monatlichen Rate von 0 kleiner 50%.
- Die Bonität wirkt negativ auf die Wahrscheinlichkeit für einen Kreditausfall: Je höher die Bonität, desto geringer ist die Wahrscheinlichkeit für einen Kreditausfall.
- Die monatliche Rate wirkt positiv auf die Wahrscheinlichkeit für einen Kreditausfall: Je größer die monatliche Rate, desto größer ist die Wahrscheinlichkeit für einen Kreditausfall.

Abgesehen von der Interpretation des Vorzeichens der Koeffizienten ist keine direkte Interpretation der Koeffizienten einer logistischen Regression möglich. 
Insbesondere können die Regressionskoeffizienten nicht wie bei der linearen Regression als Steigung bzw. als Veränderung der abhängigen Variable $y$ bei einer marginalen Veränderung der unabhängigen Variable $x$ interpretiert werden.

### Relatives Risiko

Seien diesmal $p_1(x_1)=P(Y=1|x_1)$ bzw. $p_1(x_2)=P(Y=1|x_2)$ die Wahrscheinlichkeiten, dass die abhängige Variable $Y$ den Wert $1$ annimmt für gegebene Werte $x_1$ und $x_2$ für die unabhängigen Variablen $X$.

Das *relative Risiko* **RR** ist definiert als das Verhältnis der Eintrittswahrscheinlichkeiten für die Zielgröße bei zwei unterschiedlichen Ausprägungen der Einflussgröße.

Beispiel Kreditausfallprognose:
Sei $p_1$ die Wahrscheinlichkeit, dass ein Kredit nicht zurückgezahlt wird. Sei $X$ eine unabhängige Variable „Kunde hat Girokonto“ mit den Ausprägungen $1$ („ja“) und $0$ („nein“). Seien $x_1=0$ und $p_1(0)=5\,\%$ sowie $x_2=1$ und $p_1(1)=1\,\%$.

Dann beträgt das relative Risiko $RR=\frac{p_1(0)}{p_1(1)}=5$. D. h., ein Kunde, der kein Girokonto hat, hat ein fünffach höheres Risiko, seinen Kredit nicht zurückzuzahlen, als ein Kunde, der ein Girokonto hat.

### Chance und Chancenverhältnis

Die Chance (*Odd*) ist für ein gegebenes $x_1$ definiert als die Eintrittswahrscheinlichkeit dividiert durch seine Gegenwahrscheinlichkeit:

\[O\left( {{x_1}} \right) = \frac{{{p_1}\left( {{x_1}} \right)}}{{1 - {p_1}\left( {{x_1}} \right)}}\]

Das Chancenverhältnis (*odds ratio* **OR**) ist definiert als der Quotient zweier Chancen:

\[OR = \frac{{O\left( {{x_1}} \right)}}{{O\left( {{x_2}} \right)}} = \frac{{\frac{{{p_1}\left( {{x_1}} \right)}}{{1 - {p_1}\left( {{x_1}} \right)}}}}{{\frac{{{p_1}\left( {{x_2}} \right)}}{{1 - {p_1}\left( {{x_2}} \right)}}}}\]

Das Chancenverhältnis kann verwendet werden, um das relative Risiko abzuschätzen. Ist $p_1$ klein, so ist $RR\approx OR$, die Grenze für $p_1$ liegt dabei bei ca. $5\,\%$.^[Vgl. Osborne (2015), S. 34.]


### Übung `r nextExercise()`: Wahrscheinlichkeit und Chance {.exercise type=essay .shrink}

1. Wie hoch ist die Wahrscheinlichkeit, bei einem fairen sechsseitigen Würfel, eine $6$ zu würfeln?
2. Wie hoch ist die Chance, eine $6$ zu würfeln?

### Übung `r nextExercise()`: Chancenverhältnis {.exercise type=essay .shrink}

Nun ist ein zweiter Würfel im Spiel, der gezinkt ist. Die Wahrscheinlichkeit, mit diesem Würfel eine $6$ zu würfeln, beträgt $0.5$.

Wie hoch ist das Chancenverhältnis vom gezinkten zum fairen Würfel?


### Interpretation über das Chancenverhältnis^[Siehe z.B. Hosmer/Lemeshow (2000), S. 47 ff.; Krafft (1997), S. 633 ff.] 

**Binäre unabhängige Variable**

Die unabhängige Variable $x_1$ sei binär, d. h., sie hat zwei Ausprägungen, die mit $0$ und $1$ kodiert sind. Somit lautet die logistische Regressionsgleichung:

\[{p_1} = {\rm{logit}}\left( {{\beta _0} + {\beta _1}{x_1}} \right) = \frac{{{e^{{\beta _0} + {\beta _1}{x_1}}}}}{{1 + {e^{{\beta _0} + {\beta _1}{x_1}}}}} = \frac{1}{{1 + {e^{ - \left( {{\beta _0} + {\beta _1}{x_1}} \right)}}}}\]

Zur Ergänzung folgende 4-Feldertafel:

\begin{center}

\begin{tabular}{ccccc}
\toprule
&& $x_1=1$ && $x_1=0$ \\ 
\midrule
$y=1$ && $p_1(1)=\frac{e^{\beta_0+\beta_1}}{1+e^{\beta_0+\beta_1}}$ && $p_1(0)=\frac{e^{\beta_0}}{1+e^{\beta_0}}$ \\ 
\midrule
$y=0$ && $1-p_1(1)=\frac{1}{1+e^{\beta_0+\beta_1}}$ && $1-p_1(0)=\frac{1}{1+e^{\beta_0}}$ \\
\bottomrule
\end{tabular}

\end{center}

### Interpretation über das Chancenverhältnis 

Das Chancenverhältnis berechnet sich zu:

\[OR =\frac{{\frac{{{p_1}\left( {{x_1}} \right)}}{{1 - {p_1}\left( {{x_1}} \right)}}}}{{\frac{{{p_1}\left( {{x_2}} \right)}}{{1 - {p_1}\left( {{x_2}} \right)}}}} = e^{\beta_1}\]

Damit entspricht das Chancenverhältnis von $O(1)$ zu $O(0)$ gerade dem Wert der Exponentialfunktion an der Stelle $\beta_1$. Ist zusätzlich noch $p_1$ klein, dann gilt:

$$RR\approx OR=e^{\beta_1}$$

### Interpretation über das Chancenverhältnis -- binäre unabhängige Variable

\footnotesize

Beispiel Kreditausfallprognose:

Sei $p_1 = P(\textsf{Kreditausfall} = \textsf{ja})$. Die unabhängige Variable sei „Kunde hat Girokonto“ mit den Ausprägungen $1$ ("ja") und $0$ ("nein"). Eine logistische Regressionsrechnung ergibt folgende Regressionsgleichung: 

$$p_1=\rm{logit}(-1.5-0.5 \cdot \textsf{Kunde hat Girokonto})$$

Die Interpretation erfolgt in folgenden Schritten:

1. Prüfe, ob $p_1$ für alle Ausprägungen von „Kunde hat Girokonto“ klein ist. 
Diese Prüfung kann vorgenommen werden, indem im Datensatz die Anzahl der ausgefallenen Kredite durch die Gesamtanzahl aller Kreditnehmer dividiert wird. Für das vorliegende Beispiel wird ein kleines $p_1$ angenommen.
2. Schätze das relative Risiko über das Chancenverhältnis ab:

$$RR\approx OR=e^{-0.5}\approx 0.6065$$

3. Interpretation
Das Chance (und damit näherungsweise das relative Risiko) eines Kreditausfalls ist bei Kunden mit Girokonto nur etwa 0,6 mal so hoch wie bei Kunden ohne Girokonto.

### Interpretation über das Chancenverhältnis -- metrische unabhängige Variable

Auch bei metrischen unabhängigen Variablen kann die gleiche Beziehung zwischen Chancenverhältnis (und damit näherungsweise dem relativen Risiko) und dem Regressionskoeffizienten hergestellt werden.

Es wird von Erhöhung der unabhängigen Variable um $1$ ausgegangen:

$$RR\approx OR=\frac{O(x+1)}{O(x)}=e^{\beta_1} $$

### Beispiel Kreditausfall ja/nein

```{r}
load("../datasets/kreditausfall.RData")
str(kreditausfall)
```

Der Datensatz beinhaltet eine modifizierte und auf weniger Variablen zusammengefasste Lernstichprobe von 1000 Konsumentenkrediten einer süddeutschen Großbank aus den 1970er Jahren mit 300 schlechten und 700 guten Krediten. `ausfall = 1` bedeutet, der Kredit ist ausgefallen,

### Beispiel Kreditausfall ja/nein

```{r}
glm1 <- glm(ausfall ~ hoehe, family = binomial(logit), data = kreditausfall)
coef(glm1)
```

Die logistische Regression muss über das Generalisierte Lineare Modell geschätzt werden, da die Methode der kleinsten Quadrate hier nicht funktionieren würde. Statt `summary` hier nur Ausgabe der Koeffizienten.

Die Variable `hoehe` ist die Darlehenshöhe in DM.

Interpretation: Die Chance (näherungsweise die Wahrscheinlichkeit bzw. das relative Risiko), dass der Kredit ausfällt, nimmt näherungsweise mit $e^{`r sprintf("%f", coef(glm1)[2])`}$ je 1 DM Kredithöhe zu.

```{r}
exp(coef(glm1)[2])
```

### Übung `r nextExercise()`: Metrische unabhängige Variable {.exercise type=essay .shrink}

1. Wie stark ändert sich näherungsweise die Chance je 1000 DM Kredithöhe?
2. Kann in diesem Datensatz das relative Risiko zuverlässig über das Chancenverhältnis abgeschätzt werden?

### Interpretation über das Chancenverhältnis -- kategoriale unabhängige Variable

Eine kategoriale unabhängige Variable ist ein nominal oder ordinal skaliertes Merkmal mit mehr als zwei verschiedenen Ausprägungen.

- Beispiel Familienstand: ledig – verheiratet – geschieden – verwitwet

Wie bei der linearen Regression können derartige Merkmale nicht direkt als unabhängige Variable in die logistische Regressionsgleichung aufgenommen werden, sondern müssen über sogenannte Dummy-Variablen kodiert werden.

Die Anzahl der erforderlichen Dummy-Variablen entspricht der um 1 verminderten Anzahl der Ausprägungen. Es wird eine Referenzkategorie festgelegt (in R standardmäßig der alphanumerisch niedgriste Level), für die anderen Kategorien werden Koeffizienten berechnet. Mit `levels(variable)` können die Levels ausgegeben werden, der erste ist die Referenzkategorie. Mit `relevel()` kann eine andere Referenzkategorie festgelegt werden.

Die Interpretation erfolgt wie bei binären unabhängigen Variablen -- immer in Bezug auf die Referenzkategorie.


### Übung `r nextExercise()`: Kategoriale unabhängige Variable {.exercise type=essay .shrink}

1. Erzeugen Sie mit dem Datensatz `kreditausfall` ein logistisches Regressionsmodell `ausfall ~ famges`.
2. Was ist die Referenzkategorie?
3. Bei welchem Level ist die Wahrscheinlichkeit für einen Kreditausfall am höchsten, bei welchem am niedrigsten?
4. Wie hoch ist näherungsweise in Relation zur Referenzkategorie die Chance für einen Kreditausfall bei dem niedrigsten Level?


## Globale Modellgüte

### Likelihood Funktion

In der logistischen Regression wird das Modell mit der Maximum Likelihood Methode geschätzt.

Die Likelihoodfuntion $\mathcal{L}$ hängt von den zu schätzenden Parametern ab und ergibt als Zielwert eine Wahrscheinlichkeit:^[Siehe z.B. Backhaus et al (2011), S. 267 ff.; Hosmer/Lemeshow (2000), S. 8.]

$$\mathcal{L}(\beta_0,\beta_1,\dots,\beta_k)=P(\mathbf{X}=\mathbf{x}) $$

Dabei ist $\mathbf{x}$ eine ($n \times k$)-Matrix mit $n$ Beobachtungen und $k$ unabhängigen Variablen.

Dieser Zielwert entspricht der Wahrscheinlichkeit, für gegebene Schätzer der unbekannten Parameter die vorliegenden Daten zu beobachten.

Der Maximum-Likelihood-Schätzer ist dann gerade der Wert für die Parameterschätzer, bei dem die Wahrscheinlichkeit für die beobachteten Daten maximal ist. 

### Devianz

\footnotesize

Aus der Likelihoodfuntkion kann eine globales Maß für die Modellgüte, die Devianz (*deviance*) $-2LL$, abgeleitet werden:^[Siehe z.B. Backhaus et al (2011), S. 267 ff.; Hosmer/Lemeshow (2000), S. 8; Menard (2010), S. 46 ff.]

$$-2LL=c-2 \cdot \ln \mathcal{L}(\beta_0,\beta_1,\dots,\beta_k) $$

$c$ ist eine Konstante, die so angepasst wird, dass ein saturiertes Modell^[Ein Modell, das genauso viele Parameter wie Datenpunkte aufweist und damit perfekt an die Beobachtungen angepasst ist.] die Devianz $0$ ergibt. Für eine binäre Zielgröße (wie in unserem Fall) nimmt die Konstante den Werte $0$ an:

$$-2LL=-2 \cdot \ln \mathcal{L}(\beta_0,\beta_1,\dots,\beta_k) $$

Die Devianz des Modells wird immer mit der sogenannten Nulldevianz  $-2LL_0$ verglichen. Diese ergibt sich aus dem Nullmodell, d. h., alle Koeffizienten $\beta_1,\dots,\beta_k$ sind $0$, nur der konstante Parameter $\beta_0$ ist ggf. ungleich $0$. In einem binären Modell lässt sich die Nulldevianz lässt sich einfach berechnen:

$$-2LL_0=-2 \cdot \left(n_{Y=1} \cdot \ln \left(P(Y=1) \right) + n_{Y=0} \cdot \ln \left(P(Y=0) \right) \right) $$

Der Wertebereich von $-2LL$ ist $[0,\infty]$. Je kleiner die Devianz ist, desto besser ist das Modell.

### Übung `r nextExercise()`: Nulldevianz {.exercise type=essay .shrink}

Berechnen Sie die Nulldevianz im Modell `ausfall ~ famges` und vergleichen diesen mit dem Wert in der `summary`.


### Likelihood-Quotienten-Test

Über den Likelihood-Quotienten-Test (*likelihood ratio test, LR test*) kann die Devianz des geschätzten Modells mit der Nulldevianz verglichen werden. Somit wird die Nullhypothese $H_0: \beta_1,\beta_2,\dots,\beta_k=0$ mit der Alternativhypothese, mindestens ein $\beta$ ist verschieden von $0$, verglichen. Dies entspricht dem F-Test in der linearen Regression.^[Siehe z.B. Backhaus et al (2011), S. 267 ff.; Hosmer/Lemeshow (2000), S. 11 ff.; Menard (2010), S. 46 ff.]

Die Prüfgröße $T$ ist der mit $-2$ multiplizierte und logarithmierte Quotient aus der Likelihood des Nullmodells $\mathcal{L}_0$ (alle Koeffizienten außer $\beta_0$ sind $0$) und der des zu überprüfenden Modells $\mathcal{L}_{\beta}$:

$$T=-2\cdot \ln \frac{\mathcal{L}_0}{\mathcal{L}_{\beta}} \sim \chi^2_{(k)} $$

$T$ ist $\chi^2$ verteilt mit $k$ (= Anzahl der unabhängigen Variablen) Freiheitsgraden.

Den Likelihood-Quotienten-Test können Sie mit der Funktion `lrtest(model)` aus dem Paket `lmtest` durchführen.

### Übung `r nextExercise()`: Likelihood-Quotienten-Test {.exercise type=essay .shrink}

Führen Sie den Likelihood-Quotienten-Test im Modell `ausfall ~ famges` durch und interpretieren das Ergebnis.

### AIC und BIC -- Vergleich von Modellen

\footnotesize

Das Akaike Informationskriterium (AIC -- Akaike Information Criterion) und das Bayes Informationskriterium (BIC -- Bayes Information Criterion) leiten sich beide aus der Devianz bzw. der Likelihood Funktion ab.^[Siehe z.B. Kuha (2004).]

Beim AIC wird zusätzlich wird die Anzahl der Modellparameter (Anzahl der unabhängigen Variable $k+1$) als eine Art Strafterm mitberücksichtigt:

$$AIC=-2\cdot \ln \mathcal{L}(\beta_0, \beta_1, \dots, \beta_k)+2\cdot (k+1) = -2LL+2\cdot (k+1) $$

Im BIC wird darüberhinaus der Stichprobenumfang $n$ berücksichtigt:

$$BIC=-2\cdot \ln \mathcal{L}(\beta_0, \beta_1, \dots, \beta_k)+\ln(n)\cdot (k+1) = -2LL +\ln(n)\cdot (k+1) $$

Für größere Stichprobenumfänge ist BIC daher besser geeignet als das AIC.

Für beide Gütemaße gilt wie für die Devianz: je kleiner, desto besser. Dies kann bei einem Modellvergleich helfen. Sie können das AIC direkt im `summary` ablesen. Mit `AIC(model)` bzw. `BIC(model)` können diese auch direkt abgerufen werden.

*Hinweis:* AIC und BIC sind auch geeignete Gütemaße für die lineare Regression.


### Übung `r nextExercise()`: AIC und BIC {.exercise type=essay .shrink}

Bestimmen Sie AIC und BIC der Modelle `ausfall ~ hoehe` und `ausfall ~ famges` und vergleichen diese. Welches Modell würden Sie bevorzugen?


### Vergleich von Modellen

Ob sich die Devianz zweier Modelle signifikant unterscheidet, können Sie mit dem Likelihood-Quotienten-Test überprüfen:

```{r echo = FALSE}
library(lmtest)
glm2 <- glm(ausfall ~ famges, family = binomial(logit), data = kreditausfall)
pW <- lmtest::lrtest(glm1, glm2)$`Pr(>Chisq)`[2]
pW_print <- base::round(pW, 4) 
```

```{r}
lrtest(glm1, glm2)
```

Der p-Wert ist mit $`r pW_print`$ kleiner als $\alpha=0.05$. Somit unterscheiden sich die Modelle signikant und `ausfall ~ hoehe` ist das bessere Modell.

### Pseudo-R^2^

Aufgrund der einfachen Interpretierbarkeit des R^2^ in der linearen Regression (Anteil der erklärten Streuung) wurden vergleichbare Gütemaße für die logistische Regression hergeleitet.

In Analogie zu *Total Sum of Squares* $SST=\sum(y_i-\bar{y})$ sowie *Regression Sum of Squares* $SSR=\sum (\hat{y}_i-\bar{y})$ und *Error Sum of Squares* $SSE=\sum (y_i-\hat{y}_i)$ können Sie Nulldevianz, Nulldevianz -- Modelldevianz und Modelldevianz sehen:

- **McFaddens R^2^**

$$R^2_{McF}=1-\frac{\ln\mathcal{L}_{\beta}}{\ln\mathcal{L}_0} \qquad \qquad \textsf{vgl.} \quad R^2=1-\frac{SSE}{SST} $$

Der Wertebereich liegt ebenfalls zwischen 0 und 1, wobei bei einem Wert von $0.4$ oder größer von einer guten Modellanpassung ausgegangen werden kann.^[Siehe z.B. Backhaus et al. (2011), S. 269 ff.]

### Pseudo-R^2^

\footnotesize

Andere davon abgeleitete Pseudo-R^2^ Statistiken berücksichtigen zusätzlich den Stichprobenumfang $n$.^[Siehe z.B. Backhaus et al (2011), S. 269 ff.; Menard (2010), S. 49 f.]

- **Cox & Snell R^2^**

$$R^2_{CS}=1-\left( \frac{\ln\mathcal{L}_{\beta}}{\ln\mathcal{L}_0} \right)^{\frac{2}{n}} $$

Der Wert 1 kann hier nicht erreicht werden.

- **Nagelkerkes R^2^**

$$R^2_{N}=\frac{R^2_{CS}}{1-\mathcal{L}^{\frac{2}{n}} _0} $$
Durch die Normierung mit der Maximalwert des $R^2_{CS}$ kann hier wieder 1 erreicht werden.

Für die Auswahl der Pseudo-R^2^ Statistiken sei auf die Literatur verwiesen. Berechnen können Sie diese mit der Funktion `PseudoR2(model)` aus dem Paket `BaylorEdPsych`. Hier werden noch einige weitere Pseudo-R^2^2 Statistiken ausgegeben.

### Übung `r nextExercise()`: Pseudo-R^2^ {.exercise type=essay .shrink}

1. Modellieren Sie mit dem `kreditausfall` Datensatz `ausfall` in Abhängigkeit von allen anderen Variablen.
2. Ersetzen Sie `alter` durch `log(alter)` und vergleichen die Modelle.
3. Warum wurde `log(alter)` erzeugt?
4. Bestimmen Sie die Pseudo-R^2^ Statistiken für das Modell aus 2 und interpretieren Sie $R^2_{McF}$.


## Signifikanz der Regressionskoeffizienten

### Wald-Test

Im `summary` des logistischen Regressionsmodells werden standardmäßig die Ergebnisse des Wald-Tests ausgegeben. Dieser überprüft (wie der t-Test in der linearen Regression), ob sich die Koeffizienten signifikant von 0 unterscheiden.

Die Prüfgröße $T$ ist (ähnlich wie in der linearen Regression) der Quotient aus Schätzer und Varianz des Schätzers:

$$T=\frac{\hat{\beta}}{var(\hat{\beta})}\sim \chi^2_{(1)} $$

Die Prüfgröße ist $\chi^2$-verteilt mit einem Freiheitsgrad. Im Gegensatz zum t-Test bei der linearen Regression wird ist keine Unterscheidung zwischen ein- und zweiseitigen Test möglich, hier wird immer nur die $H_0: \beta_k=0$ überprüft.

Kritisch anzumerken ist, dass Untersuchungen gezeigt haben, dass die Performance des Wald-Tests nicht sehr gut ist. Er verwirft die Nullhypothese oft nicht, obwohl der Regressionskoeffizient signifikant ist.^[Siehe z.B. Backhaus et al. (2011), S. 279 f.; Hosmer/Lemeshow (2000), S. 11 ff.]

### Likelihood-Quotienten-Test

Als Alternative bietet sich der Likelihood-Quotienten-Test an. Jetzt wird die Likelihood des Modells $\mathcal{L}_{\beta}$ nicht mit dem Nullmodell verglichen, sondern mit einem Modell, dass um eine Variable $k$ verringert wurde ($\mathcal{L}_k$).

$$T=-2 \cdot \frac{\ln \mathcal{L}_k}{\ln \mathcal{L}_{\beta}}\sim \chi^2_{(1)} $$

$T$ ist $\chi^2$-verteilt mit einem Freiheitsgrad^[Vgl. Likelihood-Quotienten-Test für das Gesamtmodell: Anzahl der Freiheitsgrade = Anzahl der unabhängigen Variablen, die Differenz in den Freiheitsgraden zwischen Nullmodell und dem eigentlichen Modell, wie hier die Differenz zwischen dem um eine Variable verringerten Modell.]

Sie können den Likelihood-Quotienten-Test zur Überprüfung der Signifikanz der Schätzers mit `lrtest(model, "variable")` aufrufen.


### Übung `r nextExercise()`: Likelihood-Quotienten-Test für einzelne Koeffizienten {.exercise type=essay .shrink}

Bestimmen Sie die Signifikanz des Schätzers für `log(alter)` aus dem Modell aus der vorherigen Übung mit Hilfe des Likelihood-Quotienten-Tests und vergleichen das Ergebnis mit dem des Wald-Tests.


## Regressionsdiagnostik

### Multikollinearität

Auch bei der logistischen Regression sollte eine Regressionsdiagnostik durchgeführt werden. Hier ein paar erste Hinweise:^[Siehe z.B. Field et al. (2012). S. 342 f.; Menard (2010), S. 107 ff., S. 125 ff.]

**Multikollinearität**

Das Auftreten von Multikollinearität kann wie in der linearen Regression mit dem Variance Inflation Factor überprüft werden (`vif(model)` aus dem Paket `car`).

Lösungsmöglichkeiten bei Auftreten von Multikollinearität:

- Eine Variable eliminieren, allerdings gibt es meist keine Hinweise, welche zu eliminieren ist.
- Unter Umständen hilft es, mehr Daten zu erfassen.
- Hauptkomponentenanalyse, um so die miteinander korrelierenden Variablen zusammenzufassen; dies erschwert aber möglicherweise die Interpretation.

### Übung `r nextExercise()`: Multikollinearität {.exercise type=essay .shrink}

Überprüfen Sie das Modell aus der vorherigen Übung auf Multikollinearität.

### Linearer Zusammenhang

Es muss überprüft werden, ob ein linearer Zusammenhang zwischen den unabhängigen Variablen und dem *logit* der abhängigen Variablen besteht.

Eine Möglichkeit zur Überprüfung eines möglichen nicht-linearen Zusammenhangs bietet die Box-Tidwell Transformation.

Der Box-Tidwell Ansatz fügt einen Term der Form $X\cdot \ln(X)$ zum Modell hinzu. Wenn der Koeffizient für diesen Term signifikant ist, gibt es einen nicht-linearen Zusammenhang zwischen dem $logit(Y)$ und $X$.

Der Exponent für $X$ ergibt sich aus $1$ plus dem Quotienten aus dem ursprünglichen Koeffizienten für $X$ und dem Koeffizienten für $X\cdot \ln(X)$  aus dem neuen Modell.

Da möglicherweise weiterhin nicht lineare Anteile verbleiben, kann durch Iteration auch der "optimale" Exponent gefunden werden.

### Linearer Zusammenhang

Überprüfung am Beispiel des Modells `ausfall ~ hoehe`

```{r}
# Box-Tidwell-Test auf Linearität
lintest <- glm(ausfall ~ hoehe + I(hoehe*log(hoehe)), family = binomial(logit), data = kreditausfall)
# Ausgabe p-Wert
coefficients(summary(lintest))[3,]

```

Der p-Wert < 0.05 deutet auf einen nicht-linearen Zusammenhang hin.

### Einflussreiche Beobachtungen

Einflussreiche Beobachtungen können über die sogenannnten *Devianz*- und *Pearson*-Residuen sowie die Änderungen in der Devianz oder im Pearson-$\chi^2$ identifiziert werden.^[Siehe z.B. Hedderich, Sachs (2016) S. 802 f.; Hosmer et al. (2013) S. 188 ff.]

- **Devianz-Residuen**

Diese ergeben sich aus der Differenz zum gesättigten Modell in den Einzelwahrscheinlichkeiten.

$$d_i=\sqrt{-2 \cdot \left(y_i \cdot \ln \frac{y_i}{n_i\hat{\pi}_i} \right) + (n_i-y_i) \cdot \ln \frac{n_i-y_i}{n_i \cdot(1-\hat{\pi}_i)}} $$

mit 

\begin{tabular}{llll}
& $n_i$ & & Anzahl der Beobachtungen, deren Kovariatenkombination mit der \\
&&& $i$-ten Beobachtung übereinstimmt  \\
& $\hat{\pi}_i$ & & geschätzte Wahrscheinlichkeit für die $i$-te Beobachtung  \\
\end{tabular}


### Einflussreiche Beobachtungen

- **Pearson-Residuen**

$$r_i=\frac{y_i-n_i\hat{\pi}_i}{\sqrt{n_i\hat{\pi}_i(1-\hat{\pi}_i)}} $$

Die Summen der Residuen sind jeweils näherungsweise $\chi^2$-verteilt mit $n-k-1$ Freiheitsgraden und können so als Gütemaß verwendet werden.

### Einflussreiche Beobachtungen

Unter Zuhilfenahme der Elemente der *Hat*-Matrix können die Änderungen in Pearson's $\chi^2$ und in der Devianz berechnet werden, wenn die jeweilige Beobachtung entfernt würde.

- **Delta-$\chi^2$**

$$\Delta \chi^2= \frac{r^2_j}{1-h_{ii}} $$

- **Delta-Devianz**

$$\Delta D_i=d^2_i+r^2_i \cdot \frac{h_{ii}}{1-h_{ii}} $$

mit

\begin{tabular}{llll}
& $h_{ii}$ & & Diagonalelemente der \textit{Hat}-Matrix \\
\end{tabular}

Ein Plot der $\chi^2$-Änderung oder der Devianz-Änderung $\Delta D_i$ gegen die geschätzten Wahrscheinlichkeiten zeigt besonders einflussreiche Beobachtungen.

### Einflussreiche Beobachtungen am Beispiel

Am Beispiel des Modell `ausfall ~ hoehe` wird die Berechnung der Residuen sowie der $\chi^2$- und Devianz-Änderungen gezeigt, sowie deren Plots.

```{r LogReg, eval = FALSE}
# Berechnungen
DevResid <- resid(glm1)
PearsonResid <- resid(glm1, type = "pearson")
hats <- hatvalues(glm1)
DeltaChiSq <- PearsonResid^2 / (1 - hats)
DeltaDev <- DevResid^2 + PearsonResid^2 * hats / (1 - hats)
ProbDach <- fitted(glm1)
# Plots
plot1 <- gf_point(DeltaChiSq ~ ProbDach, size = 0.75)
plot2 <- gf_point(DeltaDev ~ ProbDach, size = 0.75)
grid.arrange(plot1, plot2, nrow = 1)
```

### Einflussreiche Beobachtungen am Beispiel

\footnotesize

```{r LogReg, echo = FALSE, out.width = "60%", fig.align="center"}
```

Die Kurve von links oben nach rechts unten gehört zu Beobachtungen mit $y_i=1$, die von links unten nach rechts oben entsprechend zu Beobachtungen mit $y_i=0$.
Einflussreiche Beobachtungen würden sich am ehesten links oder rechts oben, abgesetzt von den anderen Werten zeigen. Insgesamt ist das Bild hier unauffällig.


## Klassifikationseigenschaften

### Konfusionsmatrix

Die Konfusionsmatrix stellt eine Kreuztabelle dar, in der die empirisch beobachteten Häufigkeiten für die Zielgröße mit den durch das logistische Regressionsmodell berechneten Häufigkeiten zusammen dargestellt werden:^[Siehe z.B. Backhaus et al (2011), S. 271.]

\begin{center}
\begin{tabular}{lcccc}
\\
\toprule
&&\multicolumn{3}{c}{Aus dem Modell} \\
&&\multicolumn{3}{c}{berechnete Häufigkeiten} \\
\midrule
&&& 0 & 1 \\
\midrule
Beobachtete & 0 && $n_{11}$ & $n_{12}$ \\
Häufigkeiten & 1 && $n_{21}$ & $n_{22}$ \\
\bottomrule
\\
\end{tabular}
\end{center}

Das logistische Regressionsmodell liefert Wahrscheinlichkeiten für eine $1$ in der Zielgröße. Daher müssen diese noch in $0$ oder $1$ umgewandelt werden, je nachdem, ob die Wahrscheinlichkeit z. B. kleiner oder größer $0.5$ ist. Diese Grenze wird auch als *cutpoint* bezeichnet.

### Prognostizierte Ausfälle

Es wird eine zusätzliche Variable `modelliert` angelegt, die die mit dem Modell prognostizierten Ausfälle enthält, hier am Beispiel des Modells `ausfall ~ hoehe`.

```{r}
cutpoint = 0.5
kreditausfall$modelliert <- (fitted(glm1) > cutpoint) * 1
```

Die Kreuztabelle kann mit `xtabs` angelegt werden.

```{r}
km <- xtabs(~ausfall + modelliert, data = kreditausfall)
km
```

### Auswertung der Konfusionsmatrix

\footnotesize

Verschiedene Werte können aus der Konfusionsmatrix berechnet werden: Missklassifikationsrate, Genauigkeit, Sensitivität (True Positive Rate, TPR), Spezifizität (True Negative Rate, TNR) und False Positive Rate (FPR).

```{r echo = FALSE}
library(knitr)
kable(km)
n = sum(km)
```

\begin{align*}
\textsf{Missklassifikationsrate} & = \frac{n_{12}+n_{21}}{n} = \frac{`r km[1,2]`+ `r km[2,1]`}{`r n`}  = `r round((km[1,2]+km[2,1])/n,2)` \\
\textsf{Genauigkeit} & =\frac{n_{11}+n_{22}}{n} = \frac{`r km[1,1]`+ `r km[2,2]`}{`r n`} = `r round((km[1,1]+km[2,2])/n,2)` = 1-\textsf{Missklassifikationsrate} \\
\textsf{TPR} & = \frac{n_{22}}{n_{21}+n_{22}} = \frac{`r km[2,2]`}{`r km[2,1]` + `r km[2,2]`}  = `r round(km[2,2]/(km[2,1] + km[2,2]),2)` \\
\textsf{TNR} & = \frac{n_{11}}{n_{11}+n_{12}} = \frac{`r km[1,1]`}{`r km[1,1]` + `r km[1,2]`} = `r round(km[1,1]/(km[1,1] + km[1,2]),2)`  \\
\textsf{FPR} & =\frac{n_{12}}{n_{11}+n_{12}} = \frac{`r km[1,2]`}{`r km[1,1]` + `r km[1,2]`} = `r round(km[1,2]/(km[1,1] + km[1,2]),2)` = 1-\textsf{Spezifität} 
\end{align*}

### Bewertung der Kennzahlen

Ein logistisches Regressionmodell ist gut, wenn im Vergleich zu einer rein zufälligen Zuordnung \dots

- die Missklassifikationsrate gering ist,
- die Genauigkeit groß ist,
- die Sensitivität (True Positive Rate) groß ist,
- die Spezifität groß ist,
- die False Positive Rate gering ist.

**Hinweis:** Die Gütekriterien werden überschätzt (Genauigkeit, TPR, Spezifität) bzw. unterschätzt (Missklassifikationsrate und FPR), wenn sie aus derselben Stichprobe wie die Regressionsparameter berechnet werden. Daher sollte die Stichprobe (sofern sie insgesamt groß genug ist) geteilt werden. Aus der einen Teilstichprobe werden die Modellparameter $\beta_0, \beta_1, \dots, \beta_k$ geschätzt, aus der anderen Teilstichprobe die Gütekriterien berechnet.^[Siehe z.B. Backhaus et al (2011), S. 273; Krafft (1997), S. 631.]

### Übung `r nextExercise()`: Konfusionsmatrix und Klassifikationseigenschaften {.exercise type=essay .shrink}

Berechnen Sie für das Modell aus der vorherigen Übung die Konfusionsmatrix und die daraus resultierenden Kennzahlen und interpretieren diese.



### Liftwerte und Liftkurve

Liftwerte setzen die modellierte Wahrscheinlichkeit für eine $1$ in der Zielgröße ins Verhältnis zum Anteil der $1$ an der Gesamtzahl der Beobachtungen.

Dazu werden die modellierten Wahrscheinlichkeiten der Größe nach geordnet. Dann werden die Liftwerte aus den Perzentilen der geordneten modellierten Wahrscheinlichkeiten abgeleitet.

Beispiel Kreditausfallprognose: Eine Bank hat eine durchschnittliche Kreditausfallquote von 10$\,$%. Nach einer logistischen Regressionsmodellierung erhält die Bank am 90$\,$%-Perzentil eine modellierte Kreditausfallwahrscheinlichkeit von 25$\,$% und am 95$\,$%-Perzentil eine modellierte Kreditausfallwahrscheinlichkeit von 30$\,$%.

Dann ist der 10$\,$%-Lift $=\frac{25\,\%}{10\,\%}=2.5$ und der 5$\,$%-Lift $=\frac{30\,\%}{10\,\%}=3.0$ 

Interpretation 10$\,$%-Lift: Bei den 10$\,$% Krediten mit den höchsten Ausfallwahrscheinlichkeiten ist das Risiko eines Kreditausfalls um den Faktor 2.5 höher als im Durchschnitt über alle Kredite und entsprechend für den 5$\,$%-Lift um den Faktor 3.

### Liftkurve

\footnotesize

Mit der Funktion `performance` aus dem Paket `ROCR` kann eine Liftkurve geplottet werden. Dazu wird ein `prediction`-Objekt benötigt.

```{r eval = FALSE}
library(ROCR)
# erstes Argument: modellierte Wahrscheinlichkeite, zweites Argument: tatsächliche Ausprägungen
pred <- prediction(fitted(glm1), kreditausfall$ausfall)
# Das Perfomance Objekt enthält die Liftwerte
lift <- performance(pred, "lift", "rpp")
# Durchschnittlicher Liftwert
avglift = mean(lift@y.values[[1]], na.rm = TRUE)
# Plot
gf_line(lift@y.values[[1]] ~ lift@x.values[[1]], 
        xlab = "Rate of positive predictions", ylab = "Lift values") %>% 
  gf_hline(yintercept = avglift)
# Hinweis: eine automatische Plot-Variante geht auch mit
# plot(lift)
# abline(h = avglift)
```

### Liftkurve

\footnotesize

```{r echo = FALSE, fig.align="center", out.width="50%"}
library(ROCR)
# erstes Argument: modellierte Wahrscheinlichkeite, zweites Argument: tatsächliche Ausprägungen
pred <- prediction(fitted(glm1), kreditausfall$ausfall)
# Das Perfomance Objekt enthält die Liftwerte
lift <- performance(pred, "lift", "rpp")
# Durchschnittlicher Liftwert
avglift = mean(lift@y.values[[1]], na.rm = TRUE)
# Plot
gf_line(lift@y.values[[1]] ~ lift@x.values[[1]], 
        xlab = "Rate of positive predictions", ylab = "Lift values") %>% 
  gf_hline(yintercept = avglift) %>% 
  gf_text(I(avglift+0.18) ~ -0.03, hjust = 0, label = "Überdurch-\nschnittliche\nWerte", size = 6, color = "blue") %>% 
  gf_text(I(avglift-0.1) ~ 0.8, vjust = 0, label = "Durchschnittlicher Lift", size = 6, color = "blue")
```

Ein Modell ist dann gut, wenn es hohe Werte z. B. für den 5$\,$%- oder 10$\,$%-Lift und einen niedrigen Anteil überdurchschnittlicher Werte hat. Hier beträgt der 10$\,$%-Lift beträgt ca. `r round(lift@y.values[[1]][which(lift@x.values[[1]]== 0.1)],2)`, ist also recht niedrig und der Anteil der überdurchschnittlichen Werte ist auch sehr hoch.

**Hinweis:** Liftwerte hängen von dem zugrunde liegenden Durchschnitt ab, sind also nicht für verschiedene Grundgesamtheiten vergleichbar.


### Liftkurve

Im vorliegen Fall liegt der Durchschnittswert bei `r round(avglift,2)`.

Der Anteil der überdurchschnittlichen Werte beträgt:

```{r}
sum(lift@y.values[[1]]>avglift, na.rm = TRUE)/length(lift@y.values[[1]])
```

Beide Werte deuten eher auf eine mangelnde Modellgüte hin.

Die Liftwerte für einzelne Quantile erhalten Sie aus dem Perfomance-Objekt:

```{r}
# 5%-Lift
idx = which(lift@x.values[[1]]== 0.05)
lift@y.values[[1]][idx]
# 10% Lift
idx = which(lift@x.values[[1]]== 0.1)
lift@y.values[[1]][idx]
```


### ROC-Kurve

Die ROC-Kurve (ROC = Receiver Operating Characteristic) verallgemeinert die Konfusionsmatrix. Bei der Konfusionsmatrix wird von einem festen Cutpoint ausgegangen, d. h., bei einem Cutpoint von 50$\,$% werden modellierte Wahrscheinlichkeiten größer als 50$\,$% der Ausprägung $1$ zugeordnet und modellierte Wahrscheinlichkeiten kleiner 50$\,$% werden der Ausprägung $0$ zugeordnet.

Für die ROC-Kurve wird dieser Cutpoint variiert. Für alle möglichen Cutpoints zwischen 0$\,$% und 100$\,$% werdem die modellierten Wahrscheinlichkeiten den Ausprägungen $0$ und $1$ zugeordnet. Für jeden Cutpoint werden die True Positive Rate (Sensitivität) und die False Positive Rate (1$\,$--$\,$Spezifität) bestimmt und in ein Koordinatensystem abgetragen.

Für einen Cutpoint von 0$\,$% ist TPR = 1 und FPR = 1. Für einen Cutpoint von 100$\,$% ist TPR = 0 und FPR = 0.

### ROC-Kurve

\footnotesize

```{r out.width = "60%", fig.align="center"}
library(ggfortify)
# ROC Kurve
roc <- performance(pred, "tpr", "fpr") 
autoplot(roc) %>% gf_labs(title = "")
```

### AUC-Wert

Die Qualität der Klassifikationeigenschaft lässt sich über den AUC-Wert bestimmen.

```{r echo = FALSE, out.width = "60%", fig.align="center", fig.asp = 1}
gf_line(roc@y.values[[1]] ~ roc@x.values[[1]], xlab = "False positive rate", ylab = "True positive rate", color = "blue") %>%
  gf_area(fill = "lightblue", alpha = 0.3) %>%
  gf_segment(0 + 1 ~ 0 + 1, color = "grey") %>%
  gf_text(5/16 ~ 9/16, label = "AUC", vjust = 0.5, hjust = 0.5, size = 5, color = "blue")
```

Der AUC-Wert ist die Fläche unter der ROC-Kurve. AUC liegt im Bereich von $0.5$ (Winkelhalbierende, das Modell erklärt gar nichts) bis $1$.^[AUC und Gini-Koeffizient (Konzentrationsmaß) hängen zusammen: $\textsf{Gini}=2 \cdot AUC - 1$.]

```{r}
performance(pred,"auc")@y.values[[1]]
```

### AUC-Wert

Aus dem AUC-Wert lässt sich die Güte der Klassifikation abschätzen:^[Vgl. Hosmer/Lemeshow (2000), S. 162.]

\begin{tabular}{ll}
\\
\toprule
Bereich & Qualität \\
\midrule
$0.5 \le \textsf{AUC-Wert}<0.7$ & Schlechte Modellgüte \\
$0.7 \le \textsf{AUC-Wert}<0.8$	& Akzeptable Modellgüte \\
$0.8 \le \textsf{AUC-Wert}<0.9$ &	Exzellente Modellgüte \\
$\textsf{AUC-Wert} \ge 0.9$ & Außerordentliche Modellgüte \\
\bottomrule
\\
\end{tabular}

Im Beispiel zeigt sich mangelhafte Modellgüte.

In der Regel werden Werte $> 0.9$ nicht erreicht.

**Hinweis:** Im Gegensatz zum Lift sind AUC-Werte verschiedener Modelle miteinander vergleichbar, auch mit verschiedenen Grundgesamtheiten.

### Optimaler Cutpoint

Aus der ROC-Kurve lässt sich prinzipiell der optimale Cutpoint ablesen: Das ist der Punkt, der am nächsten zu TPR = 1 und FPR = 0 liegt.

Mit Hilfe der Performance-Daten lässt sich dieser ermitteln:

```{r}
cost.perf = performance(pred, "cost")
pred@cutoffs[[1]][which.min(cost.perf@y.values[[1]])]
```


### Übung `r nextExercise()`:  Liftwerte, Liftkurve, ROC-Kurve und AUC {.exercise type=essay .shrink}

1. Plotten Sie für das Modell aus der vorherigen Übung die Liftkurve einschließlich des durchschnittlichen Wertes.
2. Bestimmen Sie den Anteil der Lift-Werte, die über dem Durchschnitt liegen.
3. Bestimmen Sie den 5$\,$%- und den 10$\,$%-Lift.
4. Plotten Sie die ROC-Kurve.
5. Bestimmen und bewerten Sie AUC.
6. Ermitteln Sie den optimalen Cutpoint.


### Übung `r nextExercise()`: Dividendenausschüttung {.exercise type=essay .shrink}

\footnotesize

Der Datensatz `DividendPayout` (hier ein Teilmenge der ursprünglichen Daten, vgl. Rojahn/Lübke (2014), Betriebswirtschaftliche Forschung und Praxis, 66 (6), S. 636-651.) untersucht die Frage, welche firmenspezifischen Determinanten auf die Dividendenausschüttungswahrscheinlichkeit wirken.

- Abhängige Variable: `Payout` -- Dividenzahlung im aktuellen Jahr (1 – ja, 0 – nein)
- Unabhängige Variablen (jeweils aus dem Vorjahr): `ROA` -- Gesamtkapitalrentabilität, `FCF` -- Free Cash-Flow im Verhältnis zum Umsatz, `Growth` -- Umsatzwachstumsrate, `FKQ` -- Fremdkapitalquote, `Cash` -- Liquide Mittel relativ zur Bilanzsumme, `Size1` -- Bilanzsumme, `Size2` -- Umsatzerlöse, `History` -- Höhe der Dividendenzahlung

1. Wie hoch war der Anteil der Unternehmen, die keine Dividende ausgeschüttet haben?
2. Bestimmen Sie in einem geeignetem logistischen Regressionmodell die Einflussgrößen auf die Dividendenzahlung.
3. Berechnen Sie die Odds Ratios. Können die Odds Ratios als Näherung für die relativen Wahrscheinlichkeiten für eine Dividendenzahlung verwendet werden?
4. Beurteilen Sie die Klassifikationseigenschaften des Modells.

```{r finish-Oeko-LogistischeRegression, include=FALSE}
rm(pathToImages)
finalizePart(partname)
```


 

