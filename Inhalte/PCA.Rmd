```{r setup-PCA, include=FALSE}
# ---------------------------------------------------------------------------
#% maintainer:
#%   - Karsten Luebke
#%
# ---------------------------------------------------------------------------
source("../prelude.R")
initPart(
    "PCA",            # Dateiname ohne Suffix
    "Clusterananlyse"   # Verzeichnisname im Bilderverzeichnis 
    )
pathToImages <- getPathToImages()
# ---------------------------------------------------------------------------

library(mosaic)
library(corrplot)
library(psych)

tips <- assertData("tips.csv", "https://goo.gl/whKjnl")
```
# PCA


### Lernziele {exclude-only=NOlernziele}


Die Studierenden ...

- wissen, was *Unüberwachtes* Lernen in diesem Zusammenhang bedeutet.
- können die Hauptidee der Hauptkomponentenanalyse (PCA) erläutern.
- wissen, wozu ein *Screeplot* verwendet wird.
- wissen, wie man eine PCA ein R berechnet.
- wissen, wozu ein *Biplot*  verwendet wird.
- wissen, dass *Cronbachs Alpha* eine Kennzahl, um die Güte einer PCA-Lösung zu quantifizieren.




### Unüberwachtes Lernen

**Unüberwachtes Lernen** (engl.: unsupervised learning): Es gibt *keine* bekannte abhängige Variable $y$, die modelliert werden soll

Methoden (u. a.):

- **Hauptkomponentenanalyse** (engl.: Principal Component Analysis): Finde (wenige) Linearkombinationen der Variablen: Zusammenfassung von Variablen, Dimensionsreduktion. `prcomp()`^[Mächtige Alternative: Funktion `prinicpal()` aus dem Paket `psych`]
- **Clusteranalyse** (engl.: Cluster Analysis): Finde Gruppen (Cluster) von Beobachtungen, die innerhalb der Cluster homogen, zwischen den Clustern heterogen sind^[Clustern von Variablen analog]. `hclust()`, `kmeans()`


### Hauptkomponentenanalyse

**Idee**: Fasse korrelierte Variablen (linear) zusammen. Die resultierenden Komponenten sind unkorreliert und beinhalten einen möglichst großen Anteil der (multivariaten) Gesamtvariation.

Z. B. für die Hauptkomponente $1$: $$z_{i1}=\lambda_{11} \tilde{x}_{i1}  + \lambda_{12} \tilde{x}_{i2}  + \ldots + \lambda_{1p} \tilde{x}_{ip}$$
mit den Ladungen $\lambda$ und den zentrierten ($\bar{\tilde{x}}=0$) und ggfs. skalierten ($sd_{\tilde{x}}=1$) Merkmalen. $z_{i1}$: Score von Beobachtung $i$ auf der 1. Hauptkomponente.

### Beispiel 

Zusammenfassung der Multi-Item Likert Skala zu: *Leichtigkeit beim Verständnis von Statistik*:^[Auf einer Skala von 1 (Trifft überhaupt nicht zu) über 4 (Weder zutreffend, noch unzutreffend) bis 7 (Trifft voll und ganz zu) [Candace Schau: Survey of Attitudes Toward Statistics, SATS-36](http://www.evaluationandstatistics.com/)]

- Statistische Formeln sind leicht zu verstehen.
- Statistik ist ein kompliziertes Fach.*
- Statistik ist ein Fach, das die meisten Menschen schnell lernen.
- Das Lernen von Statistik erfordert sehr viel Disziplin.*
- Statistik beinhaltet sehr umfangreiche Rechnungen.*
- Statistik ist eine sehr technische Materie.*
- Die meisten Menschen müssen lernen anders zu denken, um Statistik anwenden zu können.^[Die Items mit Sternchen \* sind sogenannte inverse Items, bei denen die Zustimmung eine höhere Schwierigkeit im Umgang mit Statistik bedeutet.]

### Multivariate Streuung

```{r, echo=FALSE, fig.align="center", out.width="80%"}
data("iris")
iris.s <- iris[,3:4] %>%
  scale() %>%
  data.frame()
pca.iris <- prcomp(iris.s)

plot(iris.s, xlab="x", ylab="y", main="Richtung der größten Streuung")
arrows(0, 0, pca.iris$rotation[1,1],  pca.iris$rotation[2,1], lwd=2, col="#00998a")
```

### Übung `r nextExercise()`: Multivariate Streuung {.exercise type=yesno answer=no}

Senkrecht (d. h. unkorreliert) zur eingezeichneten ersten Hauptkomponente liegt die zweite. Stimmt die Aussage: Die Streuung der Daten in Richtung der zweiten Hauptkomponente ist größer als in Richtung der ersten?^[Es gibt maximal so viele Hauptkomponenten wie Variablen.]

- Ja.
- Nein.

<div class="notes">
***Nein***: Die zweite Hauptkomponente ist unkorreliert zur ersten -- und beinhaltet weniger Variation. Sie steht senkrecht zur ersten, und dort streuen die Daten weniger.
```{r,  echo=FALSE, fig.align="center", out.width="50%"}
plot(iris.s, xlab="x", ylab="y", main="Richtung der Streuungen", asp=1)
arrows(0, 0, pca.iris$rotation[1,1]*pca.iris$sdev[1],  pca.iris$rotation[2,1]*pca.iris$sdev[1], lwd=1.5, col="#00998a")
arrows(0, 0, pca.iris$rotation[1,2]*pca.iris$sdev[2],  pca.iris$rotation[2,2]*pca.iris$sdev[2], lwd=1.5, col="#00998a")
```
</div>


### Übung `r nextExercise()`: Score {.exercise type=yesno answer=yes}

```{r, echo=FALSE, fig.align="center", out.width="50%"}
data("iris")
iris.s <- iris[,3:4] %>%
  scale() %>%
  data.frame()

plot(iris.s, xlab="x", ylab="y", main="Richtung der größten Streuung")
points(iris.s[121,], col = "red", pch = 19, cex = 1.5)

pca.iris <- prcomp(iris.s)
arrows(0, 0, pca.iris$rotation[1,1],  pca.iris$rotation[2,1], lwd=2, col="#00998a")
```

Die rot markierte Beobachtung: Hat diese auf der ersten Hauptkomponente einen hohen Wert (Score)?

- Ja.
- Nein.

<div class="notes">
***Ja***, in Pfeilrichtung ist die Beobachtung *vorne*.
</div>


### Vorbereitung: Trinkgelddaten

Einlesen der "Tipping"^[Bryant, P. G. and Smith, M (1995) Practical Data Analysis: Case Studies in Business Statistics. Homewood, IL: Richard D. Irwin Publishing] Daten sowie laden des Pakets `mosaic`.

```{r, eval= FALSE, message=FALSE}
download.file("https://goo.gl/whKjnl", destfile = "tips.csv")
tips <- read.csv2("tips.csv")
# Alternativ - heruntergeladene Datei einlesen:
# tips <- read.csv2(file.choose()) 

library(mosaic) # Paket laden
```


### Vorbereitung Installation Zusatzpakete

Einmalige Installation von Zusatzpaketen

```{r, eval=FALSE}
install.packages(c("corrplot", "psych"))
```


### Vorbereitung: Skalierung numerischer Variablen

```{r, eval=FALSE}
tipsscale <- tips %>% 
  select(size, total_bill, tip) %>% # metrische Variablen wählen
  scale() %>%                       # Skalieren 
  data.frame()                      # Als Datensatz definieren
```

```{r, echo=FALSE}
tipsscale <- tips %>% 
  dplyr::select(size, total_bill, tip) %>% # Variablen wählen
  scale() %>%                              # Skalieren 
  data.frame()                             # Als Datensatz definieren
```


### Übung `r nextExercise()`: Korrelation/ Kovarianz {.exercise type=yesno answer=yes}

Stimmt die Aussage: Bei skalierten/standardisierten^[d. h., $\bar{x}=0, sd=1$] Variablen ist die Kovarianzmatrix identisch zu der Korrelationsmatrix?

- Ja.
- Nein.

<div class="notes">
***Ja***, der Korrelationskoefizient normiert die Kovarianz durch das Produkt der Standardabweichungen. Sind diese $1$, so gilt `cov(x)==cor(x)`
</div>


### Korrelation

```{r  fig.align="center", out.width="40%"}
library(corrplot) # ggfs. einmalig vorab installieren
cor(tipsscale) %>%
  corrplot()
```


### Übung `r nextExercise()`: Korrelation {.exercise type=A-B-C-D answer=A}

Welche der folgenden Aussagen stimmt?

A.  Die Variable `size` korreliert positiv mit allen anderen Variablen.
B.  Die höchste Korrelation gibt es zwischen `size` und `tip`.
C.  Die niedrigste Korrelation gibt es zwischen `total_bill` und `tip`.
D.  Es gibt nur negative Korrelation.

<div class="notes">
Alle Korrelationen sind *blau*, also ist *D* falsch und ***A*** richtig. *B* und *C* falsch, da die Korrelation zwischen `tip` und `total_bill` (und damit nicht `size`) die höchste ist: Der Kreis ist größer und die Farbe intensiver. (Beachte: *Korrelationen* innerhalb einer Variable (Diagonale, z.B. `cor(tip ~ tip)`) sind immer $=1$ und uninterssant.)

`r kable(cor(tipsscale))`  
</div>


### Hauptkomponentenanalyse mit R

```{r}
ergpca <- prcomp( ~ size + total_bill + tip, 
                  data=tipsscale)
```


### Screeplot tipsscale

Ein **Screeplot** stellt die Varianz ($y$-Achse) der Hauptkomponenten ($x$-Achse) dar.^[Ein Kriterium zur Bestimmung der Anzahl Hauptkomponenten ist: Nimm die Anzahl, die links der Knickstelle liegt (hier bei $2$), also eine Hauptkomponente.]
```{r  fig.align="center", out.width="60%"}
plot(ergpca, type="l")
```


### Übung `r nextExercise()`: Screeplot {.exercise type=A-B-C answer=C}

Welche der folgenden Aussagen stimmt?

A.  Alle Hauptkomponenten haben in etwa die gleiche Varianz.
B.  Erst bei der $k=3$ Hauptkomponente gibt es einen Abfall in der Varianz.
C.  Die erste Hauptkomponente hat eine deutlich höhere Varianz als die anderen.

<div class="notes">
Nur die Varianz der ersten Hauptkomponente ist $>1$, also ***C***.
</div>


### Multivariate Varianz

```{r}
summary(ergpca)
```

### Übung `r nextExercise()`: Multivariate Varianz {.exercise type=A-B-C answer=A}

Welche der folgenden Aussagen stimmt?

A.  Die erste Hauptkomponente enthält mehr als die Hälfte der Gesamtvarianz.
B.  Die ersten zwei Hauptkomponenten haben eine Varianz größer als $1$.
C.  Die Varianz der Hauptkomponenten nimmt zu.

<div class="notes">
`Cumulative Proportion 0.7263`$>0.5$, daher ***A***. *B* ist falsch, da nur bei der ersten Komponente gilt: `Standard deviation` $>1$ (und damit auch Varianz), *C*, weil die Varianz abnimmt.
</div>


### tipsscale: Biplot

Ein **Biplot** visualisiert die Ladungen der Variablen sowie die Werte der Beobachtungen auf den ersten Hauptkomponenten (*Scores*).

```{r  fig.align="center", out.width="40%"}
biplot(ergpca)
```


### Übung `r nextExercise()`: Biplot {.exercise type=A-B-C answer=A}

Welche der folgenden Aussagen stimmt?

A.  Beobachtung `171` zahlt eine hohe Rechnung.
B.  Beobachtung `171` sind viele Personen.
C.  Weiß nicht.

<div class="notes">
Beobachtung `171` liegt rechts unten. D. h. in Richtung der Variablen `tip` und `total_bill` ein hoher Wert, in Richtung der Variablen `size` eher durchschnittlich, daher ***A***.

`r kable(round(tipsscale[171,],2))`

Unskaliert:

`r kable(tips[171, c("size", "total_bill", "tip")])`

</div>


### Ladungen

Die Ladungen geben das Gewicht $\lambda$ der einzelnen Variablen für die jeweilige Hauptkomponente an. 
```{r}
ergpca$rotation[,1:2]
```

### Übung `r nextExercise()`: Ladungen {.exercise type=A-B-C answer=B}

Welche der folgenden Aussagen stimmt?

A.  Die Variable `total_bill` ist für die erste Hauptkomponente unwichtig.
B.  Die Variablen `total_bill` und `tip` haben auf der zweiten Hauptkomponente eine andere Richtung als die Variable `size`.
C.  Je höher der Wert für `tip` ist, desto höher ist der Wert auf der zweiten Hauptkomponente (Score).

<div class="notes">
*A* stimmt nicht, da die Ladung relativ hoch ist. *C* stimmt nicht, da das Vorzeichen negativ ist, d. h., je größer `tip` ist, desto geringer ist der Score. ***B*** stimmt, da die Ladung für `size`$>0$ ist, für `tip` und `total_bill`$<0$.
</div>



### Cronbachs Alpha 

**Cronbachs Alpha** ist eine Maßzahl für die **interne Konsistenz** einer Skala (*Reliabilität*). Sollte i.d.R. $>0.7$ sein.^[Wird i.d.R. für sog. Multi-Item Skalen verwendet. Hier **nicht** angebracht, da keine Multi-Item Skala vorliegt!]

```{r echo=TRUE, eval=FALSE}
library(psych) # ggfs. einmalig vorab installieren
ca <- alpha(tipsscale, check.keys = TRUE)
summary(ca)
```

::: {.small}
```{r echo=FALSE, eval=TRUE}
library(psych) # ggfs. einmalig vorab installieren
ca <- psych::alpha(tipsscale, check.keys = TRUE)
summary(ca)
```
:::

### Übung `r nextExercise()`: Cronbachs Alpha {.exercise type=yesno answer=yes}

Wäre eine interne Konsistenz der "Skala" hier akzeptabel?

- Ja.
- Nein.

<div class="notes">
***Ja***, da `raw_alpha` $>0.7$.

Aber: **Cronbachs Alpha** ist eine Maßzahl für die **interne Konsistenz** einer Skala (*Reliabilität*). Da keine Multi-Item Skala vorliegt, hier nicht sinnvoll.
</div>


### Offene Übung `r nextExercise()`: Dimensionsreduktion {.exercise type=essay}

Lassen sich die Variablen `size` und `total_bill` zu einer zusammenfassen?

```{r,include=FALSE}
tipsscaleUeb <- tips %>% 
  dplyr::select(size, total_bill) %>% # Variablen wählen
  scale() %>%                         # Skalieren 
  data.frame()                        # Als Datensatz definieren

pcaUeb <- prcomp( ~. ,  data=tipsscaleUeb)
summary(pcaUeb)
```


<div class="notes">
Eine Hauptkomponentenanalyse ergibt, dass die erste Hauptkomponenten mehr als $75\,\%$ der Gesamtvarianz beinhaltet.

`tipsscaleUeb <- tips %>% select(size, total_bill) %>% scale() %>% data.frame()`  
`pcaUeb <- prcomp( ~. ,  data=tipsscaleUeb)` 
`summary(pcaUeb)`
</div>

```{r child = './useR/useR-PCA.Rmd', eval = showuseR}
```



```{r finish-PCA, include=FALSE}
rm(pathToImages)

detach("package:corrplot", unload = TRUE)
detach("package:psych", unload = TRUE)
finalizePart(partname)
```
