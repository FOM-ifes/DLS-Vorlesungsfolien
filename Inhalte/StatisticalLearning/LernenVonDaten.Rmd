
```{r setup-LernenVonDaten, include=FALSE}
# ---------------------------------------------------------------------------
#% maintainer:
#%   - Norman Markgraf
#%
# ---------------------------------------------------------------------------
source("../../prelude.R")
initPart(
    "LernenVonDaten",  # Dateiname ohne Suffix
    "StatisticalLearning"      # Verzeichnisname im Bilderverzeichnis 
    )
pathToImages = getPathToImages()
# ---------------------------------------------------------------------------


```





```{r libs-lernenvondaten, include = FALSE}
library(mosaic)
library(ISLR)
library(here)
library(gridExtra)
library(caret)
library(tidyverse)
library(kableExtra)
```








# Aus Daten lernen


## Grundlagen

### Definitionen^[Dieses Kapitel orientiert sich an James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). *An introduction to statistical learning*. New York City: Springer.]


- [Lernen:]{.cemph} Aufbau einer Repräsentation, wobei neue Informationen in ein semantisches Netzwerk eingeknüpft werden, so dass eine relativ konsistente Änderung des Verhaltenspotenzials realisiert wird. Lernen basiert auf der Aufnahme von Daten aus der Umwelt (d.h. auf Erfahrung).

- [Informationen:]{.cemph} Angaben über Sachverhalte und Vorgänge. Informationen können mit Hilfe von **Relationen** und einer geordneten Menge von Argumenten repräsentiert werden. Beispiel: Der Satz "Anna ist die Mutter von Berta" könnte repräsentiert werden als `mutter(Anna, Berta)`.

- [Daten:]{.cemph} Zum Zweck der Verarbeitung zusammengefasste Zeichen, die aufgrund bekannter oder unterstellter Abmachungen Informationen darstellen.

- [Lernen aus Daten]{.cemph} verweist auf ein Methodenrepertoire, um (menschliches) Wissen aufzubauen oder Entscheidungen zu unterstützen (z.B. durch präzise Vorhersagen).



### Beispiele zu Anwendungen aus dem maschinellen Lernen 


- Autonomes Fahren
- Spracherkennung (Siri, Alexa, ...)
- Produktempfehlung ("Kunden, die sich für dieses Produkt interessiert haben, kauften auch ...").
- Schrifterkennung bei der Post
- Personalisierte Werbung
- Maschinenwartung (predictive maintenance)

Aber auch:

- Identitätserkennung am Gang (Regierung von China)
- [Tippgeräusche verraten Inhalte des Getippten](https://people.eecs.berkeley.edu/~tygar/papers/Keyboard_Acoustic_Emanations_Revisited/preprint.pdf) 
- Wahlwerbung personalisiert auf die Persönlichkeit (Cambridge Analytica?)
- ... ? 


### Einführendes Beispiel

Gegeben sei ein Datensatz zu Verkaufszahlen eines Produkts in 200 Märkten und zu den Werbebudgets in den drei Medien TV, Radio und Zeitung. Ein Nutzer kann sich z.B. Fragen, ob und wie stark die Verkaufszahlen vom TV-Werbebudget abhängen.^[Datensatz `Advertising`; Quelle: http://faculty.marshall.usc.edu/gareth-james/ISL/data.html]


```{r plot-advertising, echo = FALSE, out.width = "70%", fig.asp=.3}
Advertising <- read.csv(paste0(here::here(), "/datasets/Advertising.csv"))

p1 <- gf_point(sales ~ TV, data = Advertising) %>% gf_lm()
p2 <- gf_point(sales ~ radio, data = Advertising) %>% gf_lm()
p3 <- gf_point(sales ~ newspaper, data = Advertising) %>% gf_lm()

grid.arrange(p1, p2, p3, nrow = 1)
```

- In diesem Beispiel sind die Werbebudgets *Eingabevariablen* und die Verkaufszahlen die *Ausgabevariable*.
- Eingabevariablen werden auch als *Prädiktoren*, *Features* oder *unabhängige Variablen* bezeichnet.
- Ausgabevariablen werden auch als *abhängige Variable*, *Ergebnis-* oder *Responsevariable* bezeichnet.


### Grundlegende Funktion im Lernen aus Daten


- Sei $Y$ eine quantitative Ergebnisvariable und $X_1, X_2, \dots, X_p$ seien $p$ Prädiktorvariablen.

- Gibt es eine Abhängigkeit zwischen $Y$ und $X = (X_1, X_2, \dots, X_p)$, so kann man diese der folgenden Form darstellen:

$$Y = f(x) + \epsilon$$


- Dabei ist $f$ eine fixe, aber unbekannte Funktion von $X_1, X_2, \dots X_p$ und $\epsilon$ ist ein zufälliger Fehlerterm, der unabhängig von $X$ ist und einen Mittelwert von Null aufweist.

- $f$ repräsentiert die systematische Information von $X$ über $Y$.

- $f$ kann mehr als eine Prädiktorvariable beinhalten.

- Ziel ist es, die unbekannte Funktion $f$ zu schätzen.


### $f$ schätzen zur Vorhersage

Es gibt zwei typische Gründe, um $f$ zu schätzen: [Vorhersage]{.cemph} und [Erklärung]{.cemph}.

Ist $X$ bekannt, $Y$ aber nicht, so kann man $Y$ [vorhersagen]{.cemph}: $\hat{Y}=\hat{f}(X)$, wobei $\hat{f}$ die Schätzung für $f$ darstellt und $\hat{Y}$ die Vorhersage für $Y$. $\hat{f}$ wird in diesem Fall oft als *black box* verstanden, d.h. die genaue Form von $\hat{f}$ ist nicht von Interesse, sondern die möglichst exakte Vorhersage für $Y$ zählt.

Die Genauigkeit der Vorhersage $\hat{Y}$ hängt von zwei Größen ab:

- der *reduzierbare* Fehler: Durch Wahl geeigneter Verfahren kann $\hat{f}$ evtl. genauer geschätzt werden
- der *nicht reduzierbare* Fehler: Es kann sein, dass uns Informationen fehlen, die für eine genaue Vorhersage nötig wären. Formal: $Y=f(x)+\epsilon$ und $\epsilon$ ist per Definition nicht von $X$ vorhersagbar.


### Kostenfunktion 

- Ziel einer Vorhersage ist es, den *reduzierbaren Fehler* zu minimieren.

- Häufig wird dazu diese Funktion (Kostenfunktion) minimiert:

$$E(Y-\hat{Y})^2 = E[f(X) + \epsilon - \hat{f}(X)]^2
                 = \underbrace{[f(X) - \hat{f}(X)]^2}_\text{reduzierbar} + \underbrace{Var(\epsilon)}_\text{nicht reduzierbar}$$
                 
                 
                 
- $E$ steht für den *erwarteten Wert* der quadrierten Differenz zwischen vorhergesagtem ($\hat{Y}$) und tatsächlichem Wert ($Y$).

- $Var(\epsilon)$ steht für die Streuung (Varianz), die mit dem Fehlerterm $\epsilon$ einhergeht.

- $\epsilon$ stellt eine obere Schranke für die Genauigkeit einer Schätzung dar. Die Höhe ist i.d.R. unbekannt.

- $X$ ist ein Vektor an Prädiktorvariablen.

- $\hat{f}$ ist die anhand der Daten geschätzte Funktion $f$.



### Illustration zum $MSE$


```{r resids-plot, out.width = "70%", fig.asp = .5, echo = FALSE, results = "hold"}

set.seed(42)
N      <- 100
beta   <- 0.4
intercept <- 1


sim <- tibble(
  x = rnorm(N),
  error1 = rnorm(N, mean = 0, sd = .5),
  error2 = rnorm(N, mean = 0, sd = 2),
  y1 = intercept + x*beta + error1,
  y2 = intercept + x*beta + error2,
  pred = 1 + x*beta
)



p1 <- ggplot(sim, aes(x, y1)) +
  geom_abline(intercept = intercept, slope = beta, colour = "red") +
  geom_point(colour = "#00998a") +
  geom_linerange(aes(ymin = y1, ymax = pred), colour = "grey40") +
  ylim(-6,+6) + labs(title = "A - wenig MSE")


p2 <- ggplot(sim, aes(x, y2)) +
  geom_abline(intercept = intercept, slope = beta, colour = "red") +
  geom_point(colour = "#00998a") +
  geom_linerange(aes(ymin = y2, ymax = pred), colour = "grey40") +
ylim(-6,+6) + labs(title = "B - viel MSE")


grid.arrange(p1, p2, ncol = 2)
```


Geringer (links; A) vs. hoher (rechts, B) Vorhersagefehler (MSE)



### $f$ schätzen zur Erklärung

- Anstelle von Genauigkeit in der Vorhersage kann das Interesse auch primär in der Art und Weise liegen, *wie* $Y$ durch $X$ beeinflusst wird.

- Dabei ist nicht unbedingt eine kausale Einflussnahme gemeint, es kann auch lediglich eine statistische Einflussnahme betrachtet werden (in Ermangelung des Wissens der kausalen Einflüsse).

- [Erklärung]{.cemph} im statistischen Sinne meint die Quantifizierung der Veränderung von $Y$ als Funktion von $X$.

- Ziel ist die möglichst genaue und einfach verständliche Explizierung von $\hat{f}$.

- Beispiele (Datensatz `Advertising`):
  - Welcher Medientyp trägt überhaupt zum Verkauf bei?
  - Welcher Medientyp trägt am meisten zum Verkauf bei?
  - Wie viel Zuwachs an Verkauf geht mit einem gegebenem Zuwachs an z.B. TV-Werbebudget einher?
  
  
  



### Parametrische Methoden

Methoden zur Schätzung von $f$ können in zwei Gruppen unterteilt werden: parametrische und nicht-parametrische Methoden.


- Basieren auf Annahmen der funktionalen Form.
- Beispiel: $f$ ist linear in $X$, z.B.


$$f(X) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p$$


- Die Koeffizienten werden so geschätzt, dass die Kostenfunktion minimal ist.

- Vorteil: Die Annahme der Linearität vereinfacht das Problem erheblich; es sind nur noch $p+1$ Koeffizienten zu schätzen.
- Nachteil: Ein lineares $f$ ist vermutlich unterkomplex für die Wirklichkeit, d.h. $\hat{f}$ und $f$ können sich stark unterscheiden.



### Nonparametrische Methoden

- Charakteristikum nonparametrischer Methoden ist, dass keine expliziten Annahmen über die funktionale Form von $f$ getroffen werden.
- Stattdessen wird $\hat{f}$ so bestimmt, dass die Kostenfunktion klein wird -- ohne dass die Funktion zu willkürlich wird.
- Vorteil: Bessere Anpassung an die Wirklichkeit
- Nachteil: Eine größere Anzahl an Beobachtungen ist nötig als bei parametrischen Modellen.



### Beispiele für parametrische und nonparametrische Modelle^[Einige Diagramme dieses Skripts stammen aus "An Introduction to Statistical Learning, with applications in R"  (Springer, 2013) with permission from the authors: G. James, D. Witten,  T. Hastie and R. Tibshirani.]


Vorhersage von Einkommen (`income`) durch Bildung (`years of education`) und Dienstjahre (`seniority`).

::::::::: {.columns}
::: {.column width="30%" }

Parametrisches Modell

```{r p-param, echo = FALSE, out.width = "70%", caption = "Parametrisches Modell"}

knitr::include_graphics(file.path(pathToImages, "parametrisch.pdf"), error = FALSE)
```

Zu *ungenau*, Unteranpassung (*underfitting*)

:::
::: {.column width="30%" }


Nonparametrisches Modell
```{r p-nonparam, echo = FALSE, out.width = "70%", caption = "nonparametrisches Modell"}
knitr::include_graphics(file.path(pathToImages, "nonparametrisch.pdf"), error = FALSE)

```

Zu *genau*, Überanpassung (*overfitting*) 

:::

::: {.column width="30%" }

Wahres Modell

```{r p-wahr, echo = FALSE, out.width = "70%", caption = "nonparametrisches Modell"}
knitr::include_graphics(file.path(pathToImages, "wahresmodell.pdf"), error = FALSE)

```

Das wahre Modell ($f$) zeigt einigen nicht-reduzierbaren Fehler.

:::
:::::::




### Geleitetes vs. ungeleitetes Lernen

#### Geleitetes Lernen (supervised learning)

- Für jede Beobachtung der Prädiktoren $x_i, i = 1,2, \dots, n$ gibt es einen zugehören Wert der Responsevariablen $y_i$.
- Ziel ist es, *zukünftige* (aktuell noch unbekannte) Beobachtungen vorherzusagen *oder* die Funktion $\hat{f}$ gut zu verstehen.
- Beispiele für Verfahren des geleiteten Lernens sind: lineare Regression, baumbasierte Verfahren, Support Vector Machines und viele andere.

#### Ungeleitetes Lernen (unsupervised learning)

- Für jede Beobachtung $i=1,2, \dots n$ liegen eine Reihe von Prädiktorvariablen $x_i$ vor, aber *keine* Werte einer Responseveriablen.
- Die Bestimmung von $\hat{f}$  ist daher nicht möglich.
- Beispiele für Verfahren des ungeleiteten Lernens sind: Clusteranalyse (cluster analysis), Hauptkomponentenanalyse (principal components analysis), Nächste-Nachbarn-Analyse (Nearest Neighbors).


### $k-$Nächste-Nachbarn als Beispiel für eine Klassifikation I/II

- Ein Datensatz mit $p$ Variablen kann als $p-$dimensionaler Raum verstanden werden, mit $p=3$ entspricht es dem 3D-Raum.
- Die Entfernung zwischen zwei Punkten kann (z.B.) mittels der *euklidischen Distanz* berechnet werden (Satz des Pythagoras).
- Hat ein zu klassifizierender Fall mehrheitlich Fälle der Klasse $C_1$ in seiner Umgebung, so wird er ebenfalls dieser Klasse zugeordnet.
- "Umgebung" ist definiert als die $k$ Fälle, die die geringste Entfernung zum zu klassifizierenden Punkt aufweisen.
- Die Wahl von $k$ wird i.d.R. vom Benutzer gewählt und nicht durch die Daten geschätzt (Tuning-Parameter).



### $k-$Nächste-Nachbarn als Beispiel für eine Klassifikation II/II

Es soll vorhergesagt werden, ob es morgen regnet (Ergebnisvariable). Als Prädiktoren stehen Luftdruck und Luftfeuchtigkeit von $n=500$ Tagen zur Verfügung. "Morgen" ist mit einem Stern gekennzeichnet.

```{r echo = FALSE, out.width="70%", fig.asp = .5}
set.seed(42)
twoClassSim(n = 500, intercept = -5, linearVars = 2, noiseVars = 0,
  corrVars = 0, corrType = "AR1", corrValue = .7, mislabel = 0,
  ordinal = FALSE) -> sim_df

sim_df$Class_num <- ifelse(sim_df$Class ==
                           "Class1", 1, 0)


p1_knn <- sim_df %>%
  filter(Linear1 > -2, Linear1 < 2) %>%
  filter(Linear2 > -2, Linear2 < 2) %>%
  ggplot() +
  aes(x = Linear1, y = Linear2, group = 1) +
  geom_point(aes(color = Class, shape = Class), size = 3) +
  annotate(geom = "point", x = 2, y = -1, shape = 8, size = 5) +
  labs(x = "x1", y = "x2",
       class = "Regen") +
  theme(legend.position = "none") +
  scale_color_manual(values = c("#440154ff", "#B4DE2CFF")) +
  scale_fill_manual(values = c("grey50", "grey90")) +
  geom_rug(aes(color = Class))


# Compute distance to target point

sim_df <- sim_df %>% 
  mutate(distance = sqrt(((2-Linear1)^2 + (-1-Linear2)^2))) %>% 
  arrange(distance) %>% 
  mutate(nearest5 = if_else(row_number() <= 5, TRUE, FALSE))

p2_knn <- p1_knn + coord_cartesian(xlim = c(1.5, 2.1), ylim = c(-1.5, -0.5)) +
  geom_segment(x = 2, y = -1, aes(xend = Linear1, yend = Linear2), 
               data = sim_df %>% filter(nearest5 == TRUE)) +
  annotate(geom = "point", x = 2, y = -1, shape = 8, size = 5)


grid.arrange(p1_knn, p2_knn, nrow = 1)
```


Links: Prädiktoren (X- bzw. Y-Achse) und Ergebnisvariable (Farbe/Form der Punkte) im Datensatz.
Rechts: Die zum Zieltag (morgen) fünf nächsten Nachbarn.


### Regression vs. Klassifikation

Geleitete Verfahren kann man in *Regression* und *Klassifikation* (kategoriale Responsevariable) einteilen.

Nicht alle Verfahren sind klar in eine der beiden Gruppen einteilbar: Die logistische Regression gibt Wahrscheinlichkeiten der Klassenzugehörigkeit zurück und wird daher manchmal als Methode der Regression und manchmal als Methode der Klassifikation aufgeführt.

#### Regression

Ist die Responsevariable metrisch, so spricht man von einer *Regression*. 

Beispiel: Vorhersage der Verkaufszahlen (metrische Responsevariable) anhand des TV-Werbebudgets.

#### Klassifikation

Ist die Responsevariable kategorial, so spricht man von einer *Klassifikation*.

Beispiel: Gruppierung von Kunden auf Basis ähnlicher Einkaufsmuster.


### Tuningparameter

::::::::: {.columns}
::: {.column width="50%" }
- Viele statistische Modelle enthalten Parameter, deren Werte nicht von den Daten geschätzt werden, sondern vom Nutzer bestimmt werden (Tuning- oder Hyperparameter).
- Die Festlegung dieser Werte kann großen Einfluss auf die Modellgüte haben bzw. Unter- oder Überanpassung verursachen.
- Beispiele:
  - Anzahl der Nachbarn  $k$ in der Nächste-Nachbarn-Analyse-
  - Grad des Polynoms in einer Polynom-Regression
- Häufig wird auf Basis von Vorwissen eine Auswahl an Werten der Tuningparameter ausgewählt und für jeden Wert die Modellgüte (im Test-Datensatz) bestimmt.

:::
::: {.column width="50%" }

```{r echo = FALSE, out.width="70%"}
knitr::include_graphics(file.path(pathToImages, "tuning-parameter-prozess.png"), error = FALSE)

```

:::
:::::::


### Es gibt kein "bestes" Modell

Je nach Datenlage kann das eine oder andere Modell (Algorithmus) besser passen.

```{r echo = FALSE, out.width="90%", fig.asp=.5}
df <- tibble(x = c(-3, -3, 3, 3),
                 y = c(-3, -2, 1, -3)
)

df2 <- tibble(x = c(-3, -3, 3, 3),
                 y = c(-2, 3, 3, 1)
)



ggplot(df) +
  aes(x = x, y = y) +
  geom_polygon(fill = "grey80") +
  geom_polygon(data = df2, fill = "grey60") +
  geom_segment(x = -3, xend = 3,
               y = -2, yend = 1,
               color = "firebrick", size = 1) +
  scale_x_continuous(breaks = NULL) +
  labs(title = "A. Lineare Klassifizierung schneidet  gut ab") +
  theme(title = element_text(size = rel(0.6))) +
  scale_y_continuous(breaks = NULL) -> p1


ggplot(df) +
  aes(x = x, y = y) +
  geom_polygon(fill = "grey80") +
  geom_polygon(data = df2, fill = "grey60") +
  geom_segment(x = -3, xend = 3,
               y = -2, yend = -2,
               color = "firebrick", size = 1) +
  geom_segment(x = -3, xend = 3,
               y = +1, yend = +1,
               color = "firebrick", size = 1) +
    geom_segment(x = -1.5, xend = -1.5,
               y = -3, yend = -2,
               color = "firebrick", size = 1) +
    geom_segment(x = 1, xend = 1,
               y = -2, yend = 1,
               color = "firebrick", size = 1) +
    labs(title = "B. Rekursive Partionierung schneidet schlecht ab") +
    theme(title = element_text(size = rel(0.6))) +
  scale_x_continuous(breaks = NULL) +
  scale_y_continuous(breaks = NULL) +
    geom_segment(x = -3, xend = 1,
               y = -.7, yend = -.7,
               color = "firebrick", size = 1) -> p2


df3 <- tibble(x = c(-3, -3, 3, 3),
                  y = c(-3, 3, 3, -3)
)

df4 <- tibble(x = c(-2, -2, 3, 3),
                  y = c(-3, 2, 2, -3)
            )

ggplot(df3) +
  aes(x = x, y = y) +
  geom_polygon(fill = "grey80") +
  geom_polygon(data = df4, fill = "grey60") +
  labs(title = "C. Lineare Klassifizierung schneidet schlecht ab") +
  theme(title = element_text(size = rel(0.6))) +
  scale_x_continuous(breaks = NULL) +
  scale_y_continuous(breaks = NULL) +
  geom_segment(x = -3, xend = 3,
               y = -3, yend = 2.5,
               color = "firebrick",
               size = 1) -> p3


ggplot(df3) +
  aes(x = x, y = y) +
  geom_polygon(fill = "grey80") +
  geom_polygon(data = df4, fill = "grey60") +
  geom_segment(x = -2, xend = -2,
               y = -3, yend = 3,
               size = 1,
               color = "firebrick") +
  labs(title = "D. Rekursive Partionierung schneidet gut ab") +
  theme(title = element_text(size = rel(0.6))) +
  scale_x_continuous(breaks = NULL) +
  scale_y_continuous(breaks = NULL) +
  geom_segment(x = -2, xend = 3,
               size = 1,
               y = 2, yend = 2,
               color = "firebrick") -> p4


grid.arrange(p1, p2, p3, p4,
             ncol = 2)
```


### Der "Fluch der Dimension"

:::::::{.columns}



::: {.column width="50%"}

- In einem binären Klassifikationsproblem ($Y \in \{0;1\}$) liegen $k=2$ Prädiktoren $X_1, X_2 \in \mathbb{R}$ vor.
- Der Wertebereich jedes Prädiktors ist in $s=4$ gleich große Segmente $S$ geteilt; die Mehrheit der Fälle in jedem Sektor entscheidet über die Vorhersage neuer Fälle.
- $k=1 \leftrightarrow S=4$; $k=2 \leftrightarrow S=4^2 \dots$
- Bei $k$ Prädiktoren liegt die Anzahl der Segmente bei $S = s^k$ (exponentielles Wachstum)



:::








::: {.column width="50%"}


```{r echo = FALSE}
set.seed(42)
twoClassSim(n = 500, intercept = -5, linearVars = 10, noiseVars = 0,
  corrVars = 0, corrType = "AR1", corrValue = 0, mislabel = 0,
  ordinal = FALSE) -> sim_df

sim_df$Class_num <- ifelse(sim_df$Class ==
                           "Class1", 1, 0)


sim_df %>%
  filter(Linear01 > -2, Linear01 < 2) %>%
    filter(Linear02 > -2, Linear02 < 2) %>%
  ggplot() +
  aes(x = Linear01, y = Linear02, group = 1) +
  stat_summary_2d(aes(z = Class),
                  fun = function(z) names(which.max(table(z))),
                  bins = 3,
                  alpha = .5,
                  drop = TRUE) +
  geom_point(aes(color = Class, shape = Class), size = 3) +
annotate(geom = "point", x = 2, y = -1, shape = 8, size = 5) +
  labs(x = "x1", y = "x2",
       class = "Regen") +
  theme(legend.position = "none") +
  scale_color_manual(values = c("#440154ff", "#B4DE2CFF")) +
  scale_fill_manual(values = c("grey50", "grey90"))+
  theme_light()

```




```{r p-curse, echo = FALSE, out.width = "70%", caption = "Parametrisches Modell"}

knitr::include_graphics(file.path(pathToImages, "curse2-crop.pdf"), error = FALSE)
```

:::

:::::::



   
## Modellgüte bei Regressionsmodellen


### Je genauer die Vorhersage, desto besser das Modell

- Die Güte eines Modells wird anhand der Genauigkeit der Vorhersage bemessen.
- *Trainingsdatensatz*: Datensatz, in dem das Modell berechnet wird.
- *Testdatensatz*: Neuer Datensatz, in dem das Modell nicht berechnet wurde.
- Es gibt keine Garantie, dass eine Methoden mit hoher Modellgüte im Trainingsdatensatz zu hoher Modellgüte in Testdatensatz führt!

#### Mittlerer Quadratfehler ((root) mean squared error)

$$MSE = \frac{1}{n}\sum_{i=1}^{n}{(y_i - \hat{f}(x_i))^2}; RMSE = \sqrt{MSE}$$

- Je kleiner (R)MSE, desto besser die Modellgüte.


#### Erklärte Varianz, $R^2$ (R squared)

$$R^2=\frac{\sum_{i=1}^n (\hat{y}_i-\bar{y})^2}{\sum_{i=1}^n (y_i-\bar{y})^2}= 1-\frac{\sum_{i=1}^n (y_i-\hat{y}_i)^2}{\sum_{i=1}^n (y_i-\bar{y})^2}$$

- Je größer $R^2$, desto besser die Modellgüte.



### Beispiel: Unter- und überangepasste Modelle


Gegeben sei ein Trainingsdatensatz (A) und zwei Modelle (B und C). D zeigt das wahre^[Hier wurde das Modell selber bestimmt (simuliert); normalerweise ist das wahre Modell unbekannt.] Modell.

```{r p-under-over-fitting, echo = FALSE, out.width="70%", fig.asp=.3}
x <- seq(from = 1, to = 10, by = .3)
y <- sin(x) + rnorm(n = length(x), mean = 0, sd = .3)

daten <- tibble(x, y)


p0 <- ggplot(daten) +
  aes(x = x, y = y) +
  coord_fixed(ratio = 5/1) +
  labs(y = "", x = "") +
  geom_point() +
  theme(axis.text = element_blank())


p1 <- p0 + 
  ggtitle("A")

p2 <- p0 + 
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  ggtitle("B")


p3 <- p0 + geom_line(color = "blue") +
  ggtitle("C") -> p3


p4 <- p0 +  stat_function(n = 99, fun = sin, color = "darkgreen") +
  ggtitle("D") 

## ----overfitting-4-plots, echo = FALSE, fig.cap = "Welches Modell (Teil B-D; rot, grün, blau) passt am besten zu den Daten (Teil A) ?", out.width = "100%"----

grid.arrange(p1, p2, p3, p4, ncol = 4)
```

A: Datensatz, B: Unteranpassung, C: Überanpassung, D: Wahres Modell

Es ist *nicht* entscheidend, ob $MSE$ im Trainingsdatensatz klein ist, sondern im *Test*datensatz: $MSE_{\text{Test}} = MW(y_o - \hat{f}(x_0))$, wobei $(x_0, y_0)$ eine bisher nicht beobachteter (unbekannter) Fall ist. MW steht für den Mittelwert.


### $MSE$ im Trainings- und Testdatensatz


```{r p-mse, echo = FALSE, out.width = "70%", caption = "Parametrisches Modell"}

knitr::include_graphics(file.path(pathToImages, "MSE.pdf"), error = FALSE)
```

::: {.small}
Links: Daten von $f$ simuliert (schwarze Punkte). Drei Varianten von $\hat{f}$ sind dargestellt: eine lineare Regressionsline (orange) und zwei flexiblere Modelle (blaue und grün). Rechts: Training-MSE (grau), Test-MSE (rot) und minimal möglicher MSE (gepunktet). Die Quadrate zeigen die Trainings- und Test-MSE-Werte der drei Modelle des linken Teildiagramms.
:::


### Verzerrung und Varianz (Bias-Varianz-Trade-Off)

- Der Erwartungswert des Test-MSE kann in drei Größen aufgeteilt werden:
  - die Varianz von $\hat{f}$
  - die (quadrierte) Verzerrung von $\hat{f}$  
  - die Varianz des Fehlerterms $\epsilon$.
  
- Das heißt 
$$MSE_{\text{Test}} = E\left(y_o - \hat{f}(x_0) \right)^2 = Var \left(\hat{f}(x_0)) \right) + \left[Bias \left( \hat{f}(x_0) \right) \right]^2 + Var(\epsilon)$$

- $MSE$ ist immer nichtnegativ und nie kleiner als $Var(\epsilon)$.
- Je stärker eine statistisches Modell *gleichzeitig* Verzerrung und Varianz reduziert, desto besser wird die Modellgüte.


- [Varianz]{.cemph}: Das Ausmaß, in dem sich $\hat{f}$ ändern würde, wenn ein anderer Trainingsdatensatz (d.h. eine andere Stichprobe) verwendet werden würde. Eine Methode sollte möglichst wenig variieren in $\hat{f}$, wenn verschiedene Trainingsdatensätze verwendet werden.

- [Verzerrung]{.cemph}: Das Ausmaß des Unterschieds von $\hat{f}$ und $f$. Das Modell in der Wirklichkeit kann sehr komplex sein und nur unzureichend von einer simplen Funktion modelliert werden.


### Verzerrung und Varianz illustriert

```{r, echo=FALSE, cache=FALSE, out.width="30%"}
# default: FOM
point_col <- "#00998a" # FOM
```
```{r, echo=FALSE, cache=FALSE, eval=FOMLayout}
point_col <- "#00998a" # FOM
```
```{r, echo=FALSE, cache=FALSE, eval=EUFOMLayout}
point_col <- "#8e000f" # EUFOM
```
```{r, echo=FALSE, out.width = "60%", fig.align="center", cache=FALSE}
set.seed(1896)

par(mfrow=c(1,2))
theta <- seq(0, 2 * pi, length = 360)
r <- c(0.1,0.25,0.5,0.75,1)

plot(c(-1, 1), c(-1, 1), type = "n", asp=1, axes = FALSE, main="Varianz", xlab = NA, ylab=NA, cex=1.1)
for (radius in r) lines(x = radius * cos(theta), y = radius * sin(theta), lwd=1.1)
points(scale(rnorm(10)), scale(rnorm(10)), pch=19, col=point_col)

plot(c(-1, 1), c(-1, 1), type = "n", asp=1, axes = FALSE, main="Verzerrung", xlab = NA, ylab=NA, cex=1.1)
for (radius in r) lines(x = radius * cos(theta), y = radius * sin(theta), lwd=1.1)
points(rnorm(10, mean=0.5, sd=0.1), rnorm(10, mean=0.5, sd=0.1), pch=19, col=point_col)
```


- Allgemein gilt: Je flexibler eine Methode, desto höher die Varianz und desto geringer die Verzerrung.
- Die Abwägung von Verzerrung und Varianz ist sowohl für Regressions- als auch für Klassifikationsmodelle von Belang 



## Modellgüte bei Klassifikationsmodellen

### Je genauer ein (Klassifikations-)modell, desto besser

- Ein einfaches Maß zur Modellgüte bei Klassifikationsmodellen ist der *Fehleranteil*: $\text{FA}_{\text{Train}} = MW \left(I(y_i \ne \hat{y_i}) \right)$.
- $I$ ist eine Indikatorvariable die 1 ist, wenn $y_i \ne \hat{y_i}$ und 0 ist, wenn $y_i = \hat{y_i}$.
- Für den Testdatensatz gilt analog: $\text{FA}_{\text{Test}} = MW \left(I(y_0 \ne \hat{y_0}) \right)$.
- Dabei ist $y_0$ die vorhergesagte Klasse des Klassifikationsmodells anhand der Beobachtung mit dem Merkmalvektor $x_0$.
- Je geringer $\text{FA}_{\text{Test}}$, desto besser das Klassifikationsmodell.


### Vier Arten von Ergebnissen einer Klassifikation

```{r class-stats2, echo = FALSE}
df <- readr::read_csv("class-results.csv")

knitr::kable(df, caption = "Vier Arten von Ergebnissen von Klassifikationen",
             booktabs = T)
```

N: Negativ, P: Positiv; R: Richtig, F: Falsch.

### Kennwerte der Klassifikation

```{r diag-stats, echo = FALSE}

df <- readr::read_csv("diag_stats.csv")

knitr::kable(df, caption = "Geläufige Kennwerte der Klassifikation.",
             booktabs = T,
             format = "latex") %>% 
  kable_styling() %>% 
  add_footnote("Anmerkungen: F: Falsch. R: Richtig. P: Positiv. N: Negativ. V: Vorhersage")
```


### ROC-Kurven


- Einige Modelle liefern Wahrscheinlichkeiten der Klassenzugehörigkeit für jeden Fall.
- Auf Basis dieser Wahrscheinlichkeiten kann man Schwellenregeln zur Klassenzugehörigkeit bestimmen (z.B. $p>.50% \rightarrow \text{"Klasse A"}$).
- Variiert die Schwellen bekommt man eine sog. ROC-Kurve, die die Güte eines Klassifikationstests anzeigt.

```{r example-rocs, echo = FALSE, out.width = "100%", fig.asp = .3}

library(plotROC)
library(gridExtra)
D.ex <- rbinom(200, size = 1, prob = .5)
M1 <- rnorm(200, mean = D.ex, sd = .3)
M2 <- rnorm(200, mean = D.ex, sd = 1.5)
M3 <- rnorm(200, mean = D.ex, sd = 10)


test <- data.frame(D = D.ex, D.str = c("Healthy", "Ill")[D.ex + 1],
                   M1 = M1, M2 = M2, stringsAsFactors = FALSE)


p1 <- ggplot(test, aes(d = D, m = M1)) + geom_roc(labels = FALSE) + style_roc() + ggtitle("A")
p2 <- ggplot(test, aes(d = D, m = M2)) + geom_roc(labels = FALSE) + style_roc() + ggtitle("B")
p3 <- ggplot(test, aes(d = D, m = M3)) + geom_roc(labels = FALSE) + style_roc() + ggtitle("C")

grid.arrange(p1, p2, p3, nrow = 1)

```

Beispiel für eine sehr gute (A), gute (B) und schlechte (C) Klassifikation.

## Resampling-Methoden

### Nachteile eines Testdatensatzes


- Teilt man einen gegebenen Datensatz zufällig in einen Trainings- und eine Testteil, so erhält man eine bestimmte Modellgüte.

```{r p-split-sample, echo = FALSE, out.width = "50%", caption = "Parametrisches Modell"}

knitr::include_graphics(file.path(pathToImages, "split-sample.pdf"), error = FALSE)
```

- Allerdings wäre die Modellgüte anders, hätte der Zufall eine andere Aufteilung in die zwei Stichprobenteile bewirkt.
- Statistische Modelle werden ungenauer, verringert man die Größe des Datensatzes. Da das Modell nur in einem Teil der Gesamtdaten berechnet wurde (im Trainingsdatensatz), ist davon auszugehen, dass die Modellparameter ungenau sind. Wenn der Test-Datensatz klein ist, steigt die Varianz des Test-MSE.
- Besser: Die Aufteilung von Training- vs. Test-Datensatz mehrfach wiederholen, wobei die Zuteilung zu den beiden Gruppen jeweils variiert wird (Resampling-Methoden)


### $k$-fache Kreuzvalidierung ($k$-fold Cross-Validation)

- Teilt man den Gesamtdatensatz in $k$ Gruppen (Faltung) auf (jeweils etwa gleich groß), so spricht man von $k$-facher Kreuzvalidierung.

```{r p-crossval, echo = FALSE, out.width = "50%", caption = "Parametrisches Modell"}

knitr::include_graphics(file.path(pathToImages, "crossval.pdf"), error = FALSE)
```

- Die erste Faltung dient als Testdatensatz und das Modell wird anhand der verbleibenden $k-1$ Faltungen berechnet. Dann wird der MSE berechnet für die 1. Faltung, die nicht Teil der Modellberechnung war.
- Dieses Vorgehen wird für alle übrigen Faltungen wiederholt (häufig $k=10$).
- Schließlich wird der Mittelwert der MSE-Werte berechnet.


### Bootstrapping

- Eine Bootstrap-Stichprobe ist ein Zufallsstichprobe *mit Zurücklegen*: Wird ein Fall gezogen, so kann er trotzdem weiterhin gezogen werden.
- Eine Bootstrap-Stichprobe hat die gleiche Größe wie die Original-Stichprobe.
- Einige Fälle können daher mehrfach (oder gar nicht) in der Bootstrap-Stichprobe erscheinen.
- In jedem Durchgang wird das statistische Modell in der Bootstrap-Stichprobe berechnet und in den nicht gezogenen Fällen (Out-of-Bag-Sample) hinsichtlich Modellgüte überprüft.


### Schema: Bootstrap

```{r echo=FALSE, out.width = "70%", fig.align="center"}
knitr::include_graphics(file.path("./images/EinfuehrungInferenz", "bootstrap.png"), error = FALSE)
```

[Abbildung: Quelle: Lock, Robin, Patti Frazer Lock, Kari Lock Morgan, Eric F. Lock, and Dennis F. Lock (2012): Statistics: UnLOCKing the Power of Data. Wiley.]{.small}


### Aufteilung des Datensatzes bei mehreren Kandidatenmodellen


:::::::{.columns}
::: {.column width="50%"}
- Berechnet man ein Modell anhand mehrerer Tuningparameter (mehrere Modellkandidaten), so sollte man die finale Auswahl der Elemente der Tuningparameter an einem komplett unbenutzten Test-Datensatz auf die Modellgüte untersuchen.
- Insgesamt sollte dann also der Datensatz in drei Teile aufgeteilt werden:
  - Übungs-Datensatz
    - Trainings-Datensatz
    - Validierungs-Datensatz
  - Test-Datensatz
  

:::
::: {.column width="50%"}
```{r echo=FALSE, out.width = "70%", fig.align="center"}
knitr::include_graphics(file.path(pathToImages, "tuning-three-parts.png"), error = FALSE)
```

DS: Datensatz

```
for each modelkandidat
  compute model, data = train_d
  compute mse, data = vali_d
end;
get model with max modellguete
compute bestmodel, data = uebungs_d
compute mse, data = test_data
```  


:::
::::::


## Rechenkosten


### Zeit- und Speicherbedarf

- Unter Rechenkosten wird hier der Bedarf eines implementierten Algorithmus eines statistischen Modells hinsichtlich Rechenzeit und Speicherbedarf (und Strom) verstanden.

- Faktoren, die die Rechenkosten beeinflussen, sind u.a.
   - Art des Algorithmus
   - Art (Güte) der Implementierung
   - Anzahl der Beobachtungen
   - Anzahl der Prädiktoren
   - Anzahl der Ausprägungen bei kategorialen Prädiktoren
   - Art der Resampling-Methode
   - Anzahl der Werte der Tuniningparameter
   - Anzahl der Kerne 
   - Geschwindigkeit der Kerne
   - Parallelisierbarkeit des Problems
  
- Auch "kleine" Datensätze (z.B. 300k Zeilen und 30 Spalten) können die Möglichkeiten eines gängigen PCs (4 Kerne, 16 GB Speicher) leicht sprengen.


### Beispiel: Rechenkosten bei Datensatz `flights` I/II

>   Airline on-time data for all flights departing NYC in 2013. Also includes useful 'metadata' on airlines, airports, weather, and planes.^[https://cran.r-project.org/web/packages/nycflights13/index.html]



```{r}
data(flights, package = "nycflights13")
dim(flights)
```

Größe des Datensatzes in MB:

```{r}
object.size(flights) 
40650104 / 1024 / 1024
```


  
  
### Beispiel: Rechenkosten bei Datensatz `flights` II/II^[https://data-se.netlify.com/2019/08/02/performance-measures-for-caret-and-lm-r/]



::::::::: {.columns}
::: {.column width="50%"}

- Statistisches Modell: Einfache lineare Regression variiert nach
  - Anzahl der Prädiktoren
  - Größe der Stichprobe
- Nur numerische Prädiktoren
- Keine Tuningparameter
- 5-fache Kreuzvalidierung

- Hardware-Specs:
  -  MacOS 10.14.5
  - 2 Rechenkerne, Intel Core i7, 2.8 GHz
  - 16GB RAM
:::

::: {.column width="50%"}

```{r echo=FALSE, out.width = "90%", fig.align="center"}
knitr::include_graphics(file.path(pathToImages, "flights-time-comp.pdf"), error = FALSE)
```

k: Anzahl der Prädiktoren
:::
::::::


### Fallstudie `flights`


Laden Sie den Datensatz `flights` und bearbeiten Sie die [Fallstudie](https://fallstudien.netlify.com/fallstudie_nycflights/fallstudie-nyc-flights) in Kleingruppen.


Begrenzen Sie sich fürs Erste auf die *Regression* (lineares Modell) als Modellierungstechnik.

Die Daten finden Sie im OC oder als R-Paket:

```{r read-flights}
library(nycflights13)  # ggf. vorab installieren

data(flights)
object.size(flights)
```

Weitere Informationen zu diesem Datensatz finden Sie z.B. [hier](https://www.rdocumentation.org/packages/nycflights13/versions/1.0.1).



### Die Regression als "Schweizer Taschenmesser"



```{r echo = FALSE, out.width="90%"}
knitr::include_graphics(file.path(pathToImages, "linear_tests_cheat_sheet.pdf"), error = FALSE)
```


S. [Post](https://lindeloev.github.io/tests-as-linear/) von Jonas Kristoffer Lindeløv


