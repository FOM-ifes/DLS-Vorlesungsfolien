
```{r setup-neuronale-netzte, include=FALSE}
# ---------------------------------------------------------------------------
#% maintainer:
#%   - Norman Markgraf
#%
# ---------------------------------------------------------------------------
source("../../prelude.R")
initPart(
    "BDA-NeuronaleNetze",  # Dateiname ohne Suffix
    "StatisticalLearning"      # Verzeichnisname im Bilderverzeichnis 
    )
pathToImages = getPathToImages()
# ---------------------------------------------------------------------------


```





```{r include = FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = TRUE,
  out.width = "70%",
  fig.align = 'center',
  fig.width = 6,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold",
  size = "tiny"
)

```






```{r libs-nnet, include = FALSE}
library(mosaic)
library(tidyverse)
library(DiagrammeR)
library(here)
library(latex2exp)
library(gridExtra)
```





# Neuronale Netze

## Einleitung


### Biologische und binäre Computer^[Quelle: http://neuralnetworksanddeeplearning.com/chap1.html]


:::::: {.columns}
::: {.column width="50%"}

```{r p-digits, echo = FALSE}
knitr::include_graphics(file.path(pathToImages, "digits.png"), error = FALSE)
```


:::
::: {.column width="50%"}

- Biologische Computer wie das menschliche neuronale System haben keine Schwierigkeiten, die handgeschriebenen Ziffern zu verstehen.

- Für binäre Computer ist diese Aufgabe sehr schwierig.

- Einen Regelkatalog zu erstellen zur Erkennung handgeschriebener Ziffern ist komplex.

:::
::::::


### Vergleich verschiedener Lernsysteme




```{r echo = FALSE, eval = FALSE}
DiagrammeR::grViz("
digraph l1{

graph[rankdir = LR,
label = 'vorab definierte Regeln']

input; rules; output;

input -> rules -> output
} 
")
```


```{r, echo = FALSE, out.width="7cm"} 
knitr::include_graphics(file.path(pathToImages, "l1.png"), error = FALSE)

```



```{r echo = FALSE, eval= FALSE}
DiagrammeR::grViz("
digraph l2{

graph[rankdir = LR,
label = 'herkömmliches maschinelles Lernen']

a [label = '@@1']; 
b [label = '@@2']; 
c [label = '@@3'];

a -> b -> c
}

[1]: 'input'
[2]: 'features'
[3]: 'output'

")
```

```{r, echo = FALSE, out.width="7cm"} 
knitr::include_graphics(file.path(pathToImages, "l2.png"), error = FALSE)

```



```{r echo = FALSE, eval=FALSE}
DiagrammeR::grViz("
digraph l3{

graph[rankdir = LR,
label = 'Neuronale Netze']

a [label = '@@1']; 
b [label = '@@2']; 
c [label = '@@3'];
d [label = '@@4'];


a -> b -> c -> d 
}

[1]: 'input'
[2]: 'simple features'
[3]: 'abstract feature'
[4]: 'output'

")
```


```{r, echo = FALSE} 
knitr::include_graphics(file.path(pathToImages, "l3.png"), error = FALSE)

```





### Muster aus Daten lernen, nicht aus Regeln^[Quelle: http://neuralnetworksanddeeplearning.com/chap1.html]


:::::: {.columns}
::: {.column width="50%"}


```{r echo = FALSE}
knitr::include_graphics(file.path(pathToImages, "mnist-100-digits.png"), error = FALSE)
```


:::
::: {.column width="50%"}

- Neuronale Netzen lernen nicht aus vorgegebenen Regeln, sondern lernen die Regeln selbständig aus vorhandenen Daten (genau wie andere Algorithmen des maschinellen Lernens auch).


:::
::::::


### Das Perceptron^[Quelle: http://neuralnetworksanddeeplearning.com/chap1.html]

:::::: {.columns}
::: {.column width="50%"}

```{r echo = FALSE}
knitr::include_graphics(file.path(pathToImages, "Perceptron.png"), error = FALSE)
```


- Das Perceptron ist die einfachste Art eines Neurons in der Forschung zur künstlichen Intelligenz.

- Ein Neuron kann verstanden werden als ...
    - ein Objekt, das eine Zahl speichert.
    - eine *Funktion*, also ein Objekt, dass eine Eingabe eine Ausgabe zuordnet.

:::
::: {.column width="50%"}

- Ein Perceptron hat mehrere binäre Eingänge (Inputs), $x_1, x2, \ldots,$ und einen binären Ausgang (Output), $a$.

- Gewichte, $w_1, w_2, \ldots,$ bestimmen die Wichtigkeit einer Eingabe.

- Die Ausgabe ist $1$, wenn die gewichtete Summe der Eingaben größer (oder gleich) als ein Schwellenwert (bias) $b$ ist:

$$
a = 
\begin{cases}
  1, & \text{wenn} \sum_i w_i x_i \ge b \\
  0, & \text{wenn} \sum_i w_i x_i < b
\end{cases}
$$


- Zur kompakteren Notation: $wx := w \cdot x := \sum_i w_i x_i$

:::
::::::



### Übung  `r nextExercise()`: Beispiel für ein Perceptron {.exercise type=essay}

Lesen Sie das Beispiel für ein Perceptron im Buch von Michael Nielsen (Kapitel 1): http://neuralnetworksanddeeplearning.com/chap1.html


:::{.scriptsize}

Michael A. Nielsen, "Neural Networks and Deep Learning", Determination Press, 2015

:::



Erklären Sie dann einer nebensitzenden Person das Beispiel zum Perceptron!





:::{.notes}

Individuell

:::





### Komplexere Netzwerke^[Quelle: http://neuralnetworksanddeeplearning.com/chap1.html]


- Mit größeren Netzwerken lassen sich komplexere Entscheidungsprozesse abbilden.


```{r echo  = FALSE, out.width="50%"}
knitr::include_graphics(file.path(pathToImages, "network2.png"), error = FALSE)
```

- Neuronale Netze mit mehr als zwei Schichten bezeichnet man auch als *tiefe* Netzwerke (Deep Learning).

- In modernen Anwendungen kann es eine große Zahl an Zwischenschichten geben.

### Sigmoide Neurone


- Damit ein neuronales Netzwerk lernt, ist es nötig, dass kleine Änderungen in der Eingabe zu kleinen Änderungen in der Ausgabe führen.

- Ein Perceptron kann diese Regeln nicht umsetzen; kleine Änderungen in den Eingaben führen (mitunter) zu großen Änderungen in der Ausgabe. So könnte ein dunklerer Pixel dazu führen, dass das Perceptron seine Ausgabe (komplett) ändert.

- Sigmoide Neuronen verknüpfen kleine Änderungen in der Eingabe mit kleinen Änderungen in der Ausgabe.


:::::: {.columns}
::: {.column width="50%"}


```{r echo = FALSE, results = "hold"}
e <- 2.718
x <- seq(-5, 5, by = .1)

p <- e^x / (1 + e^x)
gf_line(p ~ x) %>% 
  gf_labs(y = "output", x = "z")
```




:::
::: {.column width="50%"}

- Die Ausgabe ist nicht 0 oder 1, sondern $\sigma(w\cdot x + b)$

- $\sigma$ ist die *sigmoide* (logistische) Funktion:



$$\sigma(z)  := \frac{1}{1+ e^{-z}} =
         \frac{1}{1 + \text{exp}(-\sum_j w_jx_j - b)}$$


:::
::::::


## Funktionen

### Funktionen (I/III)

Eine *Abbildung* (oder *Funktion*) $$f: A \to B$$ mit der *Zuordnungsvorschrift* $a \mapsto b=f(a)$ 
ist wegen $(a, f(a))$ eine Relation auf $A \times B$.

Der **Definitionsbereich** (**Domain von $f$**) ist dabei:

$$\text{domain}(f) := \mathbf{D}_f := \{a \in A \,|\, f(a) \text{ ist definiert}\}$$

Der **Wertebereich** (**Range von $f$**) ist hierbei:
$$\text{range}(f) := \mathbf{W}_f := \{b \in B \,|\, \text{ es gibt ein } a \in A \text{ mit } f(a) = b\}$$

Eine Funktion ist **total**, falls $\text{domain}(f) = A$, ansonsten ist sie **partiell** (oder **rechtseindeutig**).


### Funktionen (II/III)

#### Injektiv, surjektiv und bijektiv {.definition}

Wir nennen die Funktion $f: A \to B$ ...

a) **injektiv** (oder **linkseindeutig**), falls die Gleichung $f(a)=b$ für $b \in B$ höchstens eine Lösung $a\in A$ hat; d.h. $a_1 \neq a_2 \Longrightarrow$ folgt $f(a_1) \neq f(a_2)$ oder alternativ $f(a_1) = f(a_2) \Longrightarrow a_1 = a_2$

b) **surjektiv** (oder **rechtstotal**), falls die Gleichung $f(a)=b$ für jedes $b \in B$ mindestens eine Lösung hat; d.h. $\forall b\in B: \exists a \in A: f(a)=b$ oder alternativ $\text{range}(f) = B$;

c) **bijektiv**, falls $f$ sowohl *injektiv* als auch *surjektiv* ist.


### Beispiele Funktionen

```{r Funktion, out.width="95%", echo=FALSE, fig.align="center"}
f1 = makeFun( x^2 ~ x & x)
x1 = seq(-2,2, by=0.1)
p1 <-gf_line(f1(x1) ~ x1,
        xlab="x", ylab="f(x)",
        subtitle="surjektiv, nicht bijektiv.",
        caption=TeX("$f: \\mathbf{R} \\rightarrow \\mathbf{R}^{\\geq 0}$, $f(x)=x^2$"))

f2 = makeFun( x^3 ~ x & x)
x2 = seq(-1,1, by=0.05)
p2 <- gf_line(f2(x2) ~ x2, 
        xlab="x", ylab="f(x)",
        subtitle="bijektiv",
        caption=TeX("$f: \\mathbf{R} \\rightarrow \\mathbf{R}$, $f(x)=x^3$"))

f3 = makeFun( exp(x) ~ x & x)
p3 <- gf_line(f3(x1) ~ x1,
        xlab="x", ylab="f(x)",
        subtitle="injektiv, nicht surjektiv",
        caption=TeX("$f: \\mathbf{R} \\rightarrow \\mathbf{R}^{\\geq 0}$, $f(x)=\\exp(x)$"))

f4 = makeFun( exp(-x^2) ~ x & x)
p4 <- gf_line(f4(x1) ~ x1,
        xlab="x", ylab="f(x)",
        subtitle="weder injektiv noch surjektiv",
        caption=TeX("$f: \\mathbf{R} \\rightarrow \\mathbf{R}^{\\geq 0}$, $f(x)=\\exp\\left(-x^2\\right)$"))

grid.arrange(p1, p2, p3, p4, ncol=2)
```


### Funktionen (III/III)

Ist $f$ *injektiv*, so gibt es eine Funktion $f^{-1}: B \to A$ mit:
$$f^{-1}(b)=a \Longleftrightarrow f(a) = b$$
Diese Funktion nennen wir die **Umkehrfunktion** zu $f$.

#### {.fact}

Ist $f$ *injektiv* und $f^{-1}$ *injektiv*, so ist $f$ (und $f^{-1}$) *bijektiv*.

#### Urbild{.definition}

Wir bezeichnen für eine Menge $M$ das **Urbild von $M$** mit 
$$f^{-1}(M) = \{a \in A \,|\, f(a) \in M\}.$$



## Struktur neuronaler Netze

### Multilayer Perceptrons^[Quelle: http://neuralnetworksanddeeplearning.com/chap1.html]

:::::: {.columns}
::: {.column width="50%"}

```{r echo = FALSE, out.width="100%"}
knitr::include_graphics(file.path(pathToImages, "network3.png"), error = FALSE)
```


:::
::: {.column width="50%"}


- Netzwerke mit mehreren Schichten werden auch als *Multilayer Perceptrons* (MLP) bezeichnet.

- Meist werden sigmoide Neurone verwendet, trotzdem spricht man von MLPs.

- Man bezeichnet die Schichten als
  - Eingabeschicht (input layer)
  - Zwischenschicht(en) (hidden layer(s))
  - Ausgabeschicht (output layers)

- Die Aktivierung wird von der Eingabeschicht zur Ausgabeschicht *vorwärts* geleitet: *Vorwärtsaktivierung* (feedforward activation).

:::
::::::


### Hello World der Neuronalen Netze: MNIST^[https://en.wikipedia.org/wiki/MNIST_database]-Datensatz


MNIST-Datensatz handgschriebener Ziffern^[Quelle: https://www.r-bloggers.com/exploring-handwritten-digit-classification-a-tidy-analysis-of-the-mnist-dataset/]

```{r load-mnist-data, cache = TRUE}
mnist_data_2rows<-"https://pjreddie.com/media/files/mnist_train.csv"
```


:::::: {.columns}
::: {.column width="50%"}

::: {.scriptsize}

```{r plot-mnist-tidy-comp-no-eval, eval = FALSE}
mnist_raw <- 
  read_delim(mnist_data_2rows, 
             delim = ",",
             col_names = FALSE,
             n_max = 2) 

pixels_gathered <- mnist_raw %>%
  head(10000) %>%
  rename(label = X1) %>%
  mutate(instance = row_number()) %>%
  gather(pixel, value, -label, 
         -instance) %>%
  tidyr::extract(pixel, "pixel", 
                 "(\\d+)", 
                 convert = TRUE) %>%
  mutate(pixel = pixel - 2,
         x = pixel %% 28,
         y = 28 - pixel %/% 28)
pixels_gathered
```

```{r plot-mnist-tidy-comp,echo = FALSE}

mnist_raw <- read_csv(paste0(here::here(),"/Inhalte/StatisticalLearning/mnist-raw-2rows.csv"))


pixels_gathered <- mnist_raw %>%
  head(10000) %>%
  rename(label = X1) %>%
  mutate(instance = row_number()) %>%
  gather(pixel, value, -label, 
         -instance) %>%
  tidyr::extract(pixel, "pixel", 
                 "(\\d+)", 
                 convert = TRUE) %>%
  mutate(pixel = pixel - 2,
         x = pixel %% 28,
         y = 28 - pixel %/% 28)
```


:::


:::
::: {.column width="50%"}

::: {.scriptsize}

```{r plot-mnist-tidy, echo = TRUE}
theme_set(theme_light())

p_mnist <- pixels_gathered %>%
  filter(instance <= 12) %>%
  ggplot(aes(x, y, fill = value)) +
  geom_tile() +
  facet_wrap(~ instance + label)
p_mnist
```
:::


:::
::::::

### MNIST: One-Layer-Network^[Quelle: https://ml4a.github.io/ml4a/looking_inside_neural_nets/]


:::::: {.columns}
::: {.column width="50%"}

```{r, echo = FALSE, out.width="100%"} 
knitr::include_graphics(file.path(pathToImages, "mnist-1layer.png"), error = FALSE)
```


:::
::: {.column width="50%"}


```{r, echo = FALSE, out.width="100%"} 
knitr::include_graphics(file.path(pathToImages, "weights-analogy1.png"), error = FALSE)
```

:::
::::::

### Gewichtungsfunktion^[Quelle: https://ml4a.github.io/ml4a/looking_inside_neural_nets/]


$$z = \sum wx + b$$

```{r, echo = FALSE} 
knitr::include_graphics(file.path(pathToImages, "weights-analogy2.png"), error = FALSE)
```



### Konfusionsmatrix^[Quelle: https://ml4a.github.io/ml4a/looking_inside_neural_nets/]


```{r, echo = FALSE, out.width="50%"} 
knitr::include_graphics(file.path(pathToImages, "mnist-confusion-samples.png"), error = FALSE)
```


### Rechenbeispiel


```{r, echo = FALSE} 
knitr::include_graphics(file.path(pathToImages, "nnet-comp-ex.png"), error = FALSE)
```

### Netzwerk mit zwei Zwischenschichten





```{r echo = FALSE, eval = FALSE}
DiagrammeR::grViz("
digraph l3{

graph[rankdir = LR, 
label = 'Netzwerk mit zwei Zwischenschichten',
compound = true]


subgraph input {
a [label = '@@1']; 
b [label = '@@2']; 
c [label = '@@3']; 
d [label = '@@4'];

a -> {e f g}
b -> {e f g}
c -> {e f g}
d -> {e f g}

label = 'Input'

}

e [label = '@@7']; 
f [label = '@@1']; 
g [label = '@@5']; 
h [label = '@@6']; 
i [label = '@@1']; 
j [label = '@@8']; 
k [label = '@@9']; 
l [label = '@@1']; 
m [label = '@@1'];



e -> {h i j}
f -> {h i j}
g -> {h i j}


h -> {k l m}
i -> {k l m}
j -> {k l m}



}

[1]: '...'
[2]: 'Pixel 784'
[3]: 'Pixel 001'
[4]: 'Pixel 002'
[5]: 'a(1,16)'
[6]: 'a(2,1)'
[7]: 'a(1,1)'
[8]: 'a(2,16)'
[9]: 'Ziffer 0'
[10]: 'Ziffer 9'


",
engine = "dot")
```


```{r, echo = FALSE, out.width="12cm"} 
knitr::include_graphics(file.path(pathToImages, "l3.png"), error = FALSE)
```


- Ein neuronales Netz kann als Funktion verstanden werden. 

- Im Beispiel:
  - 784 Argumente (Inputs)
  - 10 Werte (Outputs)
  - *viele* Parameter (alle Gewichte plus Schwellenwerte)


### Warum mehrere Schichten?^[Quelle: http://neuralnetworksanddeeplearning.com/chap1.html]

Das Netzwerk könnte mehrere Repräsentationsebenen der Ausgabe konstruieren:


:::::: {.columns}
::: {.column width="50%"}

- Zwischenschicht 1: visuelle Bausteine einer Ziffer
```{r, echo = FALSE} 
knitr::include_graphics(file.path(pathToImages, "mnist-edges.png"), error = FALSE)
```

:::
::: {.column width="50%"}

- Zwischenschicht 2: Repräsentation einer Ziffer
```{r, echo = FALSE, out.width = "30%"} 
knitr::include_graphics(file.path(pathToImages, "mnist-complete-zero.png"), error = FALSE)
```

:::
::::::

*Achtung*: Es gibt keine Garantie, dass das Netzwerk diese Konzeption widerspiegelt. Oft wird das Netzwerk ein lokales Minimum finden, das eine andere Konzeption darstellt.

### Kompaktere Notation

- Aktivierung des Neurons $a_0^0$:

$$a^0_0 = \sigma(w_1a_1 + w_2a_2 + \ldots + w_na_n + b)$$

- Für alle Neurone Schicht 1:

$$
\sigma \left(
\begin{bmatrix}
w_{00} & w_{01} & \cdots & w_{0n} \\
w_{10} & w_{11} & \cdots & w_{1n} \\
\vdots & \vdots & \vdots & \vdots \\
w_{k0} & w_{k1} & \cdots & w_{kn}
\end{bmatrix} 
\begin{bmatrix}
a_0^0 \\
a_1^0 \\
\vdots \\
a_n^0
\end{bmatrix} + 
\begin{bmatrix}
b_1 \\
b_2 \\
\vdots \\
b_n
\end{bmatrix}
\right)
= 
\begin{bmatrix}
a_0^1 \\
a_1^1 \\
\vdots \\
a_n^1
\end{bmatrix}
$$

- In kompakterer Notation:

$$a^1 = \sigma(\boldsymbol{W} \vec{a^0} + \vec{b})$$ 

- Anzahl der Gewicht $w_i$: $784\cdot 16 + 16 \cdot 16 + 16\cdot 10$
- Anzahl der Schwellenwerte $b_i$: $16 + 16 + 10$
- In Summe $13002$ Parameter


## Gradientenverfahren

### Optimierungsfunction $C$

- Damit Lernen (eines künstlichen neuronalen Netzwerks) möglich wird, muss bekannt sein, was eine "gute" und eine "schlechte" Ausgabe ist.

- Je näher die Ausgabe am tatsächlichen Wert ist, desto besser ist die Ausgabe.

- Dieser Sachverhalt wird über eine Optimierungsfuntion $C$ (Cost function) formalisiert.

- Für jedes Trainingsbeispiel $j$ wird $C_j$ in (Abhängigkeit der Parameter) berechnet:

$$C_j(w,b) = \sum_{i=0}^9 (y_i - \hat{y}_i)2$$

- Dann wird $C_j$ für alle  $J$ Trainingsfälle aufsummiert:

$$C(w,b) = \frac{1}{n} \sum_{j=1}^{J} c_j$$


### Minimierung einer Funktion


:::::: {.columns}
::: {.column width="50%"}

```{r, echo = FALSE, out.width="100%"} 
knitr::include_graphics(file.path(pathToImages, "minimum.png"), error = FALSE)
```

::: 
::: {.column width="50%"}

- Eine Funktion wird durch Ableiten minimiert: $\frac{dC(w)}{dW} = 0$

- Bei komplexen Funktion ist Ableiten unpraktisch.

- $C$ ist eine komplexe Funktion:
  - Input: 13002 (Gewichte und Schwellenwerte)
  - Output: 1 Zahl (Kostenwert)
  - Parameter: viele Trainingsfälle


:::
::::::


### Minimierung von $C$ mit dem Gradientenverfahren

:::::: {.columns}
::: {.column width="50%"}


```{r, echo = FALSE, out.width="100%"} 
knitr::include_graphics(file.path(pathToImages, "gradient.png"), error = FALSE)
```

::: 
::: {.column width="50%"}

Gradientenverfahren:

- Starte an einem beliebigen Punkt
- Gehe einen Schritt in Richtung des steilsten Abstiegs
- Je steiler, desto größer der Schritt

Dieses Verfahren führt zu einem *lokalen* Minimum. Für ein *globales* Minimum gibt es keine Garantie! In hochdimensionalien Suchräumen ist es sehr unwahrscheinlich, auf diese Weise das globale Mimimum zu finden.



:::
::::::

### Veranschaulichung zum Gradientenverfahren

Der Gradient $\nabla C$ gibt die Richtung des steilsten Anstiegs an:

:::::: {.columns}
::: {.column width="70%"}
```{r, echo = FALSE, out.width="100%"} 
knitr::include_graphics(file.path(pathToImages, "geogebra-gradient1.png"), error = FALSE)
```




:::
::: {.column width="30%"}

1. Berechne $\nabla C$
3. Gehe kleinen Schritt in $-\nabla C$
3. Wiederhole bis zu einem Minimum

Die Länge des Gradientenvektor $\nabla C$ gibt an, wie "steil" der steilste Anstieg an der aktuellen Stelle ist.

:::
::::::

https://www.geogebra.org/m/A4HZvzu4

https://www.geogebra.org/m/WhRWTWdd


### Beispiel


Eine einfache Funktion^[Graph: https://www.geogebra.org/m/df4w8hef] $C(x,y)$:

$$C(x,y) = \frac{3}{2}x^2 + \frac{1}{2}y^2$$

An der Stelle (1,1) ist die Richtung des steilsten Anstiegs definiert durch den Gradientenvektor $\nabla C = \left[3, 1 \right]^T$:

$$\nabla C(1,1) =
\begin{bmatrix}
3 \\
1
\end{bmatrix}
$$

Die Elemente des Gradientenvektors $\nabla C$ zeigen die relative Bedeutung der Input-Variablen $x$ und $y$ an. Hier ist $x$ 3 Mal so wichtig für die Richtung des steilsten Anstiegs wie $y$.

Wenn ein *Minimum* gesucht ist, geht man in die *entgegengesetzte* Richtung, also $-\nabla C$.



## Back Propagation (Fehlerrückführung)


### Was ist Back Propagation und wozu ist sie gut?

#### Back Propagation

- Back Propagation ist ein Algorithus, der berechnet, wie für einen bestimmten Trainingsfall die Parameter (Gewichte und Schwellenwerte) angepasst werden müssen, um die Kostenfunktion $C$ zu minimieren.

- Es ist eine Methode, um $-\nabla C$ zu berechnen.

- Das Ziel ist also, die höchste Verringerung des Kostenwerts zu finden.

- Auf diese Art wird *Lernen* erreicht: Die Werte der Gewichte $w$ und Schwellenwerte $b$ zu finden, die die Kostenfunktion minimieren.

- Aus Gründen der Rechenzeiteinsparung wird diese Berechnung nur für eine Zufallsstichprobe des Trainingsdatensatz ausgeführt, nicht für alle Trainingsfälle (stochastic gradient descent).


### Ablauf 


1. Wiederhole bis Abbruchkriterium erreicht
2. Berechne $C(w,b)$ über alle Trainingsfälle $j, j=1,2,\ldots,J$
  1. Ziehe Zufallsstichprobe (mini batch) $Z$
  2. Wiederhole für alle Parameter
    1. Berechne $-\nabla C$ (über alle Fälle aus $Z$)
  3. Aktualisiere die Parameter $w,b$ entsprechend $-\nabla C$
  



### Berechnung von $-\nabla C$ (I/III)


:::::: {.columns}
::: {.column width="50%"}



```{r l4, echo = FALSE, eval = FALSE}
DiagrammeR::grViz("
digraph l4{

graph[rankdir = LR]

a [label = '@@1']; b [label = '@@2']; 

a -> b [label = 'w(L)']
} 


[1]: 'a(L-1)'
[2]: 'a(L)'
")

```

```{r, echo = FALSE} 
knitr::include_graphics(file.path(pathToImages, "l4.png"), error = FALSE)
```




::: 
::: {.column width="50%"}


- Der Kostenwert $C$ ist die quadrierte Differenz von der Aktiverung $a$ und dem beobachteten wert $y$:

$$C_0(...) = (a^L - y)^2$$


- -Die Aktivierung $a$ eines Neuron in der Schicht $L$ ist abhängig vom Gewicht $w$, Schwellenwert $b$ sowie der Aktivierung der Neurone aus der vorherigen Schicht:


$$a_L = \sigma(w^L a^{L-1} + b^L)$$

- Den (untransformierten) Inputwert bezeichnen wir mit $z$:

$$z_L = w^L a^{L-1} + b^L$$

:::
::::::


### Berechnung von $-\nabla C$ (II/III)


Die Kostenfunktion ist eine Komponierte Funktion aus den Aktivierungen der vorherigen Schichten:





```{r l5, echo = FALSE, eval = FALSE}
DiagrammeR::grViz("
digraph l5{

graph[label = 'C als komponierte Funktion',
rankdir = 'LR']

a [label = '@@1']; 
b [label = '@@2']; 
c [label = '@@3']; 
d [label = '@@4']; 
e [label = '@@5']; 
f [label = '@@6']; 
g [label = '@@7']; 

{a b c} -> d
d -> e
{e f} -> g
} 


[1]: 'a(L-1)'
[2]: 'b(L)'
[3]: 'w(L)'
[4]: 'z(L)'
[5]: 'a(L)'
[6]: 'sigma'
[7]: 'C'


")

```


```{r, echo = FALSE, out.width="100%"} 
knitr::include_graphics(file.path(pathToImages, "l5.png"), error = FALSE)
```




### Berechnung von $-\nabla C$ (III/III)


Die partielle Ableitung von $C_0$ für $w^L$ ist die komponierte Funktion der Aktivierungen der vorherigen Schichten und wird daher mit der Kettenregel abgeleitet:

$$ \frac{\partial C_0}{\partial w^L} =\underbrace{ \frac{\partial z^L}{\partial w^l} \frac{\partial a^L}{\partial z^L} \frac{\partial C_0}{\partial a^L}}_{\text{Kettenregel}} = a^{L-1}\sigma' 2(a^L - y)$$




Diese Ableitung für für alle Trainignsfälle durchgeführt und Ergebnis wird gemittelt:


$$\frac{\partial C}{\partial w^L} = \frac{1}{n} \sum_{k=0}^{n-1} \frac{\partial C_k}{\partial w^L}$$


Im (realistischen) Fall von mehreren Neuronen und mehreren Schichten wird die Notation aufwändiger, aber die Berechnung bleibt gleich. So wird z.B. für jedes Gewicht notiert, zu welcher Schicht $j$ es führt und von welchem Neuron $k$ es fortführt: $w_{jk}$.



## Implementierung eines neuronalen Netzes


### Beispiel von Michael Nielsen

Mit nur 74 Zeilen Python2-Code hat Michael Nielsen ein neuronales Netz implementiert, das ca. 96% Genauigkeit in der Erkennung von handgeschriebenen Ziffern (MNIST-Datensatz) aufweist:


https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py

Alle zugehörigen Informationen (inkl. Daten und Diagramme) finden sich hier:

https://github.com/mnielsen/neural-networks-and-deep-learning

Python3-Code: https://github.com/MichalDanielDobrzanski/DeepLearningPython35


In **R** gibt es u.a. das Buch von F. Chollett "Deep Learning with R": https://www.manning.com/books/deep-learning-with-r

