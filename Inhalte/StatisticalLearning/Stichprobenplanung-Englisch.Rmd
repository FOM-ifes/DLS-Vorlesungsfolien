
```{r setup-stichprobenplanung, include=FALSE}
# ---------------------------------------------------------------------------
#% maintainer:
#%   - Sebastian sauer 
#%
# ---------------------------------------------------------------------------
source("../../prelude.R")
initPart(
    "Stichprobenplanung",  # Dateiname ohne Suffix
    "StatisticalLearning"      # Verzeichnisname im Bilderverzeichnis 
    )
pathToImages = getPathToImages()
# ---------------------------------------------------------------------------


options(tinytex.verbose = TRUE)

```





```{r libs-stichprobenplanung, include = FALSE}
library(mosaic)
library(pwr)
library(simr)
library(DiagrammeR)
library(lsr)
library(gridExtra)
library(tidyr)
library(MBESS)
library(MASS)
```




# Sample Size Planning


## Introduction


### How big is big enough?


... when it comes to sample size planning.


On planning a research study, the "right" sample size needs to be decided upon.


The higher the sample size $n$ ...

- the higher the *precision* of the parameter estimates
- the higher the probability of statistical significance (given there is an effect)
- the higher the costs of data collection.




### Example: What's the proportion of females among students?

The research teams investigate the proportion of female students within one university (faculty).



Let $\pi = 0.6$ (unknown to the research teams).


:::::: {.columns}
::: {.column width="50%"}

Team $A$ draws samples of size $n=10$



```{r echo = FALSE}
s1 <- do(1000) * rflip(n = 10, prob = .6)

gf_bar(~ prop, data = s1) %>% 
  gf_lims(x = c(0, 1)) %>% 
  gf_labs(x = "proportion of females") +
  annotate("label",
           x = -Inf, 
           y = +Inf,
           label = paste0("SD: ", round(sd(s1$prop), 2)),
           hjust = 0,
           vjust = 1)
  
```

 

:::


::: {.column width="50%"}

Team $B$ draws samples if size $n=100$


```{r echo = FALSE}
s2 <- do(1000) * rflip(n = 100, prob = .6)

gf_bar(~ prop, data = s2) %>% 
  gf_lims(x = c(0, 1)) %>% 
  gf_labs(x = "proportion of females") +
  annotate("label",
           x = -Inf, 
           y = +Inf,
           label = paste0("SD: ", round(sd(s2$prop), 2)),
           hjust = 0,
           vjust = 1)
  
```


:::
::::::


The *larger* samples estimated the true values more *precisely* compared to the smaller samples.




### Exercise `r nextExercise()`: Simulate the effect of sample size! {.exercise type=essay}

>   The research teams investigate the proportion of female students within one university (faculty).


Vary the sample size $n$ and the true proportion $\pi$. Compare both effects on the precision of estimation. 



```{r eval = FALSE}
library(mosaic)
s1 <- do(1000) * rflip(n = 10, prob = .6)
s2 <- do(1000) * rflip(n = 100, prob = .6)

gf_bar(~ prop, data = s1)
gf_bar(~ prop, data = s2)
```


::: {.notes}

individually

:::



### Reminder: Nuull Hypothesis Significance Testing


1. Define research question.
2. Define $H_0$ (Null hypothesis) and $H_A$ (Alternative hypothesis). Choose test statistic.
3. Compute test statistic distribution for $H_0$.
4. Determine how frequent observed effect is in $H_0$ distribution
    - frequent: Do not reject $H_0$
    - infrequent: Reject $H_0$.
    
    





### Two types of errors

|   | Decision: Do *not* reject $H_0$ | Decision: Reject $H_0$   |
|:---|:---:|:---:|
| Truth: $H_0$  | Ok | **Error Type 1** |
| Truth: $H_A$  | **Error Type 2**  | Ok  |

Song: [https://www.causeweb.org](https://www.causeweb.org): [Larry Lesser und Dominic Sousa &copy; Hypothesis on Trial](https://www.causeweb.org/cause/resources/fun/songs/hypothesis-trial)








### Error type 1 and 2 visualized


```{r echo = FALSE, out.width="70%"}
gf_dist("norm", mean = 0, sd = 1, fill = ~ cut(x, c(-Inf, 2, 100, Inf)), geom = "area", alpha = .5) %>%
  gf_dist("norm", mean = 4, sd = 1, fill = ~ cut(x, c(-Inf, -100, 2, Inf)), geom = "area", alpha = .5) %>%
  gf_labs(x = "p", y = "") %>%
  gf_vline(xintercept = 2) %>%
  gf_refine(annotate(geom = "text", x = .75, y = .42, label = "H0 not rejected")) %>% 
  gf_refine(annotate(geom = "text", x = 2.95, y = .42, label = "H0 rejected")) %>% 
  gf_refine(annotate(geom = "text", x = 0, y = .15, size = 3, label = "H0")) %>% 
  gf_refine(annotate(geom = "text", x = 1.35, y = .01, size = 2.5, label = "Error Type 2")) %>%
  gf_refine(annotate(geom = "text", x = 2.6, y = .01, size = 2.5, label = "Error Type 1")) %>% 
  gf_refine(annotate(geom = "text", x = 4, y = .15, size = 3, label = "HA")) +
  guides(fill = FALSE) # To remove the legend
```





### Fomshiny: Effect of sample size  {include-only=shiny}




[https://fomshinyapps.shinyapps.io/Stichprobenumfang/](https://fomshinyapps.shinyapps.io/Stichprobenumfang/)





### Determinants of power


The (statistical) power of a study (and the precision of an estimation) is a function of several factors.



```{r echo = FALSE, out.width = "70%"}
grViz("
     digraph precision {
     
     graph [overlap = true, fontsize = 14, layout = dot, rankdir = LR, fontname = Helvetica]
     
     node [shape = box, fixedsize = true, width = 2]
     
     a [label = '@@1']
     b [label = '@@2']
     c [label = '@@3']
     d [label = '@@4']
     e [label = '@@5']
     
     a -> e
     b -> e
     c -> e
     d -> e

     
     }
     
     [1]: 'effect size'
     [2]: 'population variance'
     [3]: 'significance level'
     [4]: 'sample size'
     [5]: 'power'
")


```


### Rather estimate precision than power?



What does Andrew Gelman tells us?^[https://statmodeling.stat.columbia.edu/2004/12/29/type_1_type_2_t/]

>   I’ve never in my professional life made a Type I error or a Type II error. But I’ve made lots of errors. How can this be?

>    A Type 1 error occurs only if the null hypothesis is true (typically if a certain parameter, or difference in parameters, equals zero). In the applications I’ve worked on, in social science and public health, I’ve never come across a null hypothesis that could actually be true, or a parameter that could actually be zero.

>   A Type 2 error occurs only if I claim that the null hypothesis is true, and I would certainly not do that, given my statement above!






## Effect size

### Primer on effet size

It is well-known that the $p$-value denotes the probability of the test statistics given that $H_0$ holds true. With the sample size $n$ increasing the $p$-value descreases (c.p.).

**Cohen's d** is a effect size for comparing two means. It denotes the degree of overlap of the distributions^[for other effect sizes, cf. R package [compute.es](https://cran.r-project.org/package=compute.es).]]: $$d=\frac{\bar{x}_A-\bar{x}_B}{\text{sd}_{\text{pool}}}$$ mit $${\text{sd}_{\text{pool}}=\sqrt{\frac{1}{n_A+n_B-2}\left((n_A-1)\cdot\text{sd}^2_A+(n_B-1)\cdot\text{sd}^2_B \right)}}$$


```{r, eval=FALSE}
# install once
install.packages("lsr")

# Load pacakge
library(lsr)
```

```{r load-library-lsr-power, echo=FALSE}
library(lsr)
```


### Effect sizes - rules of thumbs




- $|d|\geq 0.2$ small
- $|d|\geq 0.5$ medium
- $|d|\geq 0.8$ large


More rules of thumbs to be found here:

```{r}
library(pwr)  # install once!

cohen.ES(test = "r", size = "large")
# ?cohen.ES
```




### Examples to Cohen's d

```{r plot-cohens-d-effsize2, echo=FALSE, out.width="80%", fig.align="center"}
set.seed(1896)
n <- 1000
x <- scale(rnorm(n))
x02 <- x+0.2
x05 <- x+0.5
x08 <- x+0.8
x11 <- x+1.1

cohendata <- data.frame(x=c(x, x02, x, x05, x, x08, x, x11), 
                   Group=rep(c(rep(c("A","B"), each=n)), 4),
                   d=c(rep("d=0.2", 2*n), rep("d=0.5", 2*n), rep("d=0.8", 2*n) , rep("d=1.1", 2*n)))

gf_fitdistr(gformula=~x|d, color=~ordered(Group), data=cohendata, size = 2) %>%
    gf_labs(y="f(x)", x="x", color = "Group")

rm(n,x,x02,x05,x08,x11,cohendata)
```


### FOMshiny: Comparing means {includeonly=shiny}


What factors influence the effect size of mean difference?

[https://fomshinyapps.shinyapps.io/Mittelwertsvergleich/](https://fomshinyapps.shinyapps.io/Mittelwertsvergleich/)


[Power and p-values when comparing mean values](http://shinyapps.org/showapp.php?app=https://tellmi.psy.lmu.de/felix/lakens_pcurve/&by=Daniel%20Lakens&title=P-value%20distribution%20and%20power%20curves%20for%20an%20independent%20two-tailed%20t-test&shorttitle=P-value%20distribution%20and%20power%20curves)


### Power Analysis: Simulating $d, n$ and the p-value

```{r plot-p-value-n-d2, echo=FALSE, out.width="80%", fig.align="center"}
set.seed(1896)
n <- c(30, 100)
d <- c(0, 0.2, 0.5, 0.8)
pvalue <- matrix(nrow=10000*8, ncol = 4)
zaehler <- 1
for(i in 1:10000)
  for(j in n)
    for(k in d)
    {
      x0 <- rnorm(j)
      x1 <- rnorm(j, mean=k)
      pvalue[zaehler,1] <- j
      pvalue[zaehler,2] <- k
      pvalue[zaehler,3] <- t.test(x0,x1)$p.value
      zaehler <- zaehler+1
    }
pvalsim <- data.frame(n=paste0("n=",pvalue[,1]), d=paste0("d=",pvalue[,2]), pvalue=pvalue[,3])

gf_histogram(~pvalue, data = pvalsim, bins=20, 
             breaks=seq(0, 1, by=0.025), fill=~ordered(pvalue<0.05)) %>%
  gf_refine(scale_y_log10()) %>%
  gf_vline(xintercept = ~0.05) %>%
  gf_facet_grid(n~d) %>%
  gf_theme(legend.position="bottom") %>%
  gf_labs(title="Ergebnis der Simulation mit logarithmischer y-Achse",
          fill = "p-Wert < 0.05") %>% 
  gf_refine(scale_x_continuous(name = "p-Wert", breaks = c(0, .5, 1)))


```


```{r rm-simu-values3, echo=FALSE}
rm(n)
rm(d)
rm(pvalue)
rm(zaehler)
rm(i)
rm(j)
rm(k)
rm(pvalsim)
```

### Exercise  `r nextExercise()`: Effect size and power {.exercise type=A-B-C answer=A}

Which option is correct?

A.  The probability of a type-2-error decreases with the effect size increasing.
B.  The probability of a type-2-error increases with the effect size increasing.
C.  The probability of a type-2-error is not associated with the effect size.


::: {.notes}
***A***
:::





### Sample size and effect size


Let the true effect amount to Cohen $d = 0.5$.


```{r echo = FALSE, fig.asp= .618, out.width = "70%"}
compute_d_ci <- function(n, d = 0.5) {
  
  d <- tibble(
    sample1 = rnorm(n = n),
    sample2 = rnorm(n = n, mean = d)  
  ) %>% 
    pivot_longer(cols = everything(), names_to = "Gruppe", values_to = "Wert")
  
  sample_d <- cohensD(Wert ~ Gruppe, data = d)
  
  return(d = sample_d)
  
}

s1 <- do(1000) * compute_d_ci(n = 30)
s2 <- do(1000) * compute_d_ci(n = 100)
s3 <- do(1000) * compute_d_ci(n = 1000)



p1 <- gf_histogram(~compute_d_ci, data = s1) %>% 
  gf_labs(title = "n = 30") %>% 
  gf_lims(x = c(0, 1.5)) %>% 
  gf_vline(xintercept = c(confint(s1)$lower[1], confint(s1)$upper[1]), linetype = "dashed")
p2 <- gf_histogram(~compute_d_ci, data = s2) %>% 
  gf_labs(title = "n = 100") %>% 
  gf_lims(x = c(0, 1.5)) %>% 
  gf_vline(xintercept = c(confint(s2)$lower[1], confint(s2)$upper[1]), linetype = "dashed")
p3 <- gf_histogram(~compute_d_ci, data = s3) %>% 
  gf_labs(title = "n = 1000") %>% 
  gf_lims(x = c(0, 1.5)) %>% 
  gf_vline(xintercept = c(confint(s2)$lower[1], confint(s3)$upper[1]), linetype = "dashed")

grid.arrange(p1, p2, p3, nrow = 1)
```

:::{.tiny}
Vertical dashed lines indicate borders of 95% confidence intervals.
:::


The higher n, the smaller the confidence interval (for $d$): The precision increases with the sample size (c.p.).






### Exercise  `r nextExercise()`: Best/worst students came from the smallest schools? {.exercise type=essay}


The Federal Department of Education presents a shiny new study, where the association of learn achievement and schools size (number of pupils) is scrutinized.

The findings reveal:

- The *best* pupils come more often than not from *small* schools.
- The *least best* pupils come from disproportionately often from *large* schools.


(Imagine the reactions from the press.)



\vspace{3cm}

Is that possible? Explain your answer.

:::{.notes}


**Yes**, it is possible. Small samples have - on average - a higher variance compared to larger samples (c.p.). Hence, results from smaller schools will vary more strongly (in comparison to larger schools), on average. That means, that school size will be associated with the precision of estimation. Even if all schools share the same level of achievements it is to be expected that the most extreme samples will stem from the smaller schools.


:::




### Effect sizes in the wild are small


>   The analysis reveals that the 25th, 50th, and 75th percentiles corresponded to correlation coefficients values of 0.12, 0.25, and 0.42 and to Hedges’ g values of 0.15, 0.38, and 0.69, respectively. 

```{r echo = FALSE, out.width="70%",}
knitr::include_graphics(file.path(pathToImages,"effectsizes-wild.png"))
```

Ca. 10000 correlation coefficients from 3498 papers were included for this study.

Published effect sizes are *overly optimistic* with regard to the ture value; [this paper](https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00813/full) found a 50% *decrease* in effect size in preregistered studies.



:::{.tiny}
Lovakov, A., & Agadullina, E. (2017, November 27). Empirically Derived Guidelines for Interpreting Effect Size in Social Psychology. https://doi.org/10.31234/osf.io/2epc4
:::

### Power in the wild is small (at least Neuro science/psychology)



- Bakker, van Dijk & Wicherts (2012):

  - Mean sample size: $n=40$
  - Mean (published) effect size: $d = 0.5; r = .21$
  
On this base, the average power is estimated at no more than **34 %**.


:::{.tiny}
Bakker, M., van Dijk, A., & Wicherts, J. M. (2012). The Rules of the Game Called Psychological Science. *Perspectives on Psychological Science, 7(6)*, 543–554.
:::


- [Szucs und Ioannidis (2017)](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2000797):

>   Median power to detect small, medium, and large effects was 0.12, 0.44, and 0.73, reflecting no improvement through the past half-century. 

>    Journal impact factors negatively correlated with power. 


:::{.tiny}
Szucs, D., & Ioannidis, J. P. A. (2017). Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature. PLOS Biology, 15(3), e2000797. https://doi.org/10.1371/journal.pbio.2000797
:::




### Why most published research findings are false



```{r p-most-studies-wrong, echo = FALSE, out.width="70%"}

grViz("digraph manystudies {

      node [fontname = Helvetica, shape = rectangle]        
      tab1 [label = '@@1']
      tab2 [label = '@@2']
      tab3 [label = '@@3']
      tab4 [label = '@@4']
      tab5 [label = '@@5']
      tab6 [label = '@@6']
      tab7 [label = '@@7']
      
      
      tab1 -> tab2;
      tab1 -> tab3;
      tab2 -> tab4;
      tab2 -> tab5;
      tab3 -> tab6;
      tab3 -> tab7;
      
}
      
      [1]: '1000 studies conduted'
      [2]: '100  real effects'
      [3]: '900 null effects'
      [4]: '50 effects detected'
      [5]: '50 effects missed'
      [6]: '810 null effects detected'
      [7]: '90 fals alarms'
      ")  
```






### Exercise `r nextExercise()`: p value is low means true effect deteced?  {.exercise type=essay}



The probability of rejecting a true $h_0$ equls 5 % (the significance level), right?

That means that 5 % of all studies will report false alarms, right?

**No, that's wrong**


We are interested in the probability $p$ of a true effect (E+), given that the test is significant (positive, $T+$): $p(E+|T+)$, *positive predictive value*.


Check out this [App](http://shinyapps.org/showapp.php?app=https://tellmi.psy.lmu.de/felix/PPV&by=Michael%20Zehetleitner%20and%20Felix%20Sch%C3%B6nbrodt&title=When%20does%20a%20significant%20p-value%20indicate%20a%20true%20effect?&shorttitle=When%20does%20a%20significant%20p-value%20indicate%20a%20true%20effect?) to see in which cases a statistically significant value is truely a real effect.


:::{.notes}

also see [here](https://www.nature.com/news/scientific-method-statistical-errors-1.14700)

:::


### Predictive values in the wild are small



```{r echo = FALSE, out.width="50%"}
knitr::include_graphics(file.path(pathToImages,"allfalse.png"))
```

PPV (Positive Predictive Value) of Research Findings for Various Combinations of Power ($1-\beta$), Ratio of True to Not-True Relationships (R), and Bias (u)

:::{.tiny}
https://doi.org/10.1371/journal.pmed.0020124.t004, Lizenz: [CC-BY](https://journals.plos.org/plosone/s/licenses-and-copyright)

Ioannidis, J. P. (2005). Why most published research findings are false. *PLoS medicine, 2(8)*, e124.

:::


## Practical power analysis


### Simple power analysis using R (Example)


A scientist suspects a strong effect, $d=0.9$, because previous findings from her lab indicate this effect sice. Now she would like to determine the "right" sample size for her upcoming experiment.




Some details:

- test: $t$-test for independent samples
- significance level: 4, no wait, 5 %
- power level to be obtained: 80 %
- Alternative hypothesis: one-sided (in favor of her treatment)



```{r eval = FALSE}
library(pwr)  # don't forget to install
pwr.t.test(d = 0.5, power = 0.8, alternative  = "greater", 
           type = "two.sample")
```


```{r echo = FALSE}
p <- pwr.t.test(d = 0.5, power = 0.8, alternative  = "greater", type = "two.sample")

```





### Exercise  `r nextExercise()`: Which tests are included in `pwr`? {.exercise type=essay}


Find out and report back which statistical tests are supported for power analysis in the R package `pwr`!



:::{.notes}

https://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.html



pwr.p.test: one-sample proportion test
pwr.2p.test: two-sample proportion test
pwr.2p2n.test: two-sample proportion test (unequal sample sizes)
pwr.t.test: two-sample, one-sample and paired t-tests
pwr.t2n.test: two-sample t-tests (unequal sample sizes)
pwr.anova.test: one-way balanced ANOVA
pwr.r.test: correlation test
pwr.chisq.test: chi-squared test (goodness of fit and association)
pwr.f2.test: test for the general linear model
::::




### At what sample size do correlations stabilize?



```{r echo = FALSE, out.width="70%",}
knitr::include_graphics(file.path(pathToImages,"evolDemo-1024x621.jpg"))
```

:::{.tiny}
Source: Felix Schönbrodt [osf](https://osf.io/5u6hv/), LIcence: [MIT](https://osf.io/b5ahz/)
:::


>    The bottom line is: For typical scenarios in psychology (i.e., rho = .21, w = .1, confidence = 80%), correlations stabilize when n approaches 250. That means, estimates with n > 250 are not only significant, they also are fairly accurate [Source:](https://www.nicebread.de/at-what-sample-size-do-correlations-stabilize/)


### Conservative estimation of power

Assume some study investigates a difference in means between two independent samples using a $t$-test. Literature reports an effect size of Cohen's $d=0.5$.

A conservative approach is to shrink the reported effect size to the lower end of a 60 % confidence interval^[Perugini, M., Gallucci, M., & Costantini, G. (2014). Safeguard Power as a Protection Against Imprecise Power Estimates. *Perspectives on Psychological Science, 9*(3), 319–332. http://doi.org/10.1177/1745691614528519].

```{r eval = FALSE}
library(MBESS)  # dont't forget to install
ci.smd(smd=0.5, n.1=30, n.2=30, conf.level=0.60)  # gives lower of 0.28

library(pwr)
pwr.t.test(d = 0.28, power = 0.8)  # gives n of 202 (per group)
```



### Estimate precision rather than power


The R package `MBESS` allows for estimating the precision of some effect estimation.


*Example*: Estimating the sample size (ss) for the accuracy in parameter estimation (aipe) of a standardized mean difference (smd) for a 95 % confidence level not larger than 0.5 $SD$ units:


```{r}
library(MBESS)  # get help: ?MBESS 
ss.aipe.smd(delta = .5, conf.level = .95, width = .5)  
```




### Posthoc-Power is too noisy



[Example]([https://statmodeling.stat.columbia.edu/2019/01/13/post-hoc-power-calculation-like-shit-sandwich/): In an interventional study the experimental groups finds that 94 (of 100) patients recover (ie., success); whereas in the control group the success rate is at 90 of 100 patients. Hence effect size is $p_d = p_e - p_c = 0.04$ (4 %); SE equals 0.04. What's the power of this study computed posthoc?




```{r}
se <- sqrt(0.94*0.06/100 + 0.90*0.1/100); se
```


The true effect (in the population) is estimated to be somewhere between $p_d \pm 2SE$, ie., $[-0.04;0.12]$. 

The power interval resulting from this interval is $[.05;0.92]$ -- that's what we call noisy.




:::::: {.columns}
::: {.column width="50%"}
```{r echo = FALSE, out.width="70%", fig.asp= .618}
gf_dist("norm", mean = 0, sd = 1, fill = ~ cut(x, c(1.65, Inf)),
          geom = "area", alpha = .5) %>% 
  gf_labs(fill = "", y = "", title = "Mu1 = 0;Power = 5%") %>% 
  gf_refine(theme(legend.position = "none"))

```


:::
::: {.column width="50%"}

```{r echo = FALSE, out.width="70%", fig.asp= .618}
gf_dist("norm",  geom = "area", alpha = .5) %>% 
  gf_dist("norm", mean = 3, sd = 1, fill = ~ cut(x, c(1.65, Inf)),
          geom = "area", alpha = .5) %>% 
  gf_labs(fill = "", y = "", title = "Mu1 = 3; Power = 92%") %>% 
  gf_refine(theme(legend.position = "none"))
```

:::
::::::


### Simulation based sample size planning

- Simulation based planning allows for more customized power analyses even for complex designs.

- For some designs, simulation based planning is the only feasible way or the least complicated way.

- [Introductory Casestudy](https://tobiasdienlin.com/2019/03/07/what-is-statistical-power-an-illustration-using-simulated-data/)

- The R package [simr](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504) is a convenient choice for simulation based power planning for multilevel studies.




## Example for simulation based power planning


### Illustrative research question


A research teams investigate the correlation $\rho$ of two (metric) variables:

$X$: Subjective loudness of a tone 

$Y$: Subjective pitch


Assume $\rho_{XY} = 0.3$.


How large should the sample be, to find the effect with high precision (high power)?



```{r}
my_rho <- .3

# benötigte Pakete:
library(MASS)
library(mosaic)
library(tidyverse)
```


### Correlation of two bivariate normal variables

$\rho = 0.3$


:::::: {.columns}
::: {.column width="70%"}
```{r echo = FALSE}
bivn <- mvrnorm(100000, mu = c(0, 0), Sigma = matrix(c(1, .3, .3, 1), nrow = 2))

# now we do a kernel density estimate
bivn.kde <- kde2d(bivn[,1], bivn[,2], n = 50)

persp(bivn.kde, phi = 45, theta = 30)
```


:::
::: {.column width="30%"}

```{r echo = FALSE}

bivn_df <- as_tibble(bivn) 

ggplot(bivn_df) +
  aes(x = V1, y = V2) +
  geom_point(alpha = .01) +
  stat_density_2d(aes(fill = ..density..), geom = "raster", contour = FALSE) +
  scale_fill_viridis_c()
```


:::
::::::


### Simulating a correlation


:::::: {.columns}
::: {.column width="40%"}

Correlation matrix:

```{r}
sigma <- matrix(c(1, my_rho, 
                  my_rho, 1), 
                nrow = 2)
sigma
```


Variances (main axis) equal 1; correlation equal 0.3.


:::
::: {.column width="55%"}

Population (mit $n=1000$ Fällen):

```{r}
pop_rho <- mvrnorm(n = 1000, 
                   mu = c(0, 0),  
                   Sigma = sigma) %>% 
  as_tibble()

head(pop_rho, 3)
```


:::
::::::


### Computing $r$ from samples

Draw samples of $n=30$:



```{r}
set.seed(42)
d <- sample_n(pop_rho, size = 30, replace = TRUE)
```

Compute $r$:

```{r}
sim_r <- cor(V1 ~ V2, data = d) 
sim_r_p <- cor_test(V1 ~ V2, data = d)$p.value

sim_r
sim_r_p
```


### Cast the previous steps into a function

That's simpler on the long run.

```{r}
simulate_r <- function(size, rho = .3, verbose = TRUE){
  
  sigma <- matrix(c(1, rho, rho, 1), nrow = 2) 
  pop_rho <- mvrnorm(n = 1e03, mu = c(0, 0),  #
                   Sigma = sigma) %>% as_tibble()
  d <- sample_n(pop_rho, size = size, replace = TRUE)
  sim_r <- cor(V1 ~ V2, data = d) 
  sim_r_p <- cor_test(V1 ~ V2, data = d)$p.value
  if (verbose == TRUE) cat("simulated r: ", round(sim_r, 2),"\n")
  return(c(r = sim_r, p = sim_r_p))
}

set.seed(42)
simulate_r(size = 50, verbose = FALSE)
```


### Determine extent of power analysis

The research teams decides to simulate the power of 30 different sample sizes $n \in \{10, 20, 30, \ldots , 300\}$, assuming that $\rho=0.3$. To account for sample variatoin, 30 trials for each sample size shall be simulated. In sum,  $30\cdot30=900$ are to be computed.


These 900 combinations will be stored here:

```{r}
combinations <- crossing(sizes = seq(from = 10, by = 10, 
                                     length.out = 30),
                 trials = 1:30)

glimpse(combinations)
```




### Now do the simulations

The R function `map(.x, .f)`^[from R package `purrr`,  part of `tidyverse`] will *map*  each element of `.x` to the function `.f`. The resulting list will be stored as a column:


```{r eval = FALSE}
r_simulated <- combinations %>% 
  mutate(r_list = map(.x = combinations$sizes, 
                      .f = ~ simulate_r(size = ., rho = my_rho))) 
head(r_simulated)
```


```{r echo = FALSE}
r_simulated <- combinations %>% 
  mutate(r_list = map(.x = combinations$sizes, .f = ~ simulate_r(size = ., rho = my_rho, verbose = FALSE))) 

head(r_simulated, 3)
```



The column `r_list` is a *list*, ie, it consists of several information ($r$ plus $p$-value in this case).




### Unnest the list column

The column `r_list` is a *list*, ie, it consists of several information ($r$ plus $p$-value in this case).

Let's *unnest* this list column in two coluns, one for $r$, one for $p$.


```{r}
r_simulated_unnested <- r_simulated %>% 
  unnest_wider(r_list) %>% 
  mutate(is_significant = p < .05)  # Prüfung, ob signifikant

head(r_simulated_unnested)
```


### Visualizing the power

:::::: {.columns}
::: {.column width="60%"}

Compute for each sample size the number of significant trials:


```{r}
r_sig_per_size <-  
  r_simulated_unnested %>% 
  group_by(sizes) %>% 
  summarise(power = 
              sum(is_significant) / n())
```


:::
::: {.column width="40%"}

```{r}
gf_line(power ~ sizes, 
        data = 
          r_sig_per_size) %>% 
  gf_smooth() %>% 
  gf_hline(yintercept = .8)
```


:::
::::::


### Visualizing the precision of estimation


:::::: {.columns}
::: {.column width="60%"}

```{r}
r_sim_per_size <- 
  r_simulated_unnested %>% 
  group_by(sizes) %>% 
  summarise(r_iqr = 
              iqr(r))
```




:::
::: {.column width="40%"}


```{r}
gf_line(r_iqr ~ sizes, 
        data = 
          r_sim_per_size) %>% 
  gf_smooth() %>% 
  gf_hline(yintercept = 0.1)
```



:::
::::::


