---
output: html_document
editor_options: 
  chunk_output_type: console
---
```{r setup-pred-modelling, include=FALSE}
# ---------------------------------------------------------------------------
#% maintainer:
#%   - Norman Markgraf
#%
# ---------------------------------------------------------------------------
source("../../prelude.R")
initPart(
    "Pred-Modelling",  # Dateiname ohne Suffix
    "StatisticalLearning"      # Verzeichnisname im Bilderverzeichnis 
    )
pathToImages = getPathToImages()
# ---------------------------------------------------------------------------


```




```{r libs-pred-modelling, include = FALSE}
library(mosaic)
library(ISLR)
library(here)
library(gridExtra)
library(caret)
library(corrr)
```





# Prädiktive Modellierung


## Grundlagen


### Das R-Paket `caret`^[Vgl. Kap. 22 aus Sauer, S. (2019). Moderne Datenanalyse mit R: Daten einlesen, aufbereiten, visualisieren und modellieren (1. Auflage 2019). Wiesbaden: Springer.]


- Das R-Paket `caret`^[http://topepo.github.io/caret/index.html] bietet eine einheitliche Syntax für viele statistische Modelle (aktuell `r modelLookup() %>%  distinct(model) %>% nrow()`).

```{r messages = FALSE}
library(caret)

modelLookup() %>%
  head()
```



### Modellanpassung mit `caret`

- Um ein Modell zu berechnen bzw. möglichst nah an einen Datensatz anzupassen (to train/fit), bietet `caret` folgende Syntax (für alle enthaltenen Modelle):

```{r eval = FALSE}
train(ergebnisvariable ~ prädiktorliste,
      data = meine_train_daten,
      method = "meine_methode",
      trControl = meine_resampling_methode)
```

- `trControl` definiert das Resampling-Schema.
- Außerdem können noch mit `tuneGrid` die Tuningparameter definiert werden.
- Hier sollte i.d.R. der *Train*-Datensatz verwendet werden, nicht der Test-Datensatz.


### Datensatz `GermanCredit`^[https://topepo.github.io/caret/data-sets.html#german-credit-data]




```{r}
data("GermanCredit")
dim(GermanCredit)  # Zeilen * Spalten
```

- Ergebnisvariable: `Class` mit den Stufen `good` und `bad`
- Viele (potenzielle) Prädiktorvariablen, z.B. Hauseigentümer, Geschlecht, Kreditanlass, Alter, Kredithöhe
- Der Datensatz zeigt die *Kreditwürdigkeit* einer Stichprobe von Kreditnehmern sowie eine Reihe potenzieller Prädiktoren.
- Anzahl kategorialer (nicht-numerischer) Spalten: `r GermanCredit %>% select_if(negate(is.numeric)) %>% ncol()`
- Kategoriale Variablen mit vielen Stufen können problematisch für einen Algorithmus sein. Es kann besser sein, nur die wichtigsten Stufen im Datensatz zu belassen und die übrigen zu entfernen.
- Da die Ergebnisvariable kategorial ist, handelt es sich um ein Klassifikationsproblem.


### Übung `r nextExercise()`: : Überblick zu `GermanCredit` {.exercise type=A-B-C answer=B}

Wie kann man sich die Verteilung der Ergebnisvariablen ausgeben lassen?

A.  `mean(~ Class, data = GermanCredit)`
B.  `tally(~ Class, data = GermanCredit)`
C.  `format(~ Class, data = GermanCredit)``


:::{.notes}


**B**  `tally(~ Class, data = GermanCredit)`

:::


### Übung `r nextExercise()`: Korrelationsmatrix {.exercise type=essay}

1. Visualisieren Sie die Korrelationsmatrix für `GermanCredit`!
  
2. Benennen Sie die höchsten Korrelationswerte!
  

::: {.notes}

Aufgabe 1

Korrelationsmatrix erstellen:

`Cred_rs <- GermanCredit %>%   select_if(is.numeric) %>% correlate()`

Korrelationsmatrix visualisieren:

`Cred_rs %>% rplot()`


Aufgabe 2

`Cred_rs %>% stretch() %>% arrange(-r)`

:::


### z-Skalierung

Für einige Modelle ist es sinnvoll, die Daten vorab zu z-skalieren (d.h. $MW=0; sd=0$).

```{r message=FALSE}
library(mosaic)
GermanCredit_z <- GermanCredit %>% 
  mutate_if(is.numeric, zscore)  # nur numerische Spalten
```

Test:

```{r}
mean(~ Amount, data =GermanCredit_z)  # nahezu Null
sd(~ Amount, data =GermanCredit_z)  # nahezu 1
```


### Trainings- vs. Testdatensatz

- Verteilung der Kreditwürdigkeit soll in beiden Teil-Datensätzen gleich (ähnlich) sein.
- 80%-Training-Datensatz, 20%-Test-Datensatz
- Zunächst erstellen wir einen Index (`idx`) mit den Fällen für den Train-Datensatz:

```{r}
train_idx <- createDataPartition(GermanCredit_z$Class, 
                                 p = .8, list = FALSE)

train <- GermanCredit_z %>% filter(row_number() %in% train_idx)
test <- GermanCredit_z %>% filter(!(row_number() %in% train_idx))
```



Die Verteilung kann mit `tally()` überprüft werden:
```{r eval = FALSE}
tally(~Class, data = train, format = "proportion")
tally(~Class, data = test, format = "proportion")

```


### Variablen ohne Varianz entfernen

- `nearZeroVar()` liefert Variablen ohne bzw. mit kaum Streuung zurück.
- Solche Variablen für Prognosen ungeeignet.

```{r}
nzv_vars <- train %>% nearZeroVar(saveMetrics = TRUE) %>% 
  rownames_to_column() %>% 
  filter(nzv == TRUE) %>% 
  pull(rowname)


train2 <- train %>% select(-one_of(nzv_vars))
dim(train2)
```


### Übung `r nextExercise()`: Gründe für das Fehlen von Varianz {.exercise type=essay}

1. Identifizieren Sie die Variablen ohne Varianz! 

2. Stellen Sie Vermutungen an, warum es zu Variablen ohne Varianz kommt!

:::{.notes}

individuell

:::

### Hoch korrelierte Variablen entfernen

- Hoch korrelierte Variablen sind redundant, d.h. sie bergen keine (bzw. wenig) Zusatzinformation. 
- Oftmals können solche Variablen eine Modellberechnung instabil machen^[aufgrund von Kollinearität].

```{r error = TRUE}
any_row_over_90 <- function(x) any(x > .9, na.rm = TRUE)

train2 %>% 
  select_if(is.numeric) %>% 
  correlate() %>% 
  focus_if(any_row_over_90)
```

- `focus_if()` zeigt nur Spalten, für die die Bedingung zutrifft.
- Es gibt offenbar keine hoch korrelierten Spalten.


## Modelle anpassen

### Kreuzvalidierung

```{r}
my_crossval <- trainControl(method = "cv",  #cross validation
                            number = 5)
```

- `cv` steht für Kreuzvalidierung; hier mit 5 Faltungen



### Modell 1: Logistische Regression


```{r glm-fit1a, eval = FALSE}
set.seed(42)  # Zufallszahlen fixieren
glm_fit1 <- train(Class ~ .,  # Modellgleichung
                  data = train2,  # Daten
                  method = "glm",  # Modell
                  family = "binomial",  # Parameter für `glm()`
                  trControl = my_crossval)  # Kreuzvalidierung
glm_fit1
```

- Die Auswahl der Prädiktoren sollte gut begründet sein, z.B. auf Basis einer Theorie.
- Hier wurden alle Prädiktoren aufgenommen, was die Berechnung des Modells instabil werden lassen kann. 

### Modell 1: Logistische Regression - Ausgabe


```{r glm-fit1b, echo = FALSE}
set.seed(42)  # Zufallszahlen fixieren
glm_fit1 <- train(Class ~ .,  # Modellgleichung
                  data = train2,  # Daten
                  method = "glm",  # Modell
                  family = "binomial",  # Parameter für `glm()`
                  trControl = my_crossval)  # Kreuzvalidierung
glm_fit1
```


### Modell 2: Random Forests - Tuningtabelle

- Tuningparameter: Anzahl der Variablen pro Baum (`mtry`), Mindestfallzahl  (`min.node.size`) in jedem Endknoten zur Verfügung und `splitrule`, die  festlegt, anhand welcher Statistik die Partitionierung erfolgt.
- Eine Faustregel für `mtry` ist $\sqrt{k}$, wobei $k$ die Anzahl der Prädiktoren im Datensatz benennt.

```{r rf-grid}
rf_grid <- expand.grid(.mtry = c(6, 7, 8),  # Variablen pro Baum
                       .min.node.size = c(10, 10),  # Mindestfallzahl
                       .splitrule = "gini")  # Statistik zur Partitionierung
rf_grid
```



### Modell 2: Random Forest - Modellanpassung


```{r rf-fit1a, eval = FALSE}
set.seed(42)
rf_fit1 <- train(Class ~ .,
                 data = train2,
                 method = "ranger",  # Random Forest
                 tuneGrid = rf_grid,
                 num.trees = 500,  # Anzahl der Bäume
                 importance =  "permutation")  # Variablenrelevanz
rf_fit1
```

- Für jeden Modellkandidaten (wie in der Tuningtabelle ) wird die Modellgüte anhand des Teiles des Datensatzes berechnet, der nicht in die Modellanpassung einging.
- Zur Berechnung der Variablenvarianz wird ein Prädiktor permutiert und so die (geringere) Güte berechnet. Die Differenz der Güte von permutierten zu Originaldatensatz gibt einen Hinweis auf die Relevanz des Prädiktors.^[https://cran.r-project.org/web/packages/ranger/ranger.pdf]


### Modell 2: Random Forest - Ausgabe

::: {.tiny}

```{r rf-fit1b, echo = FALSE}
set.seed(42)
rf_fit1 <- train(Class ~ .,
                 data = train2,
                 method = "ranger",  # Random Forest
                 tuneGrid = rf_grid,
                 num.trees = 500,  # Anzahl der Bäume
                 importance =  "permutation")  # Variablenwichtigkeit
rf_fit1
```

:::



### Modellgüte im Test-Datensatz

Entscheidend ist die Prognosegüte im Test-, nicht im Train-Datensatz.

Regressionsmodell:

```{r glm1-pred}
glm1_pred <- predict(glm_fit1, newdata = test)
postResample(pred = glm1_pred, obs = test$Class)
```

Random-Forest-Modell:

```{r rf1-pred}
rf1_pred <- predict(rf_fit1, newdata = test)
postResample(pred = rf1_pred, obs = test$Class)
```



### Konfusionsmatrix

::: {.tiny}

```{r conf-matrix}
confusionMatrix(data = rf1_pred, reference = test$Class)
```

:::

### Variablenrelevanz

- Normierte Ausgabe (bester Prädiktor entspricht 100%) der Relevanz aller Prädiktoren

```{r varimp-rf1}
varImp(rf_fit1)$importance %>% 
  rownames_to_column() %>% 
  arrange(-Overall) %>% 
  head()
```



## Fallstudie


### Übung `r nextExercise()`:  Fallstudie `nycflights13` {.exercise type = "essay"}


Bearbeiten Sie die Fallstudie zu den New Yorker Flüge



:::{.notes}

Individuell

:::

