```{r setup-Textmining, include=FALSE}
# ---------------------------------------------------------------------------
#% maintainer:
#%   - Karsten Lübke
#%
# ---------------------------------------------------------------------------
source("../prelude.R")
initPart(
    "Textmining",  # Dateiname ohne Suffix
    "Textmining"   # Verzeichnisname im Bilderverzeichnis 
    )
pathToImages <- getPathToImages()
# ---------------------------------------------------------------------------



```
# Text-Mining

<!-- ::: {.Sinnspruch} -->
<!-- Where is the wisdom we have lost in knowledge? Where is the knowledge we have lost in information? -->

<!-- [T.S. Eliot]{.Quelle} -->
<!-- ::: -->



### Lernziele {exclude-only=NOlernziele}


Die Studierenden ...


- wissen, wozu Text Mining verwendet wird bzw. verwendet werden kann und was die Grenzen dieser Methode sind.
- kennen zentrale Begriffe (z.B. Corpus, tf-idf).
- können eine grundlagende Textmining-Analyse in R durchführen.
- wissen, was man unter einer *Sentimentanalyse* versteht.


## Einführung

### Motivation: Texte (und Social Media)

Robinson (2016):^[Blogbeitrag [http://varianceexplained.org/r/trump-tweets/](http://varianceexplained.org/r/trump-tweets/): Text analysis of Trump's tweets confirms he writes only the (angrier) Android half]

> My analysis, shown below, concludes that the Android and iPhone tweets [@realDonaldTrump] are clearly from different people, posting during different times of day and using hashtags, links, and retweets in distinct ways. What’s more, we can see that the Android tweets are angrier and more negative, while the iPhone tweets tend to be benign announcements and pictures. 


### Vorbemerkung: Kurzvorstellung

- Wir werden Selbstdarstellung-Essays einer Online Partnerbörse analysieren. Daher *bitte* ich Sie, überlegen Sie, was Sie über sich dort schreiben würden. Die ersten zwei/ drei Sätze genügen 
- Vielleicht schreiben Sie es auf, bestenfalls auf Englisch
- Eventuell finden Sie sich in den Texten, die wir analysieren werden, wieder

```{r echo=FALSE, out.width = "20%", fig.align="center"}
knitr::include_graphics(file.path(pathToImages,"IBD.jpg"), error=FALSE)
```


### Vorbemerkungen: Text-Mining

- Ziel des Text Mining ist es Information aus Texten zu gewinnen
- Text Mining ist zunächst kein hermeneutisches Verfahren und hat nicht den Anspruch z. B. einer qualitativen Inhaltsanalyse
- Text Mining ist zunächst ein exploratives, quantitatives Verfahren auf qualitativen Text-Daten -- Inferenzmethoden sind aber möglich
- Innerhalb des Text Mining werden statistische und linguistische Methoden verwendet

### Grundbegriffe Text-Mining

- Eine Sammlung von Dokumenten wird Corpus genannt
- Ein Token (oder Term) ist eine Einheit eines Textes, häufig ein Wort
- Die Dokument-Term Matrix ist eine (dünnbesetzte) Matrix, mit den Dokumenten als Zeilen und den Termen als Spalten
- Ein Topic ist ein Thema

### Übung `r nextExercise()`: Grundbegriffe {.exercise type=A-B-C answer=A}

Eine Analystin untersucht alle Unternehmensmeldungen eines Unternehmens der letzten 12 Monate. Um was handelt es sich bei den Unternehmensmeldungen?

A.  Corpus
B.  Token
C.  Topic

<div class="notes">

Eine Sammlung von Dokumenten (hier: Unternehmensmeldungen) wird Corpus (***A***) genannt.

</div>

### Tidy-Text

Im R Paket `tidytext`^[[Silge, J und Robinson, D (2016)](http://dx.doi.org/10.21105/joss.00037)] wird Text als *Tidy* Daten^[[Wickham, H (2014)](http://dx.doi.org/10.18637/jss.v059.i10)] verwendet: 

Jede Zeile enthält nur einen Term -- ggf. mit Zusatzinformationen wie Dokument, Autor/in etc..

**Literatur:** *Text Mining with R -- A Tidy Approach* von Julia Silge und David Robinson: [https://tidytextmining.com/](https://tidytextmining.com/) 

### Pakete installieren

Es gibt sehr viele R Pakete, die sich mit Text Analyse und Natural Language Processing beschäftigen^[[https://cran.r-project.org/web/views/NaturalLanguageProcessing.html](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html)]. Für die folgenden Beispiele (und mehr...) bitte folgende Pakete (zusätzlich zu `mosaic`) - die Installation ist nur einmalig nötig!

```{r, eval=FALSE}
install.packages(c("tidyverse", "tidytext", "wordcloud", 
                   "okcupiddata"))
```

Pakete laden:
```{r, message=FALSE}
library(mosaic)
library(tidyverse)
library(tidytext)
library(wordcloud)
library(okcupiddata)
```


###  Einführung Tidy-Text (1/4)

Text als Vektor von Satzteilen:

```{r}
text <- c("Dies ist ein schöner Text.",
          "Er hat mehrere Zeilen, enthält Satzeichen,",
          "Groß- und Kleinschreibung - ",
          "und Zahlen: 96, Ole!")

text
```

### Einführung Tidy-Text (2/4)

Speichern als Datensatz:

```{r FALSE}
text_df <- tibble(Zeile=1:length(text), text)

text_df
```

### Einführung Tidy-Text (3/4)

Überführung in Tidy Text: `unnest_token()`: Satzzeichen entfernen, alles kleinschreiben. Wörter, d. h. Buchstaben filtern:

```{r, eval=FALSE}
text_df %>%
  unnest_tokens(word, text) %>%
  filter(str_detect(word, "[a-z]"))
```

### Einführung Tidy-Text (4/4)

::: {.scriptsize}

```{r, echo=FALSE}
text_df %>%
  unnest_tokens(word, text) %>%
  filter(str_detect(word, "[a-z]"))
```

:::

### OkCupid Data

(Bereinigte) Profildaten von OKCupid Nutzern.^[[Kim, AY, Escobedo-Land, A (2015)](https://ww2.amstat.org/publications/jse/v23n2/kim.pdf)] Siehe auch [https://github.com/rudeboybert/JSE_OkCupid](https://github.com/rudeboybert/JSE_OkCupid). Hier besonders interessant: `essay0`:


> Response to first essay question (my self summary), trimmed to 140 characters.


```{r, results='asis'}
# Datensatz einlesen
data("profiles")

# Als "tibble" Datensatz definieren
profiles <- as_tibble(profiles)

# 1. Essay anzeigen
profiles$essay0[1]
```


### Tidy Essay

Aus dem Datensatz `profiles` relevante Variablen extrahieren, Essaynummer hinzufügen, fehlende Werte löschen, in Tidy Text überführen und Wörter filtern:

```{r, results='hide'}
tidy_essay <- profiles %>% 
  select(sex, essay0) %>%
  mutate(essay=row_number()) %>%
  na.omit() %>%
  unnest_tokens(word, essay0) %>%
  filter(str_detect(word, "[a-z]"))

tidy_essay
```

### Tidy Essay (Fortsetzung)

```{r, echo=FALSE}
tidy_essay
```

### Übung `r nextExercise()`: Wortverteilungen {.exercise type=yes-no answer=no}

Was glauben Sie: werden alle Wörter in etwa gleich häufig eingesetzt?

- Ja.
- Nein.

<div class="notes">
***Nein***: nur wenige Wörter werden häufig (in Deutsch z.B. der, die, das usw.) verwendet, die meisten Wörter selten (z. B. Grottenolm). Siehe auch weitere Analyse.
</div>


## Worthäufigkeiten

### Häufigste Wörter

Wörter zählen, gruppiert nach Geschlecht:
 
```{r}
word_freq <- tidy_essay %>% 
  group_by(sex) %>%
  count(word, sort = TRUE)
```

### Häufigste Wörter (Fortsetung)

```{r}
word_freq
```


### Zipfsches Gesetz

Die Häufigkeit ($h_i$ bzw. $f_i$) und der Rang ($r_i$) eines Wortes $i$ stehen ungefähr in einem umgekehrt proportionalen Zusammenhang, d. h. $$f_i \cdot r_i \approx c$$:

```{r}
# Wörter insgesamt je Geschlecht
total_words <- word_freq %>% 
  group_by(sex) %>%
  summarize(total = sum(n))

# Wörter je Geschlecht und insgesamt verbinden,
# Rang und Häufigkeit bilden
freq_by_rank <- left_join(word_freq, total_words) %>% 
  group_by(sex) %>%
  mutate(Rang = row_number(), 
         Häufigkeit = n/total)
```


### Abbildung Zipfsches Gesetz 

Liniendiagramm: $r_i$ auf horizontaler Achse, $f_i$ auf vertikaler Achse; logarithmische Skalierung der Achsen:

```{r, eval=FALSE}
gf_line(Häufigkeit ~ Rang , data = freq_by_rank, color= ~sex) %>%
  gf_refine(scale_x_log10()) %>%
  gf_refine(scale_y_log10())
```


### Abbildung Zipfsches Gesetz (Fortsetzung)

```{r, echo=FALSE, out.width="80%", fig.align="center"}
gf_line(Häufigkeit ~ Rang , data = freq_by_rank, color= ~sex) %>%
  gf_refine(scale_x_log10()) %>%
  gf_refine(scale_y_log10())
```


### Stoppwörter 

Stoppwörter sind häufig auftretende Wörter, von denen man annimmt, dass sie wenig Informationen zur Dokumentenanalyse enthalten. Durch das entfernen von Stoppwörtern gehen natürlich unter Umständen für die Analyse relevante Informationen verloren.^[Deutsche Stoppwörter u.a. über die Funktion `stopwords("german")` im Paket `tm`.] 

```{r, results='hide'}
# Stoppwörter einlesen
data("stop_words")

# Stoppwörter aus Essay entfernen
tidy_essay <- tidy_essay %>% 
 anti_join(stop_words)

# Wörter zählen
tidy_essay %>% 
  count(word, sort = TRUE)
```

### Stoppwörter (Fortsetzung)

```{r, echo=FALSE}
tidy_essay %>% 
  count(word, sort = TRUE)
```


### Übung `r nextExercise()`: Worthäufigkeiten {.exercise type=A-B-C answer=C}

Was glauben Sie: welches der folgenden Wörter wird am häufigsten am Anfang einer Selbestdarstellung verwendet?

A.  `beer` 
B.  `champagne`
C.  `wine`

<div class="notes">

`tidy_essay %>% count(word, sort = TRUE) %>% filter(word %in% c("beer", "champagne", "wine"))`


```{r, echo=FALSE}
tidy_essay %>% 
  count(word, sort = TRUE) %>% 
  filter(word %in% c("beer", "champagne", "wine")) %>% 
  knitr::kable()
```

Also ***C***.

</div>


### Abbildung Worthäufigkeiten

```{r, eval=FALSE}
# Datensatz mit den 25 häufigsten Wörtern
top_word <- tidy_essay %>%
  count(word, sort = TRUE) %>%
  mutate(word = reorder(word, n)) %>%
  top_n(25) 

# Säulendiagramm
gf_bar(n ~ word, stat = "identity", data = top_word) %>%
  gf_refine(coord_flip())
```


### Abbildung Worthäufigkeiten (Fortsetzung)

```{r, echo=FALSE, out.width="80%", fig.align="center"}
# Datensatz mit den 25 häufigsten Wörtern
top_word <- tidy_essay %>%
  count(word, sort = TRUE) %>%
  mutate(word = reorder(word, n)) %>%
  top_n(25) 

# Säulendiagramm
gf_bar(n ~ word, stat = "identity", data = top_word) %>%
  gf_refine(coord_flip())
```

### Word-Cloud 

Word-Cloud^[Paket `wordcloud`: [Fellows, I (2014)](https://CRAN.R-project.org/package=wordcloud)] der 100 häufigsten Wörter:

```{r, eval=FALSE}
# Worthäufigkeiten
wcf <- tidy_essay %>%
  count(word, sort = TRUE)

set.seed(1896) # Reproduzierbarkeit

# Wordcloud
wordcloud(words = wcf$word, freq = wcf$n, max.words = 100, 
          scale = c(2,.5), colors=brewer.pal(6, "Dark2"))
```


### Word-Cloud (Fortsetzung)

```{r, echo=FALSE, out.width="60%", fig.align="center"}
wcf <- tidy_essay %>%
  count(word, sort = TRUE)

set.seed(1896)

wordcloud(words = wcf$word, freq = wcf$n, max.words = 100, 
         scale = c(2,.5), colors=brewer.pal(6, "Dark2"))
```

## Vergleich von Worthäufigkeiten

### Übung `r nextExercise()`: Geschlecht {.exercise type=A-B answer=B}

Welches Geschlecht hat wohl die Autorin, der Autor des Essays

```{r, results="asis"}
profiles$essay0[2]
```


A.  weiblich
B.  männlich


<div class="notes">

`profiles$sex[2]`

```{r, echo=FALSE}
profiles$sex[2] %>% knitr::kable()
```

Also ***B***.

Wie sind *Sie* darauf gekommen?

</div>


### Relative Häufigkeiten je Geschlecht

Gruppiert nach Geschlecht: Wörter zählen, ergänzen um die Gesamtzahl der Wörter und so relative Häufigkeit der Wörter berechnen:

```{r, results='hide'}
frequency <- tidy_essay %>%
  group_by(sex) %>%
  count(word, sort = TRUE) %>%
  left_join(tidy_essay %>%
              group_by(sex) %>%
              summarise(total = n())) %>%
  mutate(freq = n/total) %>%
  select(sex, word, freq)

frequency
```

### Relative Häufigkeiten je Geschlecht (Fortsetzung)

```{r, echo=FALSE}
frequency
```


### Wörter und relative Häufigkeiten der Geschlechter

*Langen* Datensatz in einen *breiten* überführen: das Geschlecht soll die Variable für die Häufigkeit sein:

```{r, results='hide'}
frequency <- frequency %>%
  spread(sex, freq) 
```


### Übung `r nextExercise()`: Geschlecht: Wein {.exercise type=A-B answer=A}

Welches Geschlecht verwendet wohl das Wort `wine` häufiger?



A.  weiblich
B.  männlich


<div class="notes">

`frequency %>% filter(word=="wine")`

```{r, echo=FALSE}
frequency %>% filter(word=="wine") %>% knitr::kable()
```

Also ***A***.


</div>

### Übung `r nextExercise()`: Geschlecht: Bier {.exercise type=A-B answer=B}

Welches Geschlecht verwendet wohl das Wort `beer` häufiger?



A.  weiblich
B.  männlich


<div class="notes">

`frequency %>% filter(word=="beer")`

```{r, echo=FALSE}
frequency %>% filter(word=="beer") %>% knitr::kable()
```

Also ***B***.


</div>


### Abbildung Vergleich Worthäuigkeiten 

Scatterplot mit den relativen Worthäufigkeiten der Männer auf der $x$-Achse, der der Frauen auf der $y$-Achse, Punkte mit dem dazugehörigen Wort beschriften, Achsen logarithmisch skalieren und Winkelhalbierende einzeichnen:

```{r, eval=FALSE}
gf_point(f ~ m, data = frequency) %>% 
  gf_text(f ~ m, label =~word, data = frequency, 
          check_overlap = TRUE, vjust = 1.5) %>%
  gf_refine(scale_x_log10()) %>%
  gf_refine(scale_y_log10()) %>%
  gf_abline(intercept = ~0, slope = ~1, color = "red")
```

### Abbildung Vergleich Worthäuigkeiten (Fortsetzung)

```{r, echo=FALSE, out.width="80%", fig.align="center"}
gf_point(f ~ m, data = frequency) %>% 
  gf_text(f ~ m, label =~word, data = frequency, 
          check_overlap = TRUE, vjust = 1.5) %>%
  gf_refine(scale_x_log10()) %>%
  gf_refine(scale_y_log10()) %>%
  gf_abline(intercept = ~0, slope = ~1, color = "red")
```


### Ergebnis Vergleich relativer Häufigkeiten 

- Die meisten Wörter werden sehr selten verwendet
- Wenige Wörter werden häufiger verwendet
- Viele Wörter werden von beiden Geschlechtern in etwa gleich oft verwendet, einige aber nicht


### Essays mit `love` und `sex`

Überprüfen ob das verwendete Wort `love` bzw. `sex` ist, und je Essay zusammenfasen ob mindestens einmal `love` bzw. `sex` vorkommt:

```{r}
love_sex <- tidy_essay %>%
  mutate(haslove = (word=="love")) %>%
  mutate(hassex = (word=="sex")) %>%
  group_by(essay) %>%
  summarise(haslove = any(haslove) , hassex = any(hassex))
```

### Vergleich `love` und `sex`

Sagen die, die `love` sagen auch `sex`?

Kreuztabelle mit relativer Häufigkeit:

```{r}
tally(hassex ~ haslove, data = love_sex, 
      format = "proportion")
```

### $\chi^2$-Unabhängigkeitstest

::: {.scriptsize}

```{r}
xchisq.test(hassex ~ haslove, data = love_sex)
```

:::


### Übung `r nextExercise()`: Geschlecht: `travel` {.exercise type=yes-no answer=yes}

Untersuchen Sie: gibt es einen signifikanten ($\alpha=0.05$) Geschlechtsunterschied in der Verwendung des Wortes `travel`?

- Ja.
- Nein.


```{r, include=FALSE}
gender_travel <- tidy_essay %>% 
  mutate(hastravel = (word=="travel")) %>% 
  group_by(sex, essay) %>% 
  summarise(hastravel = any(hastravel))
pchi <- pval(xchisq.test(hastravel ~ sex, data = gender_travel))
```

<div class="notes">

***Ja***:

```gender_travel <- tidy_essay %>% mutate(hastravel = (word=="travel")) %>%   group_by(sex, essay) %>%   summarise(hastravel = any(hastravel))```

```tally(hastravel ~ sex, data = gender_travel, format="proportion")```

```xchisq.test(hastravel ~ sex, data = gender_travel)```


Mit einem p-Wert von $`r pchi`$ wird $H_0$ verworfen, aber der Unterschied ist insgesamt nicht groß: $p_m-p_f=`r diffprop(hastravel ~ sex, data = gender_travel)`$.


</div>


## Sentimentanalyse

### Sentiment und Emotionen

Das NRC Word-Emotion Association Lexicon^[[Mohammad, A und Turney, P (2013)](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)] enthält eine Liste von englischen Wörtern und ihren Zusammenhang mit acht grundlegenden Emotionen, sowie ihr Sentiment, d. h. Polarität. Für Forschungszwecke ist es frei (kostenlos) verwendbar.^[Für ein deutsches Lexikon siehe z.B. SentiWS [http://wortschatz.uni-leipzig.de/de/download](http://wortschatz.uni-leipzig.de/de/download).]

Laden Sie das NRC-Wörterbuch herunter (in Ihr Arbeitsverzeichnis) und importieren Sie es in R:

::: {.scriptsize}

```{r read-nrc-no-eval, eval = FALSE, echo = TRUE}
# Daten aus Textdatei importieren:
nrc <- read_delim(file = "NRC-Emotion-Lexicon-Wordlevel-v0.92.txt",
                  delim = "\t", col_names = FALSE)
# Spalten umbenennen:
nrc <- nrc %>% rename(word = X1, sentiment = X2, flag = X3)
# Nur Zeilen belassen, für die eine Emotion vorliegt:
nrc <- nrc %>% filter(flag == 1)
```

:::

```{r read-nrc, eval = TRUE, echo = FALSE}
nrc <- read_delim(file = "../datasets/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt",
                  delim = "\t", col_names = FALSE)
# Spalten umbenennen:
nrc <- nrc %>% rename(word = X1, sentiment = X2, flag = X3)
# Nur Zeilen belassen, für die eine Emotion vorliegt:
nrc <- nrc %>% filter(flag == 1)
```


### Sentiment je Geschlecht

Sentiment Lexikon verbinden, Geschlecht und Sentiment zählen, ergänzen um die Gesamtzahl der Wörter und so relative Häufigkeit je Sentiment berechnen:

```{r, results='hide'}
sentiment_by_sex <- tidy_essay %>%
  inner_join(nrc) %>%
  count(sex, sentiment) %>%
  left_join(tidy_essay %>%
              group_by(sex) %>%
              summarise(total = n())) %>%
  mutate(freq=n/total) %>%
  select(sex, sentiment, freq)
```


###  Vergleich Sentiment je Geschlecht

*Langen* Datensatz in einen *breiten* überführen: das Geschlecht soll die Variable für die Häufigkeit des Sentiments sein:

```{r}
sentiment_by_sex <- sentiment_by_sex %>%
  spread(sex, freq)
```


### Abbildung Sentiment je Geschlecht

Scatterplot mit den relativen Sentimenthäufigkeiten der Männer auf der $x$-Achse, die der Frauen auf der $y$-Achse, beschriften, Achsen logarithmisch skalieren und Winkelhalbierende einzeichnen:

```{r, eval=FALSE}
gf_point(f ~ m, data = sentiment_by_sex) %>% 
  gf_text(f ~ m, label =~sentiment, data =sentiment_by_sex, 
          vjust = 1.5) %>%
  gf_refine(scale_x_log10()) %>%
  gf_refine(scale_y_log10()) %>%
  gf_abline(intercept = ~0, slope = ~1, color = "red")
```

### Abbildung Sentiment je Geschlecht (Fortsetzung)

```{r, echo=FALSE, out.width="80%", fig.align="center"}
gf_point(f ~ m, data = sentiment_by_sex) %>% 
  gf_text(f ~ m, label =~sentiment, data =sentiment_by_sex, 
          vjust = 1.5) %>%
  gf_refine(scale_x_log10()) %>%
  gf_refine(scale_y_log10()) %>%
  gf_abline(intercept = ~0, slope = ~1, color = "red")
```


### Übung `r nextExercise()`: Log Odds Ratio  {.exercise type=A-B answer=A}

Unterschiedliche Anteile können über die Log Odds Ratios untersucht werden:

$$\text{log odds ratio}=\ln \left(\frac{(\frac{n}{total})_{female}}{(\frac{n}{total})_{male}} \right)$$


Wann ist das Log Odds Ratios für Frauen und Männer gleich?

A.  Bei $\text{log odds ratio}=0$.
B.  Bei $\text{log odds ratio}=1$.

<div class="notes">

Da $\ln 1=0$: ***A***. 

</div>

### Übung `r nextExercise()`: Unterschiede Sentiment  {.exercise type=A-B-C-D answer=C}

Untersuchen Sie: bei welchem Sentiment ist $|\text{log odds ratio}|$ maximal?


A.  `fear`
B.  `disgust`
C.  `joy` 
D.  `surprise`


```{r, include=FALSE}
sentiment_by_sex <- sentiment_by_sex %>%
  mutate(logoddratio=log(f/m)) %>%
  arrange((abs(logoddratio)))
```

<div class="notes">

```sentiment_by_sex <- sentiment_by_sex %>% mutate(logoddratio=log(f/m)) %>% arrange((abs(logoddratio)))```

```{r, echo=FALSE}
knitr::kable(sentiment_by_sex)
```

`joy` (***C***)

</div>


### Zusammenfassung Ergebnisse OkCupid

- Es gibt Wörter die von Frauen und Männern unterschiedlich oft am Anfang eines Selbstdarstellung-Essays einer Kontaktbörse verwendet werden^[Fast schon eine Tautologie...].
- Es gibt Wörter, die häufiger in Zusammenhang mit anderen Wörtern verwendet werden.
- Teilweise gibt es Unterschiede in der Emotionalität der verwendeten Wörter.


### Limitierung (im Beispiel)

- Nur den Anfang der Essays verwendet.
- Nur einzelne Wörter verwendet, keine Verneinungen, Wortkombinationen etc..
- Keine Synonyme^[siehe z.B. Paket `wordnet`], keine Wortstämme^[siehe z.B. Paket `SnowballC`].
- Keine Satzzeichen oder Emoticons.
- Keine Ironie.
- Keine Unterteilung nach sexueller Orientierung.

##  Ausblick

### Document-Term-Matrix

- Die (dünnbesetzte) Matrix mit den Dokumenten als Zeilen und den Worthäufigkeiten als Spalten ist die Datengrundlage z. B. für Dokumentenklassifikation oder auch Wortkorrelationen
- Viel Funktionalität damit -- aber auch für die Textvorverarbeitung -- ist im Paket `tm`^[[Feinerer, I, Hornik, K und Meyer, D (2008)](http://dx.doi.org/10.18637/jss.v025.i05)] enthalten

### Tf-idf

Beurteilung der Relevanz von Termen in Dokumenten eines Corpus:

- (Gewichtete) Term Frequency: $\mathit{tf}_{i,j}=\frac{n_{i,j}}{\sum_k n_{k,j}}$ mit $n_{i,j}$ der Häufigkeit des Terms $t_i$ im Dokument $d_j$
- Inverse Document Frequency eines Terms $t_i$: $\mathit{idf}_i = \log_2 \frac{|D|}{|\{d \mid t_i \in d\}|}$ mit $|D|$ der Gesamtzahl der Dokumente und $|\{d \mid t_i \in d\}|$ der Anzahl Dokumente, die den Term $t_i$ enthalten
- Term Frequency - Inverse Document Frequency: $\mathit{tf}_{i,j} \cdot \mathit{idf}_i$

Die einfache Worthäufigkeit wird durch eine gewichtete Variante ersetzt: Seltene Wörter sind wichtiger als häufige Wörter.


### Latent Dirichlet Allokation (LDA)

> LDA ist ein dreistufiges, hierarchisches bayesianisches Modell, indem jedes Dokument eines Corpus als Mischverteilung über einer zugrunde liegenden Menge von Themen modelliert wird. Jedes Thema wiederum wird modelliert als Mischverteilung über einer zugrunde liegenden Menge von Themenwahrscheinlichkeiten.^[[Blei, DM, Ng, AY und Jordan, MI (2003), Übersetzung KL](http://jmlr.csail.mit.edu/papers/v3/blei03a.html)]

Diese und weitere Funktionalität ist im Paket `topicmodels`^[[Grün, B und Hornik, K (2011)](http://doi.org/10.18637/jss.v040.i13)] implementiert.

### Modell LDA

- Ein Dokument wird aufgefasst als eine Mischung von zugrunde liegenden Themen, z. B. 2/3 Topic A (Statistik), 1/3 Topic B (BWL)
- Wörter werden als Stichprobe einer zugrunde liegenden, vom Thema abhängigen Verteilung, gezogen: z. B. $$P(\text{Wort}=\text{p-Wert}|\text{Thema}=\text{Statistik})=0.1$$
- Beim Schreiben eines Dokumentes werden "zufällig" Themen und aus diesen "zufällig" Wörter verwendet^[Bag-of-words model]
- LDA versucht dieses zu rekonstruieren. Dabei werden Verteilungsannahmen mit unbekannten, zu schätzenden Parametern und deren Prior Verteilung verwendet ^[[Darstellung in Anlehnung an Blog Chen, E (2011)](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/)]


### Zusammenfassung Text-Mining

- Ziel des Text Mining ist es Information aus Texten zu gewinnen
- Text Mining ist zunächst kein hermeneutisches Verfahren und hat nicht den Anspruch z. B. einer qualitativen Inhaltsanalyse
- Text Mining ist zunächst ein exploratives, quantitatives Verfahren auf qualitativen Text-Daten -- Inferenzmethoden sind aber möglich
- Innerhalb des Text Mining werden statistische und linguistische Methoden verwendet



```{r finish-Textmining, include=FALSE}
rm(pathToImages)
rm(list=c("freq_by_rank", "frequency", "gender_travel", "love_sex", "pchi", "profiles", "sentiment_by_sex",
          "stop_words", "text", "text_df", "tidy_essay", "top_word", "total_words", "wcf", "word_freq"))
detach("package:okcupiddata", unload = TRUE)
detach("package:wordcloud", unload = TRUE)
detach("package:tidytext", unload = TRUE)
detach("package:tidyverse", unload = TRUE)
finalizePart(partname)
```




