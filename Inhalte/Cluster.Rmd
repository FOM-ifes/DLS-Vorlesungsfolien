```{r setup-Clusteranalyse, include=FALSE}
# ---------------------------------------------------------------------------
#% maintainer:
#%   - Karsten Luebke
#%
# ---------------------------------------------------------------------------
source("../prelude.R")
initPart(
    "Cluster",       # Dateiname ohne Suffix
    "Clusterananlyse"  # Verzeichnisname im Bilderverzeichnis 
    )
pathToImages <- getPathToImages()
# ---------------------------------------------------------------------------

library(mosaic)
library(knitr)
library(viridis)

data("iris")
iris.s <- iris[,3:4] %>%
  scale() %>%
  data.frame()

tips <- assertData("tips.csv", "https://goo.gl/whKjnl")
```
# Clusteranalyse



### Lernziele {exclude-only=NOlernziele}

Die Studierenden ...

- wissen, was ein Distanzmaß ist und wozu es verwendet wird im Zusammenhang mit der Clusteranalyse.
- können ein Beispiel für ein Distanzmaß nennen und die Formel nennen.
- kennen den Ablauf der k-Means-Clusteranalyse und können ihn in R ausführen.
- kennen zumindest ein Kriterium bzw. Verfahren, um die Anzahl der Cluster zu bestimmen.



### Unüberwachtes Lernen

**Unüberwachtes Lernen** (engl.: unsupervised learning): Es gibt *keine* bekannte abhängige Variable $y$, die modelliert werden soll

Methoden (u. a.):

- **Hauptkomponentenanalyse** (engl.: Principal Component Analysis): Finde (wenige) Linearkombinationen der Variablen: Zusammenfassung von Variablen, Dimensionsreduktion. `prcomp()`
- **Clusteranalyse** (engl.: Cluster Analysis): Finde Gruppen (Cluster) von Beobachtungen, die innerhalb der Cluster homogen, zwischen den Clustern heterogen sind^[Clustern von Variablen analog]. `kmeans()`

### Beispiele

- Finden von homogenen Studierendengruppen, um ihnen gezielt Materialien zur Verfügung zu stellen.
- Analysieren, inwieweit es unterschiedliche Gruppen von Unternehmen am Markt gibt.
- Finden von Kundensegmenten auf Basis von Kundendaten.
- Identifizieren von Clustern (Typen) verschiedener Führungspersönlichkeiten.

Bayrischer Rundfunk: Ein Marktforscher erklärt die Milieu-Typen -- Faszination Wissen -- Clip vom 30.3.2015: [https://youtu.be/3MbFKuL80jk](https://youtu.be/3MbFKuL80jk)

*Wo können Sie dies Verfahren einsetzen?*


### Vorbereitung: Trinkgelddaten

Einlesen der "Tipping"^[Bryant, P. G. and Smith, M. (1995). Practical Data Analysis: Case Studies in Business Statistics. Homewood, IL: Richard D. Irwin Publishing] Daten sowie laden des Pakets `mosaic`.

```{r, eval= FALSE, message=FALSE}
download.file("https://goo.gl/whKjnl", destfile = "tips.csv")
tips <- read.csv2("tips.csv")
# Alternativ - heruntergeladene Datei einlesen:
# tips <- read.csv2(file.choose()) 

library(mosaic) # Paket laden
library(viridis)
```


### Vorbereitung: Skalierung numerischer Variablen

```{r, echo=FALSE}
tipsscale <- tips %>% 
  dplyr::select(size, total_bill, tip) %>% # Variablen wählen
  scale() %>%                              # Skalieren 
  data.frame()                             # Als Datensatz definieren
```
```{r, eval=FALSE}
tipsscale <- tips %>% 
  select(size, total_bill, tip) %>% # Variablen wählen
  scale() %>%                       # Skalieren 
  data.frame()                      # Als Datensatz definieren
```


### Methoden der Clusteranalyse

U.a.: **Partitionierend**: Beobachtungen werden zu $k$ Clustern zusammengefasst.^[Methoden um $k$ (häufig: $k \in \{2, 3, \ldots, 10\}$, aber unbekannt!) zu finden siehe z. B. Charrad M., Ghazzali N., Boiteau V. and Niknafs, A. (2014). NbClust: An R Package for Determining the Relevant Number of Clusters in a Data Set. Journal of Statistical Software, 61(6), 1-36. [http://dx.doi.org/10.18637/jss.v061.i06](http://dx.doi.org/10.18637/jss.v061.i06)]

Ähnlichkeit/Unähnlichkeit wird dabei über **Distanzmaße** bestimmt. Für metrische Merkmale z. B. euklidischer Abstand: $$d(x,y)=\sqrt{\sum_j{(x_j-y_j)^2}}$$


### Unterschiedliche Cluster $k$

```{r, echo=FALSE, fig.align="center", out.width="90%"}
set.seed(1896)
cluster.iris2 <- kmeans(iris.s, centers=2)
cluster.iris3 <- kmeans(iris.s, centers=3)
cluster.iris4 <- kmeans(iris.s, centers=4)
cluster.iris5 <- kmeans(iris.s, centers=5)

p2 <- gf_point(
  Petal.Width ~ Petal.Length, data=iris.s, xlab="x", ylab="y", col=~factor(cluster.iris2$cluster), title="k=2", 
  show.legend=FALSE
  ) + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold")) + scale_color_viridis(discrete = TRUE, option = "D")

p3 <- gf_point(
  Petal.Width ~ Petal.Length, data=iris.s, xlab="x", ylab="y", col=~factor(cluster.iris3$cluster), title="k=3", show.legend=FALSE) + theme(plot.title = element_text(hjust = 0.5, face = "bold")) + scale_color_viridis(discrete = TRUE, option = "D")

p4 <- gf_point(
  Petal.Width ~ Petal.Length, data=iris.s, xlab="x", ylab="y", col=~factor(cluster.iris4$cluster), title="k=4",
  show.legend=FALSE) + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold")) + scale_color_viridis(discrete = TRUE, option = "D")

p5 <- gf_point(
  Petal.Width ~ Petal.Length, data=iris.s, xlab="x", ylab="y", col=~factor(cluster.iris5$cluster), title="k=5",
  show.legend=FALSE) + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold")) + scale_color_viridis(discrete = TRUE, option = "D")

gridExtra::grid.arrange(p2,p3,p4,p5, ncol=2)
```


### Distanz

```{r}
tipsscale %>% head(n = 3) # Erste drei Beobachtungen

tipsscale %>% head(n = 3) %>%
  dist() # Distanz
```


### Übung `r nextExercise()`: Distanz {.exercise type=A-B-C answer=A}

Welche der folgenden Aussagen stimmt?

A.  Beobachtungen `1` und `2` sind sich am ähnlichsten.
B.  Beobachtungen `1` und `3` sind sich am ähnlichsten.
C.  Beobachtungen `2` und `3` sind sich am ähnlichsten.

<div class="notes">
Die Distanz ist zwischen `1` und `2` am kleinsten, also ***A***.
</div>



### k-Means Clusteranalyse

Der Ablauf des Verfahrens:

- 1:  Zufällige Beobachtungen als $k$ Clusterzentren.
- 2:  Zuordnung der Beobachtungen zum nächsten Clusterzentrum.
- 3:  Neuberechnung der Clusterzentren als Mittelwert der dem Cluster zugeordneten Beobachtungen.

Wiederholung bis keine Änderung in (2) oder maximale Iterationsanzahl erreicht.


### k-Means Clusteranalyse tipsscale

Mit z. B. $k=3$ Zentren:


```{r}
set.seed(1896) # Zufallszahlengenerator setzen

ergkclust <- kmeans(tipsscale, # Datensatz
                    centers = 3, # Anzahl Zentren: k
                    nstart = 10) # Anzahl Startpartitionen
```


### Ergebnis k-Means

```{r}
ergkclust$size # Anzahl Beob. je Cluster
ergkclust$centers # Cluster Zentren
```


### Übung `r nextExercise()`: Clustergröße {.exercise type=A-B-C answer=A}

Welche Aussage stimmt?

A.  Die Anzahl Beobachtungen ist in Cluster `1` am größten.
B.  Die Anzahl Beobachtungen ist in Cluster `2` am größten.
C.  Die Anzahl Beobachtungen ist in Cluster `3` am größten.

<div class="notes">
***A***, da $n_{\text{Cluster 1}}=`r ergkclust$size[1]`$  maximal ist.
</div>


### Übung `r nextExercise()`: Beschreibung Cluster {.exercise type=A-B-C answer=B}

Für welchen Cluster passt die Beschreibung *spendabel* am Besten?

A.  Cluster `1`.
B.  Cluster `2`.
C.  Cluster `3`.

<div class="notes">
Das Clusterzentrum von `2` hat den höchsten Wert in der Variable `tip`, also ***B***.
</div>


###  Übung `r nextExercise()`: {.exercise type=A-B-C answer=C}

In welcher Variable unterscheidet sich Cluster `C` am deutlichsten von den anderen?^[Beachte: Der Datensatz `tipsscale` ist standardisiert.]

A.  `size`
B.  `total_bill`
C.  `tip`

<div class="notes">
In einem standardisierten Datensatz haben alle Variablen eine Standardabweichung von $1$, daher können die Werte direkt verglichen werden. Die größte Abweichung von $0$ gibt es in Cluster `3` in der Variable `tip` (***C***).
</div>


### Anzahl Cluster $k$

Wie viele Cluster sollte man nehmen? z. B. Betrachte die Summe der Streuungen innerhalb der Cluster. Wenn diese nicht mehr deutlich kleiner wird evt. aufhören.^[Vgl. Screeplot]

\[
\sum_{j=1}^k\sum_{i=i}^{n_j}d(x_{i,j},\bar{x}_j)
\]

```{r}
set.seed(1896) # Reproduzierbarkeit

maxk <- 10 # Maximale Anzahl Cluster
anzk <- 1:maxk # Vektor mit Clusterzahl
sse <- numeric(maxk) # (leerer) Ergebnisvektor

# Schleife über Anzahl Cluster; SSE speichern
for (k in anzk) 
  sse[k] <- kmeans(tipsscale, centers = k)$tot.withinss
```

### Anzahl Cluster und Streuungssumme

```{r  fig.align="center", out.width="60%"}
gf_line(sse ~ anzk)
```


### Offene Übung `r nextExercise()`: Summe der Streuung innerhalb der Cluster {.exercise type=essay}

Warum fällt i.d.R. die Streuungssumme innerhalb der Cluster?

<div class="notes">
Die Werte der Beobachtungen variieren, streuuen. Evt. kann ein Teil der Streuung durch unterschiedliche Gruppen (hier: Cluster) erfasst werden. Je mehr verschiedene Cluster man verwendet, desto ähnlicher können sich die Beobachtungen innerhalb eines Cluster sein. Bei unveränderter Gesamtstreuung nimmt die Streuung zwischen den Clustern zu, die innerhalb der Cluster sinkt. Vgl. ANOVA mit $SST=SSG+SSE$.

Vgl. auch Varianzaanalyse und Hauptkomponentenanalyse.

</div>


### Offene Übung `r nextExercise()`: Cluster unskaliert {.exercise type=essay}

Führen Sie eine Clusteranalyse auf die nicht skalierten numerischen Variablen durch. Unterscheidet sich das Ergebnis? Warum?

```{r, include=FALSE}
tipUeb <- tips %>%
  dplyr::select(size, total_bill, tip)

tipUeb %>% 
  dist() %>% 
  hclust() %>% 
  plot()

set.seed(1896)

Uebkclust <- kmeans(tipUeb, # Datensatz
                    centers = 3, # Anzahl Zentren: k
                    nstart = 10) # Anzahl Startpartitionen

Uebkclust$size # Anzahl Beob. je Cluster
Uebkclust$centers # Cluster Zentren
```

<div class="notes">
Da die Variablen unterschiedliche Varianzen haben, unterscheidet sich das Ergebnis von der Clusteranalyse mit den standardisierten Variablen.

`tipUeb <- tips %>% select(size, total_bill, tip)`  
`set.seed(1896)`  
`Uebkclust <- kmeans(tipUeb,centers = 3, nstart = 10)`  
`Uebkclust$size`  
`Uebkclust$centers` 
</div>

```{r finish-PCA-und-Clusteranalyse, include=FALSE}
rm(pathToImages)
finalizePart(partname)
```
