```{r setup-Clusteranalyse, include=FALSE}
# ---------------------------------------------------------------------------
#% maintainer:
#%   - Karsten Luebke
#%
# ---------------------------------------------------------------------------
source("../prelude.R")
initPart(
    "Cluster",       # Dateiname ohne Suffix
    "Clusterananlyse"  # Verzeichnisname im Bilderverzeichnis 
    )
pathToImages = getPathToImages()
# ---------------------------------------------------------------------------

library(mosaic)
library(knitr)

data("iris")
iris.s <- iris[,3:4] %>%
  scale() %>%
  data.frame()

tips <- assertData("tips.csv", "https://goo.gl/whKjnl")
```
# `r nextChapter()` Clusteranalyse

### Unüberwachtes Lernen

**Unüberwachtes Lernen** (engl.: unsupervised learning): Es gibt *keine* bekannte abhängige Variable $y$, die modelliert werden soll

Methoden (u. a.):

- **Hauptkomponentenanalyse** (engl.: Principal Component Analysis): Finde (wenige) Linearkombinationen der Variablen: Zusammenfassung von Variablen, Dimensionsreduktion. `prcomp()`
- **Clusteranalyse** (engl.: Cluster Analysis): Finde Gruppen (Cluster) von Beobachtungen, die innerhalb der Cluster homogen, zwischen den Clustern heterogen sind^[Clustern von Variablen analog]. `kmeans()`

### Beispiele

- Finden von homogenen Studierendengruppen, um ihnen gezielt Materialien zur Verfügung zu stellen.
- Analysieren, inwieweit es unterschiedliche Gruppen von Unternehmen am Markt gibt.
- Finden von Kundensegmenten auf Basis von Kundendaten.
- Identifizieren von Clustern (Typen) verschiedener Führungspersönlichkeiten.

Bayrischer Rundfunk: Ein Marktforscher erklärt die Milieu-Typen -- Faszination Wissen -- Clip vom 30.3.2015: [https://youtu.be/3MbFKuL80jk](https://youtu.be/3MbFKuL80jk)

*Wo können Sie dies Verfahren einsetzen?*


### Vorbereitung: Trinkgelddaten

Einlesen der "Tipping"^[Bryant, P. G. and Smith, M. (1995). Practical Data Analysis: Case Studies in Business Statistics. Homewood, IL: Richard D. Irwin Publishing] Daten sowie laden des Pakets `mosaic`.

```{r, eval= FALSE, message=FALSE}
download.file("https://goo.gl/whKjnl", destfile = "tips.csv")
tips <- read.csv2("tips.csv")
# Alternativ - heruntergeladene Datei einlesen:
# tips <- read.csv2(file.choose()) 

library(mosaic) # Paket laden
```


### Vorbereitung: Skalierung numerischer Variablen

```{r, echo=FALSE}
tipscale <- tips %>% 
  dplyr::select(size, total_bill, tip) %>% # Variablen wählen
  scale() %>%                              # Skalieren 
  data.frame()                             # Als Datensatz definieren
```
```{r, eval=FALSE}
tipscale <- tips %>% 
  select(size, total_bill, tip) %>% # Variablen wählen
  scale() %>%                       # Skalieren 
  data.frame()                      # Als Datensatz definieren
```


### Methoden der Clusteranalyse

U.a.: **Partitionierend**: Beobachtungen werden zu $k$ Clustern zusammengefasst.^[Methoden um $k$ (häufig: $k \in \{2, 3, \ldots, 10\}$, aber unbekannt!) zu finden siehe z. B. Charrad M., Ghazzali N., Boiteau V. and Niknafs, A. (2014). NbClust: An R Package for Determining the Relevant Number of Clusters in a Data Set. Journal of Statistical Software, 61(6), 1-36. [http://dx.doi.org/10.18637/jss.v061.i06](http://dx.doi.org/10.18637/jss.v061.i06)]

Ähnlichkeit/Unähnlichkeit wird dabei über **Distanzmaße** bestimmt. Für metrische Merkmale z. B. euklidischer Abstand: $$d(x,y)=\sqrt{\sum_j{(x_j-y_j)^2}}$$


### Unterschiedliche Cluster $k$

```{r, echo=FALSE, fig.align="center", out.width="90%"}
set.seed(1896)
cluster.iris2 <- kmeans(iris.s, centers=2)
cluster.iris3 <- kmeans(iris.s, centers=3)
cluster.iris4 <- kmeans(iris.s, centers=4)
cluster.iris5 <- kmeans(iris.s, centers=5)
p2 <- xyplot(Petal.Width ~ Petal.Length, data=iris.s, xlab="x", ylab="y", col=cluster.iris2$cluster, main="k=2")
p3 <- xyplot(Petal.Width ~ Petal.Length, data=iris.s, xlab="x", ylab="y", col=cluster.iris3$cluster, main="k=3")
p4 <- xyplot(Petal.Width ~ Petal.Length, data=iris.s, xlab="x", ylab="y", col=cluster.iris4$cluster, main="k=4")
p5 <- xyplot(Petal.Width ~ Petal.Length, data=iris.s, xlab="x", ylab="y", col=cluster.iris5$cluster, main="k=5")
gridExtra::grid.arrange(p2,p3,p4,p5, ncol=2)
```


### Distanz

```{r}
tipscale[1:3,] # Erste drei Beobachtungen

tipscale[1:3,] %>%
  dist() # Distanz
```


### Übung `r nextExercise()`: Distanz {.exercise type=A-B-C answer=A}

Welche der folgenden Aussagen stimmt?

A.  Beobachtungen `1` und `2` sind sich am ähnlichsten.
B.  Beobachtungen `1` und `3` sind sich am ähnlichsten.
C.  Beobachtungen `2` und `3` sind sich am ähnlichsten.

<div class="notes">
Die Distanz ist zwischen `1` und `2` am kleinsten, also ***A***.
</div>



### k-Means Clusteranalyse

Der Ablauf des Verfahrens:

- 1:  Zufällige Beobachtungen als $k$ Clusterzentren.
- 2:  Zuordnung der Beobachtungen zum nächsten Clusterzentrum.
- 3:  Neuberechnung der Clusterzentren als Mittelwert der dem Cluster zugeordneten Beobachtungen.

Wiederholung bis keine Änderung in (2) oder maximale Iterationsanzahl erreicht.


### k-Means Clusteranalyse tipscale

Mit z. B. $k=3$ Zentren:


```{r}
set.seed(1896) # Zufallszahlengenerator setzen

ergkclust <- kmeans(tipscale, # Datensatz
                    centers = 3, # Anzahl Zentren: k
                    nstart = 10) # Anzahl Startpartitionen
```


### Ergebnis k-Means

```{r}
ergkclust$size # Anzahl Beob. je Cluster
ergkclust$centers # Cluster Zentren
```


### Übung `r nextExercise()`: Clustergröße {.exercise type=A-B-C answer=C}

Welche Aussage stimmt?

A.  Die Anzahl Beobachtungen ist in Cluster `1` am größten.
B.  Die Anzahl Beobachtungen ist in Cluster `2` am größten.
C.  Die Anzahl Beobachtungen ist in Cluster `3` am größten.

<div class="notes">
***C***, da $n_{\text{Cluster 3}}=`r ergkclust$size[3]`$  maximal ist.
</div>


### Übung `r nextExercise()`: Beschreibung Cluster {.exercise type=A-B-C answer=A}

Für welchen Cluster passt die Beschreibung *spendabel* am Besten?

A.  Cluster `1`.
B.  Cluster `2`.
C.  Cluster `3`.

<div class="notes">
Das Clusterzentrum von `1` hat den höchsten Wert in der Variable `tip`, also ***A***.
</div>


###  Übung `r nextExercise()`: {.exercise type=A-B-C answer=B}

In welcher Variable unterscheidet sich Cluster `3` am deutlichsten von den anderen?^[Beachte: Der Datensatz `tipscale` ist standardisiert.]

A.  `size`
B.  `total_bill`
C.  `tip`

<div class="notes">
In einem standardisierten Datensatz haben alle Variablen eine Standardabweichung von $1$, daher können die Werte direkt verglichen werden. Die größte Abweichung von $0$ gibt es in Cluster `3` in der Variable `total_bill` (***B***).
</div>


### Anzahl Cluster $k$

Wie viele Cluster sollte man nehmen? z. B. Betrachte die Summe der Streuungen innerhalb der Cluster. Wenn diese nicht mehr deutlich kleiner wird evt. aufhören.^[Vgl. Screeplot]

\[
SSE=\sum_{j=1}^k\sum_{i=i}^{n_j}(x_{i,j}-\bar{x}_j)^2
\]

```{r}
set.seed(1896) # Reproduzierbarkeit

maxk <- 10 # Maximale Anzahl Cluster
anzk <- 1:maxk # Vektor mit Clusterzahl
sse <- numeric(maxk) # (leerer) Ergebnisvektor

# Schleife über Anzahl Cluster; SSE speichern
for (k in anzk) 
  sse[k] <- kmeans(tipscale, centers = k)$tot.withinss
```

### Anzahl Cluster und Streuungssumme

```{r  fig.align="center", out.width="60%"}
xyplot(sse ~ anzk, type="l")
```


### Offene Übung `r nextExercise()`: Summe der Streuung innerhalb der Cluster {.exercise type=essay}

Warum fällt i.d.R. die Streuungssumme innerhalb der Cluster ($SSE$)?

<div class="notes">
Die Werte der Beobachtungen variieren, streuuen. Evt. kann ein Teil der Streuung durch unterschiedliche Gruppen (hier: Cluster) erfasst werden. Je mehr verschiedene Cluster man verwendet, desto ähnlicher können sich die Beobachtungen innerhalb eines Cluster sein. Bei unveränderter Gesamtstreuung ($SST$) nimmt die Streuung zwischen den Clustern zu ($SSB$), die innerhalb der Cluster sinkt ($SSE$).

Vgl. auch Varianzaanalyse und Hauptkomponentenanalyse.

</div>


### Offene Übung `r nextExercise()`: Cluster unskaliert {.exercise type=essay}

Führen Sie eine Clusteranalyse auf die nicht skalierten numerischen Variablen durch. Unterscheidet sich das Ergebnis? Warum?

```{r, include=FALSE}
tipUeb <- tips %>%
  dplyr::select(size, total_bill, tip)

tipUeb %>% 
  dist() %>% 
  hclust() %>% 
  plot()

set.seed(1896)

Uebkclust <- kmeans(tipUeb, # Datensatz
                    centers = 3, # Anzahl Zentren: k
                    nstart = 10) # Anzahl Startpartitionen

Uebkclust$size # Anzahl Beob. je Cluster
Uebkclust$centers # Cluster Zentren
```

<div class="notes">
Da die Variablen unterschiedliche Varianzen haben, unterscheidet sich das Ergebnis von der Clusteranalyse mit den standardisierten Variablen.

`tipUeb <- tips %>% select(size, total_bill, tip)`  
`set.seed(1896)`  
`Uebkclust <- kmeans(tipUeb,centers = 3, nstart = 10)`  
`Uebkclust$size`  
`Uebkclust$centers` 
</div>

```{r finish-PCA-und-Clusteranalyse, include=FALSE}
rm(pathToImages)
finalizePart(partname)
```
